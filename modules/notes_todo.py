#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Feb 23 12:41:01 2021

@author: ghiggi
"""
# PyTorch uses a caching memory allocator to speed up memory allocations. 
# As a result, the values shown in nvidia-smi usually donâ€™t reflect the true memory usage

## torch.cuda.empty_cache() 
# - To free all unused memory
# - Not really good practice as memory re-allocation is time consuming

## Check if python process are still alive when quiting 
# ps -elf | grep python



### Memory deallocation example
from warnings import WarningMessage
import torch 
a = torch.zeros(100,100,100).cuda()

print(torch.cuda.memory_allocated())

del a
print(torch.cuda.memory_allocated())
torch.cuda.synchronize()
print(torch.cuda.memory_allocated())

# https://github.com/openclimatefix/predict_pv_yield_2/blob/master/notebooks/20.0_simplify_data_loading.ipynb 

## Profiler 
# - https://pytorch.org/docs/master/profiler.html
# - https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html



 
#  - (because )

# torch.utils.data.get_worker_info()
# - worker id, dataset replica, initial seed, etc.), 
# - returns None in main process

# Random seeds in workers with its worker_init_fn option
# - By default, each worker will have its PyTorch seed set to base_seed + worker_id,
#    where base_seed is a long generated by main process using its RNG (thereby, consuming a RNG state mandatorily)
# - You may access the PyTorch seed set for each worker with either
#    torch.utils.data.get_worker_info().seed or torch.initial_seed(),

# Dataset access together with its internal IO,
#  transforms (including collate_fn) runs in the worker process.

## map-style datasets, 
# - The main process generates the indices using sampler and sends them to the workers. 
#   So any shuffle randomization is done in the main process which guides loading by assigning indices to load.

# For iterable-style datasets, since each worker process gets a replica 
#  of the dataset object, naive multi-process loading will often 
#  result in duplicated data.

# On Unix, fork() is the default multiprocessing start method. 
#  Using fork(), child workers typically can access the dataset and Python 
#  argument functions directly through the cloned address space



#As such, we can't instantiate the xarray.DataArray holding the satellite data 
# in SatelliteDataset.__init__() because then DataLoader would try to copy that
#  DataArray into each worker process.
#  Instead, we delay the creation of the DataArray until after the worker processes
#  have been created.

