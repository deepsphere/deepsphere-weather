{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized pooling through interpolation\n",
    "\n",
    "1. Compute interpolation weights between any two spherical samplings with [CDO](https://code.mpimet.mpg.de/projects/cdo) (through [SCRIP](https://github.com/SCRIP-Project/SCRIP/wiki/SCRIP-User-Guide) or [YAC](https://dkrz-sw.gitlab-pages.dkrz.de/yac/)).\n",
    "    * When downsampling, conservative interpolation (i.e., that preserve the integral) best represent the finer grid.\n",
    "    * When upsampling, conservative interpolation will keep the piece-wise constant structure of the coarser grid. Bilinear will be smoother. See a [comparison](https://pangeo-xesmf.readthedocs.io/en/latest/notebooks/Compare_algorithms.html).\n",
    "2. Use these weights to pool and unpool in pytorch by a multiplication with a sparse matrix.\n",
    "    * These weights are also used to remap ERA5 to any of our spherical samplings (from an N320 Gaussian grid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from matplotlib import pyplot as plt\n",
    "import pygsp as pg\n",
    "\n",
    "from .modules import remap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cdo from conda environment from a jupyter that is not in the environment.\n",
    "# import os, subprocess\n",
    "# cdo = os.path.join(sys.exec_prefix, 'bin/cdo')\n",
    "# p = subprocess.run([cdo, '-V'], stderr=subprocess.PIPE)\n",
    "# print(p.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphs to remap to (pool and unpool to).\n",
    "graph1 = pg.graphs.SphereHealpix(subdivisions=2, nest=True, k=4, kernel_width=None)\n",
    "graph2 = pg.graphs.SphereHealpix(subdivisions=1, nest=True, k=4, kernel_width=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get true HEALPix pixels (not Voronoi cells) for verification\n",
    "\n",
    "Update `SphericalVoronoiMesh_from_pygsp` (in `remap.py`) with the below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph2\n",
    "radius = 1\n",
    "\n",
    "def xyz2lonlat(x,y,z, radius=6371.0e6):\n",
    "    \"\"\"From cartesian geocentric coordinates to 2D geographic coordinates.\"\"\"\n",
    "    latitude = np.arcsin(z / radius)/np.pi*180\n",
    "    longitude = np.arctan2(y, x)/np.pi*180\n",
    "    return longitude, latitude \n",
    "\n",
    "# Hack to get HEALPix true vertices (quadrilateral polygons).\n",
    "import healpy as hp\n",
    "npix = graph.n_vertices\n",
    "nside = np.sqrt(npix/12)\n",
    "step = 8  # number of vertices per edge (edges are not geodesics)\n",
    "vertices = hp.boundaries(nside, range(npix), nest=graph.nest, step=step)\n",
    "assert vertices.shape == (npix, 3, 4*step)\n",
    "list_polygons_lonlat = []\n",
    "for tmp_xyz in vertices:\n",
    "    tmp_lon, tmp_lat = xyz2lonlat(tmp_xyz[0],tmp_xyz[1],tmp_xyz[2], radius=radius)\n",
    "    list_polygons_lonlat.append(np.column_stack((tmp_lon, tmp_lat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HEALPix vertices are ordered counter-clockwise.\n",
    "vertex = vertices[7]\n",
    "lat, lon = pg.utils.xyz2latlon(vertex[0], vertex[1], vertex[2])\n",
    "plt.scatter(lon, lat)\n",
    "plt.xlim(0, 2*np.pi)\n",
    "plt.ylim(-np.pi/2, np.pi/2)\n",
    "for i, (lon_i, lat_i) in enumerate(zip(lon, lat)):\n",
    "    plt.text(lon_i, lat_i, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get interpolation (remapping) weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remap.get_available_interp_methods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remap.compute_interpolation_weights(graph1, graph2, method='conservative', normalization='fracarea') # destarea’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build interpolation (pooling) matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_interpolation_matrix(src_graph, dst_graph):\n",
    "    \"\"\"Return the sparse matrix that interpolates between two spherical samplings.\"\"\"\n",
    "\n",
    "    ds = remap.compute_interpolation_weights(src_graph, dst_graph, method='conservative', normalization='fracarea') # destarea’\n",
    "\n",
    "    # Sanity checks.\n",
    "    np.testing.assert_allclose(ds.src_grid_center_lat, src_graph.signals['lat'])\n",
    "    np.testing.assert_allclose(ds.src_grid_center_lon, src_graph.signals['lon'])\n",
    "    np.testing.assert_allclose(ds.dst_grid_center_lat, dst_graph.signals['lat'])\n",
    "    np.testing.assert_allclose(ds.dst_grid_center_lon, dst_graph.signals['lon'])\n",
    "    np.testing.assert_allclose(ds.src_grid_frac, 1)\n",
    "    np.testing.assert_allclose(ds.dst_grid_frac, 1)\n",
    "    np.testing.assert_allclose(ds.src_grid_imask, 1)\n",
    "    np.testing.assert_allclose(ds.dst_grid_imask, 1)\n",
    "\n",
    "    col = ds.src_address\n",
    "    row = ds.dst_address\n",
    "    dat = ds.remap_matrix.squeeze()\n",
    "    # CDO indexing starts at 1\n",
    "    row = np.array(row) - 1\n",
    "    col = np.array(col) - 1\n",
    "    weights = sparse.csr_matrix((dat, (row, col)))\n",
    "    assert weights.shape == (dst_graph.n_vertices, src_graph.n_vertices)\n",
    "\n",
    "    # Destination pixels are normalized to 1 (row-sum = 1).\n",
    "    # Weights represent the fractions of area attributed to source pixels.\n",
    "    np.testing.assert_allclose(weights.sum(axis=1), 1)\n",
    "    # Interpolation is conservative: it preserves area.\n",
    "    np.testing.assert_allclose(weights.T @ ds.dst_grid_area, ds.src_grid_area)\n",
    "\n",
    "    # Unnormalize.\n",
    "    weights = weights.multiply(ds.dst_grid_area.values[:, np.newaxis])\n",
    "\n",
    "    # Another way to assert that the interpolation is conservative.\n",
    "    np.testing.assert_allclose(np.asarray(weights.sum(1)).squeeze(), ds.dst_grid_area)\n",
    "    np.testing.assert_allclose(np.asarray(weights.sum(0)).squeeze(), ds.src_grid_area)\n",
    "\n",
    "    return weights\n",
    "\n",
    "def build_pooling_matrices(weights):\n",
    "    \"\"\"Normalize interpolation matrix for pooling and unpooling.\"\"\"\n",
    "    pool = weights.multiply(1/weights.sum(1))\n",
    "    unpool = weights.multiply(1/weights.sum(0)).T\n",
    "    return pool, unpool\n",
    "\n",
    "weights = build_interpolation_matrix(graph1, graph2)\n",
    "pool, unpool = build_pooling_matrices(weights)\n",
    "\n",
    "# Check normalization.\n",
    "np.testing.assert_allclose(pool.sum(1), 1)\n",
    "np.testing.assert_allclose(unpool.sum(1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the interpolation matrices\n",
    "\n",
    "* Can be seen as a bipartite graph made of source and destination pixels.\n",
    "* Entries (edge weights) are the overlapping areas between source and destination pixels.\n",
    "    * The row-sum is the areas of destination pixels.\n",
    "    * The column-sum is the areas of source pixels.\n",
    "* Matrix can be row- (destination areas equal 1) or column-normalized (source areas equal 1).\n",
    "    * Row-normalize: pooling matrix.\n",
    "    * Column-normalize: unpooling matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interpolation_matrix(weights):\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(24, 4))\n",
    "    axes[0].hist(weights.data, bins=100)\n",
    "    axes[0].set_title('histogram of overlaping areas')\n",
    "    im = axes[1].imshow(weights.toarray())\n",
    "    fig.colorbar(im, ax=axes[1])\n",
    "    axes[1].set_title('non-normalized interpolation matrix')\n",
    "\n",
    "    def plot_area(area, name, ax):\n",
    "        ax.plot(area, '.')\n",
    "        assert np.allclose(area.mean(), 4*np.pi / len(area))\n",
    "        ax.axhline(area.mean(), ls='--', c='grey')\n",
    "        ax.text(0, area.mean(), 'mean area', c='grey', va='top')\n",
    "        ax.set_title(f'{name} pixel areas')\n",
    "\n",
    "    area_src = weights.sum(0)\n",
    "    area_dst = weights.sum(1)\n",
    "    plot_area(area_src.T, 'source', axes[2])\n",
    "    plot_area(area_dst, 'destination', axes[3])\n",
    "\n",
    "plot_interpolation_matrix(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Close to the 4x 0.25 of average pooling on HEALPix with pixels subdivided into 4 (not Voronoi).\n",
    "* Almost all ones, because most pixels are simply included in a parent.\n",
    "  That is true of the true HEALPix pixels, less so for Voronoi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "im = axes[0].imshow(pool.toarray())\n",
    "fig.colorbar(im, ax=axes[0])\n",
    "im = axes[1].imshow(unpool.toarray())\n",
    "fig.colorbar(im, ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When is interpolation invertible?\n",
    "\n",
    "* Unpooling then pooling can be non-destructive (no loss of information).\n",
    "    * `pool @ unpool = I` when the bipartite interpolation graph is disconnected, i.e., parent vertices have disjoint supports.\n",
    "    * In which case `pool` is the [Moore–Penrose inverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) of `unpool`.\n",
    "* Pooling then unpooling is necessarily destructive (reduction of degrees of freedom).\n",
    "    * `unpool @ pool` should be a block-diagonal (averaging over pooled vertices) matrix if all pixels are included in a single parent (and properly ordered).\n",
    "    * Should `unpool` be the Moore–Penrose inverse of `pool`?\n",
    "\n",
    "The two above work for true HEALPix pixels (not the Voronoi cells), with pooling `[0.25, 0.25, 0.25, 0.25]` and unpooling `[1, 1, 1, 1]`, because that sampling scheme is exactly hierarchical.\n",
    "\n",
    "Can we use this to evaluate the quality of a coarsening/interpolation or of a (hierarchical) sampling?\n",
    "* Can look at `1 - np.diag(pool @ unpool)` (as the row-sum is one, that is also the sum of off-diagonal elements).\n",
    "* `np.sum(1 - np.diag(pool @ unpool)) / npix` is the fraction of averaged/mixed pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example(weights):\n",
    "    unpool = (weights / weights.sum(0)).T\n",
    "    pool = weights / weights.sum(1)[:, np.newaxis]\n",
    "\n",
    "    print(unpool)\n",
    "    print(pool)\n",
    "\n",
    "    print(pool @ unpool)\n",
    "    print(unpool @ pool)\n",
    "\n",
    "print('Is invertible:')\n",
    "example(np.array([\n",
    "    [1, 3, 0, 0],\n",
    "    [0, 0, 3, 1],\n",
    "]))\n",
    "print('Is not invertible:')\n",
    "example(np.array([\n",
    "    [1, 3, 0, 0],\n",
    "    [0, 1, 3, 1],\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrices(mat1, mat2, axes=None):\n",
    "    if sparse.issparse(mat1):\n",
    "        mat1 = mat1.toarray()\n",
    "    if sparse.issparse(mat2):\n",
    "        mat2 = mat2.toarray()\n",
    "    if axes is None:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 4))\n",
    "    im = axes[0].imshow(mat1)\n",
    "    axes[0].figure.colorbar(im, ax=axes[0])\n",
    "    im = axes[1].imshow(mat2)\n",
    "    axes[1].figure.colorbar(im, ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pool @ unpool\n",
    "\n",
    "# Only if non-destructive.\n",
    "# assert np.allclose(p, np.identity(graph2.N), atol=1e-10)\n",
    "\n",
    "err = np.identity(graph2.N) - p\n",
    "plot_matrices(p.toarray(), err)\n",
    "\n",
    "# Another way to see the error.\n",
    "# pool_pinv = np.linalg.pinv(unpool.toarray())\n",
    "# assert np.allclose(pool_pinv @ unpool, np.identity(graph2.n_vertices), atol=1e-10)\n",
    "# err = pool.toarray() - pool_pinv\n",
    "# plot_matrices(pool_pinv, err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inversion_error(pool, unpool, ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    # diag = np.diag((pool @ unpool).toarray()\n",
    "    diag = pool.multiply(unpool.T).sum(1)\n",
    "    err = 1 - diag\n",
    "    ax.plot(err, '.')\n",
    "    err = np.sum(err) / len(err)\n",
    "    ax.set_title(f'averaging error per pixel ({err:.1%} overall error)')\n",
    "\n",
    "plot_inversion_error(pool, unpool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = unpool @ pool\n",
    "\n",
    "def block_diag(blocksize, nblock):\n",
    "    block = np.ones((int(blocksize), int(blocksize))) / blocksize\n",
    "    return sparse.block_diag([block]*nblock)\n",
    "\n",
    "# Only a true error for the original HEALPix pixels. Not the Voronoi ones (which may overlap).\n",
    "err = block_diag(int(graph1.n_vertices / graph2.n_vertices), graph2.n_vertices) - p\n",
    "\n",
    "plot_matrices(p.toarray(), err.toarray())\n",
    "\n",
    "# Another way to see the error.\n",
    "# unpool_pinv = np.linalg.pinv(pool.toarray())\n",
    "# err = unpool.toarray() - unpool_pinv\n",
    "# plot_matrices(unpool_pinv, err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplacian coarsening\n",
    "\n",
    "* And preservation of its action and spectral properties.\n",
    "* See also [Spectrally approximating large graphs with smaller graphs](https://arxiv.org/abs/1802.07510)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph1 = pg.graphs.SphereHealpix(subdivisions=8, nest=False, k=20, kernel_width=None)\n",
    "# graph2 = pg.graphs.SphereHealpix(subdivisions=2, nest=False, k=20, kernel_width=None)\n",
    "# weights = build_interpolation_matrix(graph1, graph2)\n",
    "# pool, unpool = build_pooling_matrices(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_laplacians(L, graph):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    err = L - graph.L\n",
    "    plot_matrices(L, err, axes)\n",
    "    graph.compute_fourier_basis()\n",
    "    e, U = np.linalg.eigh(L.toarray())\n",
    "    axes[2].plot(graph.e, '.-', label='original')\n",
    "    axes[2].plot(e, '.-', label='reconstructed')\n",
    "    axes[2].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrices(graph1.L, graph2.L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing on a graph of lower or higher resolution.\n",
    "# TODO: a scaling factore is missing.\n",
    "plot_laplacians(pool @ graph1.L @ unpool, graph2)\n",
    "plot_laplacians(unpool @ graph2.L @ pool, graph1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph compression.\n",
    "plot_laplacians(pool @ unpool @ graph2.L @ pool @ unpool, graph2)\n",
    "plot_laplacians(unpool @ pool @ graph1.L @ unpool @ pool, graph1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining\n",
    "\n",
    "* Chaining multiple pooling layers across resolutions.\n",
    "* Conservative remapping two consecutive times isn't equivalent to remapping directly.\n",
    "    * Same issue as inversion.\n",
    "    * Ambiguity: if two up and down pixels are attached to an intermediatary, how to distribute the areas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = [\n",
    "    pg.graphs.SphereHealpix(4, k=8),\n",
    "    pg.graphs.SphereHealpix(2, k=8),\n",
    "    pg.graphs.SphereHealpix(1, k=8),\n",
    "]\n",
    "\n",
    "weights1 = build_interpolation_matrix(graphs[0], graphs[1])\n",
    "weights2 = build_interpolation_matrix(graphs[1], graphs[2])\n",
    "weights3 = build_interpolation_matrix(graphs[0], graphs[2])\n",
    "\n",
    "# Toy example illustrating the mixing.\n",
    "# weights2 = sparse.csr_matrix(np.array([\n",
    "#     [1, 1],\n",
    "#     [0.5, 0],\n",
    "# ]))\n",
    "# weights1 = sparse.csr_matrix(np.array([\n",
    "#     [0.5, 1, 0, 0, 0],\n",
    "#     [0, 0.1, 0.6, 0.1, 0.2],\n",
    "# ]))\n",
    "# weights3 = sparse.csr_matrix(np.array([\n",
    "#     [0.2, 0.9, 0.6, 0.1, 0.2],\n",
    "#     [0.3, 0.2, 0, 0, 0],\n",
    "# ]))\n",
    "\n",
    "# Same areas.\n",
    "np.testing.assert_allclose(weights1.sum(1), weights2.sum(0).T)\n",
    "np.testing.assert_allclose(weights1.sum(0), weights3.sum(0))\n",
    "np.testing.assert_allclose(weights2.sum(1), weights3.sum(1))\n",
    "\n",
    "pool1 = weights1.multiply(1/weights1.sum(1))\n",
    "pool2 = weights2.multiply(1/weights2.sum(1))\n",
    "pool3 = weights3.multiply(1/weights3.sum(1))\n",
    "\n",
    "unpool1 = weights1.multiply(1/weights1.sum(0)).T\n",
    "unpool2 = weights2.multiply(1/weights2.sum(0)).T\n",
    "unpool3 = weights3.multiply(1/weights3.sum(0)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = pool2 @ pool1\n",
    "np.testing.assert_allclose(pool.sum(1), 1)\n",
    "np.testing.assert_allclose(pool3.sum(1), 1)\n",
    "\n",
    "unpool = unpool1 @ unpool2\n",
    "np.testing.assert_allclose(unpool.sum(1), 1)\n",
    "np.testing.assert_allclose(unpool3.sum(1), 1)\n",
    "\n",
    "# Encoder-decoder on multi-scale sampling.\n",
    "unpool1.shape, unpool2.shape, pool2.shape, pool1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chaining is conservative by distributing area back.\n",
    "areas = weights2.sum(1)\n",
    "np.testing.assert_allclose(pool2.T @ areas, weights1.sum(1))\n",
    "np.testing.assert_allclose(pool.T @ areas, weights1.sum(0).T)\n",
    "np.testing.assert_allclose(pool3.T @ areas, weights1.sum(0).T)\n",
    "\n",
    "areas = weights1.sum(0)\n",
    "np.testing.assert_allclose(unpool1.T @ areas.T, weights2.sum(0).T)\n",
    "np.testing.assert_allclose(unpool.T @ areas.T, weights2.sum(1))\n",
    "np.testing.assert_allclose(unpool3.T @ areas.T, weights2.sum(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixing / averaging through intermediary pixels.\n",
    "assert not np.allclose(pool.toarray(), pool3.toarray())\n",
    "assert not np.allclose(unpool.toarray(), unpool3.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking on our samplings and resolutions\n",
    "\n",
    "* 5 spherical samplings.\n",
    "* Source resolution of ~400km.\n",
    "* 2 downsamplings and upsamplings for pooling and unpooling in our UNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplings = {\n",
    "    'healpix': [\n",
    "        pg.graphs.SphereHealpix(16),\n",
    "        pg.graphs.SphereHealpix(8),\n",
    "        pg.graphs.SphereHealpix(4),\n",
    "    ],\n",
    "    'icosahedral': [\n",
    "        pg.graphs.SphereIcosahedral(16),\n",
    "        pg.graphs.SphereIcosahedral(8),\n",
    "        pg.graphs.SphereIcosahedral(4),\n",
    "    ],\n",
    "    'cubed': [\n",
    "        pg.graphs.SphereCubed(22),\n",
    "        pg.graphs.SphereCubed(11),\n",
    "        pg.graphs.SphereCubed(5),\n",
    "    ],\n",
    "    'gausslegendre': [\n",
    "        pg.graphs.SphereGaussLegendre(45, nlon='ecmwf-octahedral'),\n",
    "        pg.graphs.SphereGaussLegendre(22, nlon='ecmwf-octahedral'),\n",
    "        pg.graphs.SphereGaussLegendre(11, nlon='ecmwf-octahedral'),\n",
    "    ],\n",
    "    'equiangular': [\n",
    "        pg.graphs.SphereEquiangular(38, 76),\n",
    "        pg.graphs.SphereEquiangular(19, 38),\n",
    "        pg.graphs.SphereEquiangular(10, 20),\n",
    "    ],\n",
    "    'random': [\n",
    "        pg.graphs.SphereRandom(2800, seed=1),\n",
    "        pg.graphs.SphereRandom(700, seed=1),\n",
    "        pg.graphs.SphereRandom(175, seed=1),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sampling in samplings.values():\n",
    "    weights = build_interpolation_matrix(sampling[0], sampling[1])\n",
    "    plot_interpolation_matrix(weights)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    plot_inversion_error(*build_pooling_matrices(weights), axes[0])\n",
    "    axes[1].hist((weights > 0).sum(1));\n",
    "    print('averaging over {:.1f} pixels, ({} non-zeros, {:.2%} sparsity)'.format(weights.nnz / weights.shape[0], weights.nnz, weights.nnz / np.prod(weights.shape)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdo-python3",
   "language": "python",
   "name": "cdo-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
