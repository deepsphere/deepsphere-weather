{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train best model according to report to try to reproduce results\n",
    "\n",
    "- All static features: \n",
    "    * Z500, \n",
    "    * T850, \n",
    "    * latitude, \n",
    "    * orography, \n",
    "    * land-sea mask, \n",
    "    * soil type, and \n",
    "    * top-of-atmosphere radiation\n",
    "- L=2\n",
    "- $\\Delta t$ = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/'.join(sys.path[0].split('/')[:-1]))\n",
    "\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import healpy as hp\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from modules.utils import train_model_2steps_temp, init_device\n",
    "from modules.data import WeatherBenchDatasetXarrayHealpixTemp, WeatherBenchDatasetIterative\n",
    "from modules.healpix_models import UNetSphericalTempHealpix, UNetSphericalHealpix\n",
    "from modules.test import create_iterative_predictions_healpix\n",
    "from modules.test import compute_rmse_healpix\n",
    "from modules.plotting import plot_rmses\n",
    "\n",
    "datadir = \"../data/healpix/\"\n",
    "input_dir = datadir + \"5.625deg/\"\n",
    "model_save_path = datadir + \"models/\"\n",
    "pred_save_path = datadir + \"predictions/\"\n",
    "\n",
    "if not os.path.isdir(model_save_path):\n",
    "    os.mkdir(model_save_path)\n",
    "    \n",
    "if not os.path.isdir(pred_save_path):\n",
    "    os.mkdir(pred_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define constants and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_years = ('1979', '2012')\n",
    "val_years = ('2013', '2016')\n",
    "test_years = ('2017', '2018')\n",
    "\n",
    "nodes = 12*16*16\n",
    "max_lead_time = 5*24\n",
    "nb_timesteps = 2\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,2\"\n",
    "gpu = [0, 1]\n",
    "num_workers = 10\n",
    "pin_memory = True\n",
    "\n",
    "nb_epochs = 20\n",
    "learning_rate = 8e-3\n",
    "\n",
    "obs = xr.open_mfdataset(pred_save_path + 'observations.nc', combine='by_coords')\n",
    "#rmses_weyn = xr.open_dataset(datadir + 'metrics/rmses_weyn.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions:\n",
    "\n",
    "**TODO**\n",
    "Check if the code is the same as the functions with the same name in ```modules/*.py``` and subtitute by imports in such a case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherBenchDatasetXarrayHealpixTemp(Dataset):\n",
    "    \n",
    "    \"\"\" Dataset used for graph models (1D), where data is loaded from stored numpy arrays.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : xarray Dataset\n",
    "        Dataset containing the input data\n",
    "    out_features : int\n",
    "        Number of output features\n",
    "    delta_t : int\n",
    "        Temporal spacing between samples in temporal sequence (in hours)\n",
    "    len_sqce : int\n",
    "        Length of the input and output (predicted) sequences\n",
    "    years : tuple(str)\n",
    "        Years used to split the data\n",
    "    nodes : float\n",
    "        Number of nodes each sample has\n",
    "    max_lead_time : int\n",
    "        Maximum lead time (in case of iterative predictions) in hours\n",
    "    load : bool\n",
    "        If true, load dataset to RAM\n",
    "    mean : np.ndarray of shape 2\n",
    "        Mean to use for data normalization. If None, mean is computed from data\n",
    "    std : np.ndarray of shape 2\n",
    "        std to use for data normalization. If None, mean is computed from data\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, ds, out_features, delta_t, len_sqce, years, nodes, nb_timesteps, \n",
    "                 max_lead_time=None, load=True, mean=None, std=None):\n",
    "        \n",
    "        \n",
    "        self.delta_t = delta_t\n",
    "        self.len_sqce = len_sqce\n",
    "        self.years = years\n",
    "        \n",
    "        self.nodes = nodes\n",
    "        self.out_features = out_features\n",
    "        self.max_lead_time = max_lead_time\n",
    "        self.nb_timesteps = nb_timesteps\n",
    "        \n",
    "        self.data = ds.to_array(dim='level', name='Dataset').transpose('time', 'node', 'level')\n",
    "        self.in_features = self.data.shape[-1]\n",
    "        \n",
    "        self.mean = self.data.mean(('time', 'node')).compute() if mean is None else mean\n",
    "        self.std = self.data.std(('time', 'node')).compute() if std is None else std\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Count total number of samples\n",
    "        total_samples = self.data.shape[0]        \n",
    "        if max_lead_time is None:\n",
    "            self.n_samples = total_samples - (len_sqce+1) * delta_t\n",
    "        else:\n",
    "            self.n_samples = total_samples - (len_sqce+1) * delta_t - max_lead_time\n",
    "            \n",
    "        \n",
    "        # Create indexes\n",
    "        self.idxs = [[[[sample_idx + delta_t*k for k in range(len_sqce)], sample_idx + delta_t * len_sqce], \n",
    "                      [sample_idx + delta_t * len_sqce, sample_idx + delta_t * (len_sqce+1)]] \n",
    "                     for sample_idx in range(self.n_samples)]\n",
    "        \n",
    "        # Normalize\n",
    "        self.data = (self.data - self.mean) / self.std\n",
    "        \n",
    "        if load: \n",
    "            print('Loading data into RAM')\n",
    "            self.data.to_array().load()\n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Returns sample and label corresponding to an index as torch.Tensor objects\n",
    "            The return tensor shapes are (for the sample and the label): [n_vertex, len_sqce, n_features]\n",
    "        \"\"\"\n",
    "        \n",
    "        X = (torch.tensor(self.data.isel(time=self.idxs[idx][0][0]).values).float(). \\\n",
    "             permute(1, 0, 2).reshape(self.nodes, self.in_features*self.len_sqce), \n",
    "             torch.tensor(self.data.isel(time=self.idxs[idx][0][1]).values[:, self.out_features:]).float())\n",
    "        \n",
    "        y = (torch.Tensor(self.data.isel(time=self.idxs[idx][1][0]).values[:, :self.out_features]).float(), \n",
    "             torch.Tensor(self.data.isel(time=self.idxs[idx][1][1]).values[:, :self.out_features]).float())\n",
    "        \n",
    "        return X, y \n",
    "    \n",
    "    \n",
    "def train_model_2steps_temp(model, device, train_generator, epochs, lr, validation_data, model_filename):    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, eps=1e-7, weight_decay=0, amsgrad=False)\n",
    "    \n",
    "    in_features = train_generator.dataset.in_features\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        time1 = time.time()\n",
    "        \n",
    "        val_loss = 0\n",
    "        train_loss = 0\n",
    "        \n",
    "        model.train()  \n",
    "        for batch_idx, (batch, labels) in enumerate(train_generator):\n",
    "            # Transfer to GPU\n",
    "            batch1 = batch[0].to(device)\n",
    "            constants1 = batch[1].to(device)\n",
    "\n",
    "            label1 = labels[0].to(device)\n",
    "            label2 = labels[1].to(device)\n",
    "\n",
    "            batch_size = batch1.shape[0]\n",
    "\n",
    "            # Model\n",
    "            output1 = model(batch1)\n",
    "            batch2 = torch.cat((batch1[:, :, in_features:], torch.cat((output1, constants1), dim=2)), dim=2)\n",
    "            output2 = model(batch2)\n",
    "            \n",
    "            loss = criterion(output1, label1) + criterion(output2, label2)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss = train_loss + loss.item() * batch_size\n",
    "            \n",
    "        train_loss = train_loss / (len(train_generator.dataset))\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            index = 0\n",
    "            \n",
    "            for batch, labels in validation_data:\n",
    "                # Transfer to GPU\n",
    "                batch1 = batch[0].to(device)\n",
    "                constants1 = batch[1].to(device)\n",
    "\n",
    "                label1 = labels[0].to(device)\n",
    "                label2 = labels[1].to(device)\n",
    "\n",
    "                batch_size = batch1.shape[0]\n",
    "                \n",
    "                output1 = model(batch1)\n",
    "                batch2 = torch.cat((batch1[:, :, in_features:], torch.cat((output1, constants1), \n",
    "                                                                          dim=2)), dim=2)\n",
    "                output2 = model(batch2)\n",
    "                \n",
    "                val_loss = val_loss + (criterion(output1, label1).item() \n",
    "                                       + criterion(output2, label2).item()) * batch_size\n",
    "                index = index + batch_size\n",
    "                \n",
    "        val_loss = val_loss / (len(validation_data.dataset))\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        time2 = time.time()\n",
    "        \n",
    "        # Print stuff\n",
    "        print('Epoch: {e:3d}/{n_e:3d}  - loss: {l:.3f}  - val_loss: {v_l:.5f}  - time: {t:2f}'\n",
    "              .format(e=epoch+1, n_e=epochs, l=train_loss, v_l=val_loss, t=time2-time1))\n",
    "        \n",
    "        torch.save(spherical_unet.state_dict(), model_filename[:-3] + '_epoch' + str(15+epoch) + '.h5')\n",
    "        \n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "\n",
    "def create_iterative_predictions_healpix_temp(model, device, dg):\n",
    "    \n",
    "    out_feat = dg.dataset.out_features\n",
    "    \n",
    "    train_std =  dg.dataset.std.values[:out_feat]\n",
    "    train_mean = dg.dataset.mean.values[:out_feat]\n",
    "    \n",
    "    delta_t = dg.dataset.delta_t\n",
    "    len_sqce = dg.dataset.len_sqce\n",
    "    max_lead_time = dg.dataset.max_lead_time\n",
    "    initial_lead_time = delta_t * len_sqce\n",
    "    nodes = dg.dataset.nodes\n",
    "    nside = int(np.sqrt(nodes/12))\n",
    "    n_samples = dg.dataset.n_samples\n",
    "    in_feat = dg.dataset.in_features\n",
    "    \n",
    "    # Lead times\n",
    "    lead_times = np.arange(delta_t, max_lead_time + delta_t, delta_t)\n",
    "    \n",
    "    # Lat lon coordinates\n",
    "    out_lon, out_lat = hp.pix2ang(nside, np.arange(nodes), lonlat=True)\n",
    "    \n",
    "    # Actual times\n",
    "    start = np.datetime64(dg.dataset.years[0], 'h') + np.timedelta64(initial_lead_time, 'h')\n",
    "    stop = start + np.timedelta64(dg.dataset.n_samples, 'h')\n",
    "    times = np.arange(start, stop)\n",
    "    \n",
    "    # Variables\n",
    "    var_dict_out = {var: None for var in ['z', 't']}\n",
    "    \n",
    "    # Constants\n",
    "    constants = np.array(dg.dataset.data.isel(level=slice(out_feat, None)).values)\n",
    "    \n",
    "    dataloader = dg\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "    for lead in lead_times:\n",
    "        outputs = []\n",
    "        next_batch_ = []\n",
    "        states = np.empty((n_samples, nodes, in_feat*len_sqce))\n",
    "        \n",
    "        time1 = time.time()\n",
    "\n",
    "        for i, (sample, _) in enumerate(dataloader):\n",
    "            inputs = sample[0].to(device)\n",
    "            output = model(inputs)\n",
    "            \n",
    "            next_batch_.append(inputs[:, :, in_feat:].detach().cpu().clone().numpy())\n",
    "            outputs.append(output.detach().cpu().clone().numpy()[:, :, :out_feat])\n",
    "            \n",
    "            \n",
    "        next_batch = np.concatenate(next_batch_)    \n",
    "        preds = np.concatenate(outputs)\n",
    "        states[:, :, :(len_sqce-1)*in_feat] = next_batch\n",
    "        states[:, :, (len_sqce-1)*in_feat:(len_sqce-1)*in_feat + out_feat] = preds\n",
    "        states[:, :, -(in_features - out_features):] =  constants[(len_sqce-1)*delta_t+\n",
    "                                                                  lead:n_samples+(len_sqce-1)*delta_t+lead, :]\n",
    "\n",
    "        predictions.append(preds * train_std + train_mean)\n",
    "\n",
    "        new_set = WeatherBenchDatasetIterative(states)\n",
    "        dataloader = DataLoader(new_set, batch_size=batch_size, shuffle=False, num_workers=10)\n",
    "        \n",
    "        time2 = time.time()\n",
    "        \n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    das = [];\n",
    "    lev_idx = 0\n",
    "    for var in ['z', 't']:       \n",
    "        das.append(xr.DataArray(\n",
    "            predictions[:, :, :, lev_idx],\n",
    "            dims=['lead_time', 'time', 'node'],\n",
    "            coords={'lead_time': lead_times, 'time': times, 'node': np.arange(nodes)},\n",
    "            name=var\n",
    "        ))\n",
    "        lev_idx += 1\n",
    "            \n",
    "    prediction_ds = xr.merge(das)\n",
    "    prediction_ds = prediction_ds.assign_coords({'lat': out_lat, 'lon': out_lon})\n",
    "    return prediction_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "z500 = xr.open_mfdataset(f'{input_dir}geopotential_500/*.nc', combine='by_coords').rename({'z':'z500'})\n",
    "t850 = xr.open_mfdataset(f'{input_dir}temperature_850/*.nc', combine='by_coords').rename({'t':'t850'})\n",
    "rad = xr.open_mfdataset(f'{input_dir}toa_incident_solar_radiation/*.nc', combine='by_coords')\n",
    "\n",
    "z500 = z500.isel(time=slice(7, None))\n",
    "t850 = t850.isel(time=slice(7, None))\n",
    "\n",
    "constants = xr.open_dataset(f'{input_dir}constants/constants_5.625deg.nc').rename({'orography' :'orog'})\n",
    "constants = constants.assign(cos_lon=lambda x: np.cos(np.deg2rad(x.lon)))\n",
    "constants = constants.assign(sin_lon=lambda x: np.sin(np.deg2rad(x.lon)))\n",
    "\n",
    "temp = xr.DataArray(np.zeros(z500.dims['time']), coords=[('time', z500.time.values)])\n",
    "constants, _ = xr.broadcast(constants, temp)\n",
    "\n",
    "orog = constants['orog']\n",
    "lsm = constants['lsm']\n",
    "lats = constants['lat2d']\n",
    "slt = constants['slt']\n",
    "cos_lon = constants['cos_lon']\n",
    "sin_lon = constants['sin_lon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = xr.open_mfdataset(f'{input_dir}geopotential_500/*.nc', combine='by_coords')['z']\\\n",
    ".assign_coords(level=1)\n",
    "\n",
    "t = xr.open_mfdataset(f'{input_dir}temperature_850/*.nc', combine='by_coords')['t']\\\n",
    ".assign_coords(level=1)\n",
    "\n",
    "predictors = xr.concat([z, t], 'level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors_mean = predictors.mean(('time','node')).compute()\n",
    "predictors_std = predictors.std('time').mean('node').compute()\n",
    "\n",
    "const_mean = constants.mean(('time','node')).compute()\n",
    "const_std = constants.std('time').mean(('node')).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z500, t850, orog, lats, lsm, slt, rad\n",
    "in_features = 7\n",
    "out_features = 2\n",
    "ds = xr.merge([z500, t850, orog, lats, lsm, slt, rad], compat='override')\n",
    "\n",
    "ds_train = ds.sel(time=slice(*train_years))\n",
    "ds_valid = ds.sel(time=slice(*val_years))\n",
    "ds_test = ds.sel(time=slice(*test_years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = ds_train.mean(('time','node')).compute()\n",
    "train_std = ds_train.std('time').mean('node').compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define length of sequence to take into account for loss\n",
    "len_sqce = 2\n",
    "# define time resolution\n",
    "delta_t = 6\n",
    "\n",
    "description = \"all_const_len{}_delta{}\".format(len_sqce, delta_t)\n",
    "\n",
    "model_filename = model_save_path + \"spherical_unet_\" + description + \".h5\"\n",
    "pred_filename = pred_save_path + \"spherical_unet_5days\" + description + \".nc\"\n",
    "\n",
    "# predict 5days data\n",
    "max_lead_time = 5*24\n",
    "\n",
    "feature_idx = list(range(7))\n",
    "in_features = 7\n",
    "out_features = 2\n",
    "ds = xr.merge([z500, t850, orog, lats, lsm, slt, rad], compat='override')\n",
    "ds_test = ds.sel(time=slice(*test_years))\n",
    "\n",
    "train_mean_ = train_mean.to_array()[feature_idx]\n",
    "train_std_ = train_std.to_array()[feature_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_sqce = 2\n",
    "delta_t = 6\n",
    "\n",
    "description = \"all_const_len{}_delta{}\".format(len_sqce, delta_t)\n",
    "\n",
    "model_filename = model_save_path + \"spherical_unet_\" + description + \".h5\"\n",
    "pred_filename = pred_save_path + \"spherical_unet_\" + description + \".nc\"\n",
    "rmse_filename = datadir + 'metrics/rmse_' + description + '.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention:**\n",
    "\n",
    "If ```load=True``` the kernel dies. Check problem origin and if it's necessary to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validation data\n",
    "training_ds = WeatherBenchDatasetXarrayHealpixTemp(ds=ds_train, out_features=out_features,\n",
    "                                                   len_sqce=len_sqce, delta_t=delta_t, years=train_years,\n",
    "                                                   nodes=nodes, nb_timesteps=nb_timesteps, \n",
    "                                                   mean=train_mean, std=train_std, load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into RAM\n"
     ]
    }
   ],
   "source": [
    "validation_ds = WeatherBenchDatasetXarrayHealpixTemp(ds=ds_valid, out_features=out_features, \n",
    "                                                     len_sqce=len_sqce, delta_t=delta_t, years=val_years, \n",
    "                                                     nodes=nodes, nb_timesteps=nb_timesteps, \n",
    "                                                     mean=train_mean, std=train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(training_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, \n",
    "                      pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_val = DataLoader(validation_ds, batch_size=batch_size*2, shuffle=False, num_workers=num_workers, \n",
    "                    pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention:**\n",
    "\n",
    "Problem with ```pygsp.graphs``` since the module we are trying to load doesn't seem to appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pygsp.graphs' has no attribute 'SphereHealpix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3739d444a67c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m spherical_unet = UNetSphericalHealpix(N=nodes, in_channels=in_features*len_sqce, out_channels=out_features, \n\u001b[0;32m----> 3\u001b[0;31m                                       kernel_size=3)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mspherical_unet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspherical_unet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/scratch/students/bolon/weather_prediction/modules/healpix_models.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, N, in_channels, out_channels, kernel_size)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mlaplacians\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3072\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m192\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mlaplacian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compute_laplacian_healpix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mlaplacians\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlaplacian\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/scratch/students/bolon/weather_prediction/modules/healpix_models.py\u001b[0m in \u001b[0;36m_compute_laplacian_healpix\u001b[0;34m(nodes, laplacian_type)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mresolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpygsp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSphereHealpix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnside\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresolution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_laplacian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlaplacian_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mlaplacian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_laplacian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pygsp.graphs' has no attribute 'SphereHealpix'"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "spherical_unet = UNetSphericalHealpix(N=nodes, in_channels=in_features*len_sqce, out_channels=out_features, \n",
    "                                      kernel_size=3)\n",
    "spherical_unet, device = init_device(spherical_unet, gpu=gpu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test. Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "train_loss, val_loss = train_model_2steps_temp(spherical_unet, device, dl_train, epochs=nb_epochs, \n",
    "                                               lr=learning_rate, validation_data=dl_val, \n",
    "                                               model_filename=model_filename)\n",
    "torch.save(spherical_unet.state_dict(), model_filename)\n",
    "\n",
    "# Show training losses\n",
    "plt.plot(train_loss, label='Training loss')\n",
    "plt.plot(val_loss, label='Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "del dl_train, dl_val, training_ds, validation_ds\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data\n",
    "testing_ds = WeatherBenchDatasetXarrayHealpixTemp(ds=ds_test, out_features=out_features,\n",
    "                                                  len_sqce=len_sqce, delta_t=delta_t, years=test_years, \n",
    "                                                  nodes=nodes, nb_timesteps=nb_timesteps, \n",
    "                                                  mean=train_mean, std=train_std, \n",
    "                                                  max_lead_time=max_lead_time)\n",
    "\n",
    "dataloader_test = DataLoader(testing_ds, batch_size=int(0.7*batch_size), shuffle=False,\n",
    "                             num_workers=num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions\n",
    "preds = create_iterative_predictions_healpix_temp(spherical_unet, device, dataloader_test)\n",
    "preds.to_netcdf(pred_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and save RMSE\n",
    "rmse = compute_rmse_healpix(preds, obs).load()\n",
    "rmse.to_netcdf(rmse_filename)\n",
    "\n",
    "# Show RMSE\n",
    "print('Z500 - 0:', rmse.z.values[0])\n",
    "print('T850 - 0:', rmse.t.values[0])\n",
    "plot_rmses(rmse, rmses_weyn, lead_time=6)\n",
    "\n",
    "del spherical_unet, preds, rmse\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
