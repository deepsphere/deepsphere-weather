{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train best model according to report to try to reproduce results\n",
    "\n",
    "- All static features: \n",
    "    * Z500, \n",
    "    * T850, \n",
    "    * latitude, \n",
    "    * orography, \n",
    "    * land-sea mask, \n",
    "    * soil type, and \n",
    "    * top-of-atmosphere radiation\n",
    "- L=2\n",
    "- $\\Delta t$ = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/'.join(sys.path[0].split('/')[:-1]))\n",
    "\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import healpy as hp\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from modules.utils import train_model_2steps_temp, init_device\n",
    "from modules.data import WeatherBenchDatasetXarrayHealpixTemp\n",
    "from modules.healpix_models import UNetSphericalHealpix, UNetSphericalTempHealpix\n",
    "from modules.test import create_iterative_predictions_healpix_temp\n",
    "from modules.test import compute_rmse_healpix\n",
    "from modules.plotting import plot_rmses\n",
    "\n",
    "datadir = \"../data/healpix/\"\n",
    "input_dir = datadir + \"5.625deg_nearest/\"\n",
    "model_save_path = datadir + \"models/\"\n",
    "pred_save_path = datadir + \"predictions/\"\n",
    "\n",
    "if not os.path.isdir(model_save_path):\n",
    "    os.mkdir(model_save_path)\n",
    "    \n",
    "if not os.path.isdir(pred_save_path):\n",
    "    os.mkdir(pred_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define constants and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 720*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_years = ('1979', '2012')#('1979', '2012')\n",
    "val_years = ('2013', '2016')\n",
    "test_years = ('2017', '2018')\n",
    "\n",
    "nodes = 12*16*16\n",
    "max_lead_time = 5*24\n",
    "nb_timesteps = 2\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,2\"\n",
    "gpu = [0, 1]\n",
    "num_workers = 10\n",
    "pin_memory = True\n",
    "\n",
    "nb_epochs = 20\n",
    "learning_rate = 8e-3\n",
    "\n",
    "obs = xr.open_mfdataset(pred_save_path + 'observations_nearest.nc', combine='by_coords', chunks={'time':chunk_size})\n",
    "#rmses_weyn = xr.open_dataset(datadir + 'metrics/rmses_weyn.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions:\n",
    "\n",
    "**TODO**\n",
    "Check if the code is the same as the functions with the same name in ```modules/*.py``` and subtitute by imports in such a case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "z500 = xr.open_mfdataset(f'{input_dir}geopotential_500/*.nc', combine='by_coords', chunks={'time':chunk_size}).rename({'z':'z500'})\n",
    "t850 = xr.open_mfdataset(f'{input_dir}temperature_850/*.nc', combine='by_coords', chunks={'time':chunk_size}).rename({'t':'t850'})\n",
    "rad = xr.open_mfdataset(f'{input_dir}toa_incident_solar_radiation/*.nc', combine='by_coords', chunks={'time':chunk_size})\n",
    "\n",
    "z500 = z500.isel(time=slice(7, None))\n",
    "t850 = t850.isel(time=slice(7, None))\n",
    "\n",
    "constants = xr.open_dataset(f'{input_dir}constants/constants_5.625deg.nc').rename({'orography' :'orog'})\n",
    "constants = constants.assign(cos_lon=lambda x: np.cos(np.deg2rad(x.lon)))\n",
    "constants = constants.assign(sin_lon=lambda x: np.sin(np.deg2rad(x.lon)))\n",
    "\n",
    "temp = xr.DataArray(np.zeros(z500.dims['time']), coords=[('time', z500.time.values)])\n",
    "constants, _ = xr.broadcast(constants, temp)\n",
    "\n",
    "orog = constants['orog']\n",
    "lsm = constants['lsm']\n",
    "lats = constants['lat2d']\n",
    "slt = constants['slt']\n",
    "cos_lon = constants['cos_lon']\n",
    "sin_lon = constants['sin_lon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = xr.open_mfdataset(f'{input_dir}geopotential_500/*.nc', combine='by_coords', chunks={'time':chunk_size})['z']\\\n",
    ".assign_coords(level=1)\n",
    "\n",
    "t = xr.open_mfdataset(f'{input_dir}temperature_850/*.nc', combine='by_coords', chunks={'time':chunk_size})['t']\\\n",
    ".assign_coords(level=1)\n",
    "\n",
    "predictors = xr.concat([z, t], 'level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictors_mean = predictors.mean(('time','node')).compute()\n",
    "#predictors_std = predictors.std('time').mean('node').compute()\n",
    "\n",
    "#const_mean = constants.mean(('time','node')).compute()\n",
    "#const_std = constants.std('time').mean(('node')).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z500, t850, orog, lats, lsm, slt, rad\n",
    "in_features = 7\n",
    "out_features = 2\n",
    "ds = xr.merge([z500, t850, orog, lats, lsm, slt, rad], compat='override')\n",
    "\n",
    "ds_train = ds.sel(time=slice(*train_years))\n",
    "ds_valid = ds.sel(time=slice(*val_years))\n",
    "ds_test = ds.sel(time=slice(*test_years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_mean = ds_train.mean(('time','node')).compute()\n",
    "#train_std = ds_train.std('time').mean('node').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_mean.to_netcdf(f'{input_dir}mean_train_all_vars.nc')\n",
    "#train_std.to_netcdf(f'{input_dir}std_train_all_vars.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = xr.open_mfdataset(f'{input_dir}mean_train_all_vars.nc').to_array(dim='level')\n",
    "train_std = xr.open_mfdataset(f'{input_dir}std_train_all_vars.nc').to_array(dim='level')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define length of sequence to take into account for loss\n",
    "len_sqce = 2\n",
    "# define time resolution\n",
    "delta_t = 6\n",
    "\n",
    "# predict 5days data\n",
    "max_lead_time = 5*24\n",
    "\n",
    "feature_idx = list(range(7))\n",
    "in_features = 7\n",
    "out_features = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del train_mean_\n",
    "#del train_std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"all_const_len{}_delta{}\".format(len_sqce, delta_t)\n",
    "\n",
    "model_filename = model_save_path + \"spherical_unet_\" + description + \".h5\"\n",
    "pred_filename = pred_save_path + \"spherical_unet_\" + description + \".nc\"\n",
    "rmse_filename = datadir + 'metrics/rmse_' + description + '.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention:**\n",
    "\n",
    "If ```load=True``` the kernel dies. Check problem origin and if it's necessary to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_2steps_temp_old(model, device, train_generator, batch_size, epochs, lr, validation_data):    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, eps=1e-7, weight_decay=0, amsgrad=False)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    n_samples = training_ds.n_samples\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print('\\rEpoch : {}'.format(epoch), end=\"\")\n",
    "        time1 = time.time()\n",
    "        \n",
    "        val_loss = 0\n",
    "        train_loss = 0\n",
    "        \n",
    "        model.train() \n",
    "        \n",
    "        batch_idx = 0\n",
    "        for (batch, labels) in train_generator:\n",
    "            # Transfer to GPU\n",
    "            batch1 = batch[0].to(device)\n",
    "            constants1 = batch[1].to(device)\n",
    "\n",
    "            label1 = labels[0].to(device)\n",
    "            label2 = labels[1].to(device)\n",
    "\n",
    "            batch_size = batch1.shape[0]\n",
    "\n",
    "            # Model\n",
    "            output1 = model(batch1)\n",
    "            batch2 = torch.cat((batch1[:, :, 1:, :], torch.cat((output1, constants1), dim=2).unsqueeze(2)), dim=2)\n",
    "            output2 = model(batch2)\n",
    "            \n",
    "            loss = criterion(output1, label1) + criterion(output2, label2)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss = train_loss + loss.item() * batch_size\n",
    "            print('\\rBatch idx: {}; Loss: {:.3f}'.format(batch_idx, train_loss/(batch_size*(batch_idx+1))), end=\"\")\n",
    "            batch_idx += 1\n",
    "            \n",
    "        train_loss = train_loss / (len(train_generator.dataset))\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            index = 0\n",
    "            \n",
    "            for batch, labels in validation_data:\n",
    "                #batch.load()\n",
    "                # Transfer to GPU\n",
    "                batch1 = batch[0].to(device)\n",
    "                constants1 = batch[1].to(device)\n",
    "\n",
    "                label1 = labels[0].to(device)\n",
    "                label2 = labels[1].to(device)\n",
    "\n",
    "                batch_size = batch1.shape[0]\n",
    "                \n",
    "                output1 = model(batch1)\n",
    "                batch2 = torch.cat((batch1[:, :, 1:, :], torch.cat((output1, constants1), dim=2).unsqueeze(2)), dim=2)\n",
    "                output2 = model(batch2)\n",
    "                \n",
    "                val_loss = val_loss + (criterion(output1, label1).item() \n",
    "                                       + criterion(output2, label2).item()) * batch_size\n",
    "                index = index + batch_size\n",
    "                \n",
    "        val_loss = val_loss / (len(validation_data.dataset))\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        time2 = time.time()\n",
    "        \n",
    "        # Print stuff\n",
    "        print('Epoch: {e:3d}/{n_e:3d}  - loss: {l:.3f}  - val_loss: {v_l:.5f}  - time: {t:2f}'\n",
    "              .format(e=epoch+1, n_e=epochs, l=train_loss, v_l=val_loss, t=time2-time1))\n",
    "        \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_2steps_temp(model, device, training_ds, batch_size, epochs, lr, validation_data):    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, eps=1e-7, weight_decay=0, amsgrad=False)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    n_samples = training_ds.n_samples\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print('\\rEpoch : {}'.format(epoch), end=\"\")\n",
    "        time1 = time.time()\n",
    "        \n",
    "        val_loss = 0\n",
    "        train_loss = 0\n",
    "        \n",
    "        model.train() \n",
    "        \n",
    "        random.shuffle(training_ds.idxs)\n",
    "        idxs = training_ds.idxs\n",
    "        \n",
    "        batch_idx = 0\n",
    "        for i in range(0, n_samples - batch_size, batch_size):\n",
    "            i_next = min(i + batch_size, n_samples)\n",
    "            \n",
    "            print('Reading batch...')\n",
    "            batch, labels = training_ds[idxs[i:i_next]]\n",
    "            print('Done')\n",
    "        #for (batch, labels) in train_generator:\n",
    "            #print('\\rBatch idx: {}'.format(batch_idx), end=\"\")\n",
    "            # Transfer to GPU\n",
    "            #batch.load()\n",
    "            batch1 = batch[0].to(device)\n",
    "            constants1 = batch[1].to(device)\n",
    "\n",
    "            label1 = labels[0].to(device)\n",
    "            label2 = labels[1].to(device)\n",
    "\n",
    "            batch_size = batch1.shape[0]\n",
    "\n",
    "            # Model\n",
    "            print(batch1.shape)\n",
    "            output1 = model(batch1)\n",
    "            batch2 = torch.cat((batch1[:, :, 1:, :], torch.cat((output1, constants1), dim=2).unsqueeze(2)), dim=2)\n",
    "            output2 = model(batch2)\n",
    "            \n",
    "            loss = criterion(output1, label1) + criterion(output2, label2)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss = train_loss + loss.item() * batch_size\n",
    "            print('\\rBatch idx: {}; Loss: {:.3f}'.format(batch_idx, train_loss/(batch_size*(batch_idx+1))), end=\"\")\n",
    "            batch_idx += 1\n",
    "            \n",
    "        train_loss = train_loss / (len(train_generator.dataset))\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            index = 0\n",
    "            \n",
    "            for batch, labels in validation_data:\n",
    "                #batch.load()\n",
    "                # Transfer to GPU\n",
    "                batch1 = batch[0].to(device)\n",
    "                constants1 = batch[1].to(device)\n",
    "\n",
    "                label1 = labels[0].to(device)\n",
    "                label2 = labels[1].to(device)\n",
    "\n",
    "                batch_size = batch1.shape[0]\n",
    "                \n",
    "                output1 = model(batch1)\n",
    "                batch2 = torch.cat((batch1[:, :, 1:, :], torch.cat((output1, constants1), dim=2).unsqueeze(2)), dim=2)\n",
    "                output2 = model(batch2)\n",
    "                \n",
    "                val_loss = val_loss + (criterion(output1, label1).item() \n",
    "                                       + criterion(output2, label2).item()) * batch_size\n",
    "                index = index + batch_size\n",
    "                \n",
    "        val_loss = val_loss / (len(validation_data.dataset))\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        time2 = time.time()\n",
    "        \n",
    "        # Print stuff\n",
    "        print('Epoch: {e:3d}/{n_e:3d}  - loss: {l:.3f}  - val_loss: {v_l:.5f}  - time: {t:2f}'\n",
    "              .format(e=epoch+1, n_e=epochs, l=train_loss, v_l=val_loss, t=time2-time1))\n",
    "        \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherBenchDatasetXarrayHealpixTemp(Dataset):\n",
    "    \n",
    "    \"\"\" Dataset used for graph models (1D), where data is loaded from stored numpy arrays.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : xarray Dataset\n",
    "        Dataset containing the input data\n",
    "    out_features : int\n",
    "        Number of output features\n",
    "    delta_t : int\n",
    "        Temporal spacing between samples in temporal sequence (in hours)\n",
    "    len_sqce : int\n",
    "        Length of the input and output (predicted) sequences\n",
    "    years : tuple(str)\n",
    "        Years used to split the data\n",
    "    nodes : float\n",
    "        Number of nodes each sample has\n",
    "    max_lead_time : int\n",
    "        Maximum lead time (in case of iterative predictions) in hours\n",
    "    load : bool\n",
    "        If true, load dataset to RAM\n",
    "    mean : np.ndarray of shape 2\n",
    "        Mean to use for data normalization. If None, mean is computed from data\n",
    "    std : np.ndarray of shape 2\n",
    "        std to use for data normalization. If None, mean is computed from data\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, ds, out_features, delta_t, len_sqce, years, nodes, nb_timesteps, \n",
    "                 max_lead_time=None, load=True, mean=None, std=None):\n",
    "        \n",
    "        \n",
    "        self.delta_t = delta_t\n",
    "        self.len_sqce = len_sqce\n",
    "        self.years = years\n",
    "        \n",
    "        self.nodes = nodes\n",
    "        self.out_features = out_features\n",
    "        self.max_lead_time = max_lead_time\n",
    "        self.nb_timesteps = nb_timesteps\n",
    "        \n",
    "        self.data = ds.to_array(dim='level', name='Dataset').transpose('time', 'node', 'level')\n",
    "        self.in_features = self.data.shape[-1]\n",
    "        \n",
    "        self.mean = self.data.mean(('time', 'node')).compute() if mean is None else mean\n",
    "        self.std = self.data.std(('time', 'node')).compute() if std is None else std\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Count total number of samples\n",
    "        total_samples = self.data.shape[0]        \n",
    "        \n",
    "        if max_lead_time is None:\n",
    "            self.n_samples = total_samples - (len_sqce+1) * delta_t\n",
    "        else:\n",
    "            self.n_samples = total_samples - (len_sqce+1) * delta_t - max_lead_time\n",
    "        \n",
    "        # Normalize\n",
    "        self.data = (self.data - self.mean) / self.std\n",
    "        \n",
    "        # Create indexes\n",
    "        #self.idxs = [[[[sample_idx + delta_t*k for k in range(len_sqce)], sample_idx + delta_t * len_sqce], \n",
    "        #              [sample_idx + delta_t * len_sqce, sample_idx + delta_t * (len_sqce+1)]] \n",
    "        #             for sample_idx in range(self.n_samples)]\n",
    "        \n",
    "        self.idxs = np.array(range(self.n_samples))\n",
    "        \n",
    "        \n",
    "        #if load: \n",
    "        #    print('Loading data into RAM')\n",
    "        #    self.data.load()\n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Returns sample and label corresponding to an index as torch.Tensor objects\n",
    "            The return tensor shapes are (for the sample and the label): [n_vertex, len_sqce, n_features]\n",
    "        \"\"\"\n",
    "        idx_data = self.idxs[idx]\n",
    "        #1,0,2\n",
    "        X = (\n",
    "            torch.tensor([self.data.isel(time=[idx_d + self.delta_t*k for k in range(self.len_sqce)]).values for idx_d in idx_data], \\\n",
    "                         dtype=torch.float).permute(0, 2,1,3),\\\n",
    "            \n",
    "             torch.tensor([self.data.isel(time=[idx_d + self.delta_t * self.len_sqce]).values[:,:,self.out_features:] for idx_d in idx_data],\\\n",
    "                         dtype=torch.float)\n",
    "        )\n",
    "        \n",
    "        y = ( torch.tensor([self.data.isel(time=[idx_d + self.delta_t * self.len_sqce]).values[:,:,:self.out_features] for idx_d in idx_data],\\\n",
    "                         dtype=torch.float), \n",
    "             torch.tensor([self.data.isel(time=[idx_d + self.delta_t * (self.len_sqce+1)]).values[:,:,:self.out_features] for idx_d in idx_data],\\\n",
    "                         dtype=torch.float)\n",
    "        \n",
    "        )\n",
    "        \n",
    "        #X = (torch.tensor(self.data.isel(time=self.idxs[idx][0][0]).values).float().permute(1, 0, 2), \n",
    "        #     torch.tensor(self.data.isel(time=self.idxs[idx][0][1]).values[:, self.out_features:]).float())\n",
    "        \n",
    "        #y = (torch.Tensor(self.data.isel(time=self.idxs[idx][1][0]).values[:, :self.out_features]).float(), \n",
    "        #     torch.Tensor(self.data.isel(time=self.idxs[idx][1][1]).values[:, :self.out_features]).float())\n",
    "        \n",
    "        return X, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validation data\n",
    "training_ds = WeatherBenchDatasetXarrayHealpixTemp(ds=ds_train, out_features=out_features,\n",
    "                                                   len_sqce=len_sqce, delta_t=delta_t, years=train_years,\n",
    "                                                   nodes=nodes, nb_timesteps=nb_timesteps, \n",
    "                                                   mean=train_mean, std=train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_ds = WeatherBenchDatasetXarrayHealpixTemp(ds=ds_valid, out_features=out_features, \n",
    "                                                     len_sqce=len_sqce, delta_t=delta_t, years=val_years, \n",
    "                                                     nodes=nodes, nb_timesteps=nb_timesteps, \n",
    "                                                     mean=train_mean, std=train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2160"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(training_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, \n",
    "                      pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_val = DataLoader(validation_ds, batch_size=batch_size*2, shuffle=False, num_workers=num_workers, \n",
    "                    pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "#spherical_unet = UNetSphericalTempHealpix(N=nodes, len_sqce=len_sqce, in_channels=in_features, out_channels=out_features, graph_width=3)\n",
    "\n",
    "spherical_unet = UNetSphericalHealpix(N=nodes, in_channels=in_features*len_sqce, out_channels=out_features, kernel_size=3)\n",
    "spherical_unet, device = init_device(spherical_unet, gpu=gpu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test. Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "When defining ```WeatherBenchDatasetXarrayHealpixTemp```, set torch tensor to float and not float64 (double); otherwise it raises an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0Reading batch...\n",
      "Done\n",
      "torch.Size([70, 3072, 2, 7])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/bolon/miniconda3/envs/weather/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 60, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/bolon/miniconda3/envs/weather/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/mnt/scratch/students/bolon/weather_prediction/modules/healpix_models.py\", line 209, in forward\n    x_encoded = self.encode(x)\n  File \"/mnt/scratch/students/bolon/weather_prediction/modules/healpix_models.py\", line 136, in encode\n    x_enc1 = self.conv11(x)\n  File \"/home/bolon/miniconda3/envs/weather/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/mnt/scratch/students/bolon/weather_prediction/modules/healpix_models.py\", line 59, in forward\n    x = self.conv(x)\n  File \"/home/bolon/miniconda3/envs/weather/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/mnt/scratch/students/bolon/weather_prediction/modules/layers.py\", line 340, in forward\n    outputs = self._conv(self.laplacian, inputs, self.weight)\n  File \"/mnt/scratch/students/bolon/weather_prediction/modules/layers.py\", line 113, in cheb_conv\n    B, V, Fin = inputs.shape\nValueError: too many values to unpack (expected 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-8809b43e7e99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model_2steps_temp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspherical_unet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_ds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspherical_unet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-894c53dfa9d6>\u001b[0m in \u001b[0;36mtrain_model_2steps_temp\u001b[0;34m(model, device, training_ds, batch_size, epochs, lr, validation_data)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0moutput1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mbatch2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstants1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0moutput2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weather/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weather/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weather/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weather/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weather/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/bolon/miniconda3/envs/weather/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 60, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/bolon/miniconda3/envs/weather/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/mnt/scratch/students/bolon/weather_prediction/modules/healpix_models.py\", line 209, in forward\n    x_encoded = self.encode(x)\n  File \"/mnt/scratch/students/bolon/weather_prediction/modules/healpix_models.py\", line 136, in encode\n    x_enc1 = self.conv11(x)\n  File \"/home/bolon/miniconda3/envs/weather/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/mnt/scratch/students/bolon/weather_prediction/modules/healpix_models.py\", line 59, in forward\n    x = self.conv(x)\n  File \"/home/bolon/miniconda3/envs/weather/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/mnt/scratch/students/bolon/weather_prediction/modules/layers.py\", line 340, in forward\n    outputs = self._conv(self.laplacian, inputs, self.weight)\n  File \"/mnt/scratch/students/bolon/weather_prediction/modules/layers.py\", line 113, in cheb_conv\n    B, V, Fin = inputs.shape\nValueError: too many values to unpack (expected 3)\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "train_loss, val_loss = train_model_2steps_temp(model=spherical_unet, device=device, training_ds=training_ds, batch_size=batch_size, epochs=nb_epochs, lr=learning_rate, validation_data=dl_val)\n",
    "\n",
    "torch.save(spherical_unet.state_dict(), model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:**\n",
    "The model used in the other notebooks is UnetSphericalHelapix and NOT the temporal version! \n",
    "\n",
    "If the original model is used, there is a missmatch with the dimensions. Check that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "for (batch,labels) in dl_train:\n",
    "    print(batch[0].shape)\n",
    "    print(batch[1].shape)\n",
    "    print(labels[0].shape)\n",
    "    print(labels[1].shape)\n",
    "    print('Time ', time.time()-t)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "train_loss, val_loss = train_model_2steps_temp_old(spherical_unet, device, dl_train, batch_size, epochs=nb_epochs, \n",
    "                                               lr=learning_rate, validation_data=dl_val)\n",
    "torch.save(spherical_unet.state_dict(), model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ds.data.isel(time=training_ds.idxs[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ds.idxs[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training losses\n",
    "plt.plot(train_loss, label='Training loss')\n",
    "plt.plot(val_loss, label='Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "del dl_train, dl_val, training_ds, validation_ds\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data\n",
    "testing_ds = WeatherBenchDatasetXarrayHealpixTemp(ds=ds_test, out_features=out_features,\n",
    "                                                  len_sqce=len_sqce, delta_t=delta_t, years=test_years, \n",
    "                                                  nodes=nodes, nb_timesteps=nb_timesteps, \n",
    "                                                  mean=train_mean, std=train_std, \n",
    "                                                  max_lead_time=max_lead_time)\n",
    "\n",
    "dataloader_test = DataLoader(testing_ds, batch_size=int(0.7*batch_size), shuffle=False,\n",
    "                             num_workers=num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions\n",
    "preds = create_iterative_predictions_healpix_temp(spherical_unet, device, dataloader_test)\n",
    "preds.to_netcdf(pred_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and save RMSE\n",
    "rmse = compute_rmse_healpix(preds, obs).load()\n",
    "rmse.to_netcdf(rmse_filename)\n",
    "\n",
    "# Show RMSE\n",
    "print('Z500 - 0:', rmse.z.values[0])\n",
    "print('T850 - 0:', rmse.t.values[0])\n",
    "plot_rmses(rmse, rmses_weyn, lead_time=6)\n",
    "\n",
    "del spherical_unet, preds, rmse\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
