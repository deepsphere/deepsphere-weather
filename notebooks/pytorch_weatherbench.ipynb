{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_2D(Dataset):\n",
    "    \n",
    "    def __init__(self, ds, var_dict, lead_time, batch_size=32, shuffle=True, load=True, mean=None, std=None):\n",
    "        \n",
    "        self.ds = ds\n",
    "        self.var_dict = var_dict\n",
    "        #self.batch_size = batch_size\n",
    "        #self.shuffle = shuffle\n",
    "        self.lead_time = lead_time\n",
    "        self.lat = len(ds['lat'])\n",
    "        self.lon = len(ds['lon'])\n",
    "        self.features = len(var_dict)\n",
    "    \n",
    "        data = []\n",
    "        generic_level = xr.DataArray([1], coords={'level': [1]}, dims=['level'])\n",
    "        for var, levels in var_dict.items():\n",
    "            try:\n",
    "                data.append(ds[var].sel(level=levels))\n",
    "            except ValueError:\n",
    "                data.append(ds[var].expand_dims({'level': generic_level}, 1))\n",
    "\n",
    "        self.data = xr.concat(data, 'level').transpose('time', 'lat', 'lon', 'level')\n",
    "        self.mean = self.data.mean(('time', 'lat', 'lon')).compute() if mean is None else mean\n",
    "        self.std = self.data.std('time').mean(('lat', 'lon')).compute() if std is None else std\n",
    "        \n",
    "        # Normalize\n",
    "        self.data = (self.data - self.mean) / self.std\n",
    "        \n",
    "        self.n_samples = self.data.isel(time=slice(0, -lead_time)).shape[0]\n",
    "        self.idxs = np.arange(self.n_samples)\n",
    "        self.init_time = self.data.isel(time=slice(None, -lead_time)).time\n",
    "        self.valid_time = self.data.isel(time=slice(lead_time, None)).time\n",
    "        \n",
    "        if load: print('Loading data into RAM'); self.data.load()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idxs = self.idxs[idx]\n",
    "        X = torch.Tensor(self.data.isel(time=idx).values).view((self.features, self.lat, self.lon))\n",
    "        y = torch.Tensor(self.data.isel(time=idx + self.lead_time).values).view((self.features, \n",
    "                                                                                  self.lat, self.lon))\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "def load_test_data(path, var, years=slice('2017', '2018')):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        path: Path to nc files\n",
    "        var: variable. Geopotential = 'z', Temperature = 't'\n",
    "        years: slice for time window\n",
    "    Returns:\n",
    "        dataset: Concatenated dataset for 2017 and 2018\n",
    "    \"\"\"\n",
    "    assert var in ['z', 't'], 'Test data only for Z500 and T850'\n",
    "    ds = xr.open_mfdataset(f'{path}/*.nc', combine='by_coords')[var]\n",
    "    try:\n",
    "        ds = ds.sel(level=500 if var == 'z' else 850).drop('level')\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return ds.sel(time=years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeriodicConv2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.pad_width = int((self.kernel_size - 1)/2)\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, self.kernel_size, padding=0)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.conv.weight)\n",
    "        nn.init.zeros_(self.conv.bias)\n",
    "    \n",
    "    def pad(self, x):\n",
    "        padded = torch.cat((x[:, :, :, -self.pad_width:], x, x[:, :, :, :self.pad_width]), dim=3)\n",
    "        padded = F.pad(padded, (0, 0, self.pad_width, self.pad_width), 'constant', 0)\n",
    "        \n",
    "        return padded\n",
    "    \n",
    "    def forward(self, x):\n",
    "        padded = self.pad(x)\n",
    "        output = self.conv(padded)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeriodicCNN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = PeriodicConv2D(in_channels, 64, kernel_size)\n",
    "        self.conv2 = PeriodicConv2D(64, 64, kernel_size)\n",
    "        self.conv3 = PeriodicConv2D(64, 64, kernel_size)\n",
    "        self.conv4 = PeriodicConv2D(64, 64, kernel_size)\n",
    "        self.conv5 = PeriodicConv2D(64, out_channels, kernel_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward Pass.\n",
    "        Args:\n",
    "            x (:obj:`torch.Tensor`): input to be forwarded.\n",
    "        Returns:\n",
    "            :obj:`torch.Tensor`: output\n",
    "        \"\"\"\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = F.elu(self.conv4(x))\n",
    "        x = self.conv5(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictions(model, dg, mean, std):\n",
    "    \"\"\"Create non-iterative predictions\"\"\"\n",
    "    \n",
    "    outputs = []\n",
    "    for i, (sample, _) in enumerate(dg):\n",
    "        sample = sample.to(device)\n",
    "        output = model(sample).detach().cpu().clone().numpy().reshape((-1, 32, 64, 2))\n",
    "        outputs.append(output)\n",
    "    preds = np.concatenate(outputs)\n",
    "    \n",
    "    # Unnormalize\n",
    "    preds = preds * std.values + mean.values\n",
    "    das = []\n",
    "    lev_idx = 0\n",
    "    for var, levels in dg.dataset.var_dict.items():\n",
    "        if levels is None:\n",
    "            das.append(xr.DataArray(\n",
    "                preds[:, :, :, lev_idx],\n",
    "                dims=['time', 'lat', 'lon'],\n",
    "                coords={'time': dg.dataset.valid_time, 'lat': dg.dataset.data.lat, 'lon': dg.dataset.data.lon},\n",
    "                name=var\n",
    "            ))\n",
    "            lev_idx += 1\n",
    "        else:\n",
    "            nlevs = len(levels)\n",
    "            das.append(xr.DataArray(\n",
    "                preds[:, :, :, lev_idx:lev_idx+nlevs],\n",
    "                dims=['time', 'lat', 'lon', 'level'],\n",
    "                coords={'time': dg.dataset.valid_time, 'lat': dg.dataset.data.lat, 'lon': dg.dataset.data.lon, 'level': levels},\n",
    "                name=var\n",
    "            ))\n",
    "            lev_idx += nlevs\n",
    "    return xr.merge(das)\n",
    "\n",
    "def compute_weighted_rmse(da_fc, da_true, mean_dims=xr.ALL_DIMS):\n",
    "    \"\"\"\n",
    "    Compute the RMSE with latitude weighting from two xr.DataArrays.\n",
    "    Args:\n",
    "        da_fc (xr.DataArray): Forecast. Time coordinate must be validation time.\n",
    "        da_true (xr.DataArray): Truth.\n",
    "    Returns:\n",
    "        rmse: Latitude weighted root mean squared error\n",
    "    \"\"\"\n",
    "    error = da_fc - da_true\n",
    "    weights_lat = np.cos(np.deg2rad(error.lat))\n",
    "    weights_lat /= weights_lat.mean()\n",
    "    rmse = np.sqrt(((error)**2 * weights_lat).mean(mean_dims))\n",
    "    if type(rmse) is xr.Dataset:\n",
    "        rmse = rmse.rename({v: v + '_rmse' for v in rmse})\n",
    "    else: # DataArray\n",
    "        rmse.name = error.name + '_rmse' if not error.name is None else 'rmse'\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "\n",
    "class myAdam(Optimizer):\n",
    "    r\"\"\"Implements Adam algorithm.\n",
    "\n",
    "    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n",
    "\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
    "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
    "            (default: False)\n",
    "\n",
    "    .. _Adam\\: A Method for Stochastic Optimization:\n",
    "        https://arxiv.org/abs/1412.6980\n",
    "    .. _On the Convergence of Adam and Beyond:\n",
    "        https://openreview.net/forum?id=ryQu7f-RZ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-7,\n",
    "                 weight_decay=0, amsgrad=False):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "        super(myAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(myAdam, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', False)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data) #, memory_format=torch.preserve_format)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data) #, memory_format=torch.preserve_format)\n",
    "                    if amsgrad:\n",
    "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data) #, memory_format=torch.preserve_format)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad.add_(group['weight_decay'], p.data)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                if amsgrad:\n",
    "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    # Use the max. for normalizing running avg. of gradient\n",
    "                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
    "                else:\n",
    "                    denom = (exp_avg_sq.sqrt()).add_(group['eps'])\n",
    "\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, train_generator, epochs, lr, validation_data, patience):\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    beta2 = 0.999\n",
    "    epsilon = 1e-7#/(np.sqrt(1-beta2))\n",
    "    optimizer = myAdam(model.parameters(), lr=lr, eps=epsilon, weight_decay=0, amsgrad=False)\n",
    "    \n",
    "    min_val_loss = float(\"inf\")\n",
    "    epoch_no_improve = 0\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        time1 = time.time()\n",
    "        \n",
    "        val_loss = 0\n",
    "        train_loss = 0\n",
    "        \n",
    "        model.train()\n",
    "        for batch_idx, (batch, labels) in enumerate(train_generator):\n",
    "            # Transfer to GPU\n",
    "            batch, labels = batch.to(device), labels.to(device)\n",
    "            \n",
    "            batch_size = batch.shape[0]\n",
    "            \n",
    "            # Model\n",
    "            output = model(batch)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss = train_loss + loss.item() * batch_size\n",
    "            \n",
    "        train_loss = train_loss / (len(train_generator.dataset))\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            for batch, labels in validation_data:\n",
    "                # Transfer to GPU\n",
    "                batch, labels = batch.to(device), labels.to(device)\n",
    "                \n",
    "                batch_size = batch.shape[0]\n",
    "                \n",
    "                output = model(batch)\n",
    "\n",
    "                val_loss = val_loss + criterion(output, labels).item() * batch_size\n",
    "                \n",
    "        val_loss = val_loss / (len(validation_data.dataset))\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        time2 = time.time()\n",
    "        \n",
    "        # Print stuff\n",
    "        print('Epoch: {e:3d}/{n_e:3d}  - loss: {l:.3f}  - val_loss: {v_l:.5f}  - time: {t:2f}'\n",
    "              .format(e=epoch+1, n_e=epochs, l=train_loss, v_l=val_loss, t=time2-time1))\n",
    "                \n",
    "        # Check for early stopping\n",
    "        if val_loss < min_val_loss:\n",
    "            epoch_no_improve = 0\n",
    "            min_val_loss = val_loss\n",
    "        else:\n",
    "            epoch_no_improve += 1\n",
    "\n",
    "        if epoch_no_improve == patience:\n",
    "            print('Epoch {e:3d}: early stopping'.format(e=epoch+1))\n",
    "            return train_losses, val_losses\n",
    "        \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(datadir, model_save_fn, pred_save_fn, vars, kernel_size, lead_time, lr=1e-4, activation='elu', dr=0, \n",
    "         batch_size=128, patience=3, train_years=('1979', '2015'), valid_years=('2016', '2016'), \n",
    "         test_years=('2017', '2018'), gpu=1, iterative=False):\n",
    "\n",
    "    # 1. Open dataset and create data generators\n",
    "    z = xr.open_mfdataset(f'{datadir}geopotential_500/*.nc', combine='by_coords')\n",
    "    t = xr.open_mfdataset(f'{datadir}temperature_850/*.nc', combine='by_coords')\n",
    "    ds = xr.merge([z, t], compat='override')  # Override level. discarded later anyway.\n",
    "\n",
    "    ds_train = ds.sel(time=slice(*train_years))\n",
    "    ds_valid = ds.sel(time=slice(*valid_years))\n",
    "    ds_test = ds.sel(time=slice(*test_years))\n",
    "\n",
    "    dic = {var: None for var in vars}\n",
    "    \n",
    "    ################## FROM NOW ON THIS IS NOT THE SAME #############\n",
    "    \n",
    "    dataset_train = Dataset_2D(ds_train, dic, lead_time)\n",
    "    dataset_valid = Dataset_2D(ds_valid, dic, lead_time, mean=dataset_train.mean, std=dataset_train.std)\n",
    "    dataset_test = Dataset_2D(ds_test, dic, lead_time, mean=dataset_train.mean, std=dataset_train.std)\n",
    "    \n",
    "    dg_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "    dg_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "    dg_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "    \n",
    "\n",
    "    # 2. Build model and put on GPU\n",
    "    model = PeriodicCNN(in_channels=2, out_channels=2, kernel_size=kernel_size)\n",
    "    if torch.cuda.is_available():\n",
    "        if gpu is None:\n",
    "            device = torch.device(\"cuda\")\n",
    "            model = model.to(device)\n",
    "            model = nn.DataParallel(model)\n",
    "        else:\n",
    "            device = torch.device(\"cuda:{}\".format(gpu))\n",
    "            model = model.to(device)\n",
    "\n",
    "\n",
    "    # 3. Train model\n",
    "    train_loss, val_loss = train_model(model, device, dg_train, epochs=100, lr=lr, \n",
    "                                       validation_data=dg_valid, patience=patience)\n",
    "\n",
    "    print(f'Saving model weights: {model_save_fn}')\n",
    "    torch.save(model.state_dict(), model_save_fn)\n",
    "\n",
    "   # Create predictions\n",
    "    pred = create_predictions(model, dg_test, mean=dataset_train.mean, std=dataset_train.std)\n",
    "    print(f'Saving predictions: {pred_save_fn}')\n",
    "    pred.to_netcdf(pred_save_fn)\n",
    "\n",
    "    # Print score in real units\n",
    "    z500_valid = load_test_data(f'{datadir}geopotential_500', 'z')\n",
    "    t850_valid = load_test_data(f'{datadir}temperature_850', 't')\n",
    "    valid = xr.merge([z500_valid, t850_valid], compat='override')\n",
    "    print(evaluate_iterative_forecast(pred, valid).load() if iterative else \n",
    "          compute_weighted_rmse(pred, valid).load())\n",
    "    \n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/mnt/scratch/students/illorens/data/5.625deg/\"\n",
    "\n",
    "lr=1e-4\n",
    "activation='elu'\n",
    "dr=0\n",
    "batch_size=128\n",
    "patience=3\n",
    "train_years=('1979', '2015')\n",
    "valid_years=('2016', '2016')\n",
    "test_years=('2017', '2018')\n",
    "gpu=1\n",
    "iterative=False\n",
    "\n",
    "vars = ['z', 't']\n",
    "kernel_size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 day prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_fn = \"/mnt/scratch/students/illorens/data/predictions/models/torch_fccnn_3d.h5\"\n",
    "pred_save_fn = \"/mnt/scratch/students/illorens/data/predictions/torch_fccnn_3d.nc\"\n",
    "lead_time = 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Open dataset and create data generators\n",
    "z = xr.open_mfdataset(f'{datadir}geopotential_500/*.nc', combine='by_coords')\n",
    "t = xr.open_mfdataset(f'{datadir}temperature_850/*.nc', combine='by_coords')\n",
    "ds = xr.merge([z, t], compat='override')  # Override level. discarded later anyway.\n",
    "\n",
    "ds_train = ds.sel(time=slice(*train_years))\n",
    "ds_valid = ds.sel(time=slice(*valid_years))\n",
    "ds_test = ds.sel(time=slice(*test_years))\n",
    "\n",
    "dic = {var: None for var in vars}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FROM NOW ON THIS IS NOT THE SAME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into RAM\n",
      "Loading data into RAM\n",
      "Loading data into RAM\n"
     ]
    }
   ],
   "source": [
    "dataset_train = Dataset_2D(ds_train, dic, lead_time)\n",
    "dataset_valid = Dataset_2D(ds_valid, dic, lead_time, mean=dataset_train.mean, std=dataset_train.std)\n",
    "dataset_test = Dataset_2D(ds_test, dic, lead_time, mean=dataset_train.mean, std=dataset_train.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "dg_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "dg_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Build model and put on GPU\n",
    "model = PeriodicCNN(in_channels=2, out_channels=2, kernel_size=kernel_size)\n",
    "if torch.cuda.is_available():\n",
    "    if gpu is None:\n",
    "        device = torch.device(\"cuda\")\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "    else:\n",
    "        device = torch.device(\"cuda:{}\".format(gpu))\n",
    "        model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1/100  - loss: 0.420  - val_loss: 0.45924  - time: 440.947056\n",
      "Epoch:   2/100  - loss: 0.419  - val_loss: 0.45808  - time: 418.733010\n",
      "Epoch:   3/100  - loss: 0.419  - val_loss: 0.45883  - time: 402.845594\n",
      "Epoch:   4/100  - loss: 0.418  - val_loss: 0.46097  - time: 413.816336\n",
      "Epoch:   5/100  - loss: 0.418  - val_loss: 0.46014  - time: 433.962185\n",
      "Epoch   5: early stopping\n",
      "Saving model weights: /mnt/scratch/students/illorens/data/predictions/models/torch_fccnn_3d.h5\n"
     ]
    }
   ],
   "source": [
    "# 3. Train model\n",
    "train_loss, val_loss = train_model(model, device, dg_train, epochs=100, lr=lr, \n",
    "                                   validation_data=dg_valid, patience=patience)\n",
    "\n",
    "print(f'Saving model weights: {model_save_fn}')\n",
    "torch.save(model.state_dict(), model_save_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss, label='Training loss')\n",
    "plt.plot(val_loss, label='Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predictions: /mnt/scratch/students/illorens/data/predictions/torch_fccnn_3d.nc\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Coordinates:\n",
      "    level    int32 500\n",
      "Data variables:\n",
      "    z_rmse   float64 702.4\n",
      "    t_rmse   float64 3.153\n"
     ]
    }
   ],
   "source": [
    "# Create predictions\n",
    "pred = create_predictions(model, dg_test, mean=dataset_train.mean, std=dataset_train.std)\n",
    "print(f'Saving predictions: {pred_save_fn}')\n",
    "pred.to_netcdf(pred_save_fn)\n",
    "\n",
    "# Print score in real units\n",
    "z500_valid = load_test_data(f'{datadir}geopotential_500', 'z')\n",
    "t850_valid = load_test_data(f'{datadir}temperature_850', 't')\n",
    "valid = xr.merge([z500_valid, t850_valid], compat='override')\n",
    "print(evaluate_iterative_forecast(pred, valid).load() if iterative else compute_weighted_rmse(pred, valid).load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 day prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_fn_5d = \"/mnt/scratch/students/illorens/data/predictions/models/torch_fccnn_5d.h5\"\n",
    "pred_save_fn_5d = \"/mnt/scratch/students/illorens/data/predictions/torch_fccnn_5d.nc\"\n",
    "lead_time_5d = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_5d, val_loss_5d = main(datadir, model_save_fn_5d, pred_save_fn_5d, vars, kernel_size, lead_time_5d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss, label='Training loss')\n",
    "plt.plot(val_loss, label='Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
