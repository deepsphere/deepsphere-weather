{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_2D(Dataset):\n",
    "    \n",
    "    def __init__(self, ds, var_dict, lead_time, batch_size=32, shuffle=True, load=True, mean=None, std=None):\n",
    "        \n",
    "        self.ds = ds\n",
    "        self.var_dict = var_dict\n",
    "        #self.batch_size = batch_size\n",
    "        #self.shuffle = shuffle\n",
    "        self.lead_time = lead_time\n",
    "        self.lat = len(ds['lat'])\n",
    "        self.lon = len(ds['lon'])\n",
    "        self.features = len(var_dict)\n",
    "    \n",
    "        data = []\n",
    "        generic_level = xr.DataArray([1], coords={'level': [1]}, dims=['level'])\n",
    "        for var, levels in var_dict.items():\n",
    "            try:\n",
    "                data.append(ds[var].sel(level=levels))\n",
    "            except ValueError:\n",
    "                data.append(ds[var].expand_dims({'level': generic_level}, 1))\n",
    "\n",
    "        self.data = xr.concat(data, 'level').transpose('time', 'lat', 'lon', 'level')\n",
    "        self.mean = self.data.mean(('time', 'lat', 'lon')).compute() if mean is None else mean\n",
    "        self.std = self.data.std('time').mean(('lat', 'lon')).compute() if std is None else std\n",
    "        \n",
    "        # Normalize\n",
    "        self.data = (self.data - self.mean) / self.std\n",
    "        \n",
    "        self.n_samples = self.data.isel(time=slice(0, -lead_time)).shape[0]\n",
    "        self.idxs = np.arange(self.n_samples)\n",
    "        self.init_time = self.data.isel(time=slice(None, -lead_time)).time\n",
    "        self.valid_time = self.data.isel(time=slice(lead_time, None)).time\n",
    "        \n",
    "        if load: print('Loading data into RAM'); self.data.load()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idxs = self.idxs[idx]\n",
    "        X = torch.Tensor(self.data.isel(time=idxs).values).view((self.features, self.lat, self.lon))\n",
    "        y = torch.Tensor(self.data.isel(time=idxs + self.lead_time).values).view((self.features, \n",
    "                                                                                  self.lat, self.lon))\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "def load_test_data(path, var, years=slice('2017', '2018')):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        path: Path to nc files\n",
    "        var: variable. Geopotential = 'z', Temperature = 't'\n",
    "        years: slice for time window\n",
    "    Returns:\n",
    "        dataset: Concatenated dataset for 2017 and 2018\n",
    "    \"\"\"\n",
    "    assert var in ['z', 't'], 'Test data only for Z500 and T850'\n",
    "    ds = xr.open_mfdataset(f'{path}/*.nc', combine='by_coords')[var]\n",
    "    try:\n",
    "        ds = ds.sel(level=500 if var == 'z' else 850).drop('level')\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return ds.sel(time=years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeriodicConv2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.pad_width = int((self.kernel_size - 1)/2)\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, self.kernel_size, padding=0)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.conv.weight)\n",
    "        nn.init.zeros_(self.conv.bias)\n",
    "    \n",
    "    def pad(self, x):\n",
    "        padded = torch.cat((x[:, :, :, -self.pad_width:], x, x[:, :, :, :self.pad_width]), dim=3)\n",
    "        padded = F.pad(padded, (0, 0, self.pad_width, self.pad_width), 'constant', 0)\n",
    "        \n",
    "        return padded\n",
    "    \n",
    "    def forward(self, x):\n",
    "        padded = self.pad(x)\n",
    "        output = self.conv(padded)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeriodicCNN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = PeriodicConv2D(in_channels, 64, kernel_size)\n",
    "        self.conv2 = PeriodicConv2D(64, 64, kernel_size)\n",
    "        self.conv3 = PeriodicConv2D(64, 64, kernel_size)\n",
    "        self.conv4 = PeriodicConv2D(64, 64, kernel_size)\n",
    "        self.conv5 = PeriodicConv2D(64, out_channels, kernel_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward Pass.\n",
    "        Args:\n",
    "            x (:obj:`torch.Tensor`): input to be forwarded.\n",
    "        Returns:\n",
    "            :obj:`torch.Tensor`: output\n",
    "        \"\"\"\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = F.elu(self.conv4(x))\n",
    "        x = self.conv5(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictions(model, dg, mean, std):\n",
    "    \"\"\"Create non-iterative predictions\"\"\"\n",
    "    \n",
    "    outputs = []\n",
    "    for i, (sample, _) in enumerate(dg):\n",
    "        sample = sample.to(device)\n",
    "        output = model(sample).detach().cpu().clone().numpy().reshape((-1, 32, 64, 2))\n",
    "        outputs.append(output)\n",
    "    preds = np.concatenate(outputs)\n",
    "    \n",
    "    # Unnormalize\n",
    "    preds = preds * std.values + mean.values\n",
    "    das = []\n",
    "    lev_idx = 0\n",
    "    for var, levels in dg.dataset.var_dict.items():\n",
    "        if levels is None:\n",
    "            das.append(xr.DataArray(\n",
    "                preds[:, :, :, lev_idx],\n",
    "                dims=['time', 'lat', 'lon'],\n",
    "                coords={'time': dg.dataset.valid_time, 'lat': dg.dataset.data.lat, 'lon': dg.dataset.data.lon},\n",
    "                name=var\n",
    "            ))\n",
    "            lev_idx += 1\n",
    "        else:\n",
    "            nlevs = len(levels)\n",
    "            das.append(xr.DataArray(\n",
    "                preds[:, :, :, lev_idx:lev_idx+nlevs],\n",
    "                dims=['time', 'lat', 'lon', 'level'],\n",
    "                coords={'time': dg.dataset.valid_time, 'lat': dg.dataset.data.lat, 'lon': dg.dataset.data.lon, 'level': levels},\n",
    "                name=var\n",
    "            ))\n",
    "            lev_idx += nlevs\n",
    "    return xr.merge(das)\n",
    "\n",
    "def compute_weighted_rmse(da_fc, da_true, mean_dims=xr.ALL_DIMS):\n",
    "    \"\"\"\n",
    "    Compute the RMSE with latitude weighting from two xr.DataArrays.\n",
    "    Args:\n",
    "        da_fc (xr.DataArray): Forecast. Time coordinate must be validation time.\n",
    "        da_true (xr.DataArray): Truth.\n",
    "    Returns:\n",
    "        rmse: Latitude weighted root mean squared error\n",
    "    \"\"\"\n",
    "    error = da_fc - da_true\n",
    "    weights_lat = np.cos(np.deg2rad(error.lat))\n",
    "    weights_lat /= weights_lat.mean()\n",
    "    rmse = np.sqrt(((error)**2 * weights_lat).mean(mean_dims))\n",
    "    if type(rmse) is xr.Dataset:\n",
    "        rmse = rmse.rename({v: v + '_rmse' for v in rmse})\n",
    "    else: # DataArray\n",
    "        rmse.name = error.name + '_rmse' if not error.name is None else 'rmse'\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "\n",
    "class myAdam(Optimizer):\n",
    "    r\"\"\"Implements Adam algorithm.\n",
    "\n",
    "    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n",
    "\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
    "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
    "            (default: False)\n",
    "\n",
    "    .. _Adam\\: A Method for Stochastic Optimization:\n",
    "        https://arxiv.org/abs/1412.6980\n",
    "    .. _On the Convergence of Adam and Beyond:\n",
    "        https://openreview.net/forum?id=ryQu7f-RZ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-7,\n",
    "                 weight_decay=0, amsgrad=False):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "        super(myAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(myAdam, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', False)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data) #, memory_format=torch.preserve_format)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data) #, memory_format=torch.preserve_format)\n",
    "                    if amsgrad:\n",
    "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data) #, memory_format=torch.preserve_format)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad.add_(group['weight_decay'], p.data)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                if amsgrad:\n",
    "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    # Use the max. for normalizing running avg. of gradient\n",
    "                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
    "                else:\n",
    "                    denom = (exp_avg_sq.sqrt()).add_(group['eps'])\n",
    "\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, train_generator, epochs, lr, validation_data, patience):\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, eps=1e-7, weight_decay=0, amsgrad=False)\n",
    "    #optim.Adam(model.parameters(), lr=lr, eps=1e-7, weight_decay=0, amsgrad=False)\n",
    "    \n",
    "    min_val_loss = 1e15\n",
    "    wait = 0\n",
    "    stopped_epoch = 0\n",
    "    stop_training = False\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        time1 = time.time()\n",
    "        \n",
    "        val_loss = 0\n",
    "        train_loss = 0\n",
    "        \n",
    "        model.train()\n",
    "        for batch_idx, (batch, labels) in enumerate(train_generator):\n",
    "            # Transfer to GPU\n",
    "            batch, labels = batch.to(device), labels.to(device)\n",
    "            \n",
    "            batch_size = batch.shape[0]\n",
    "            \n",
    "            # Model\n",
    "            output = model(batch)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss = train_loss + loss.item() * batch_size\n",
    "            \n",
    "        train_loss = train_loss / (len(train_generator.dataset))\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            for batch, labels in validation_data:\n",
    "                # Transfer to GPU\n",
    "                batch, labels = batch.to(device), labels.to(device)\n",
    "                \n",
    "                batch_size = batch.shape[0]\n",
    "                \n",
    "                output = model(batch)\n",
    "\n",
    "                val_loss = val_loss + criterion(output, labels).item() * batch_size\n",
    "                \n",
    "        val_loss = val_loss / (len(validation_data.dataset))\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        time2 = time.time()\n",
    "        \n",
    "        # Print stuff\n",
    "        print('Epoch: {e:3d}/{n_e:3d}  - loss: {l:.3f}  - val_loss: {v_l:.5f}  - time: {t:2f}'\n",
    "              .format(e=epoch+1, n_e=epochs, l=train_loss, v_l=val_loss, t=time2-time1))\n",
    "                \n",
    "            \n",
    "        if (val_loss - min_val_loss) < 0:\n",
    "            min_val_loss = val_loss\n",
    "            wait = 0\n",
    "        else:\n",
    "            if wait >= patience:\n",
    "                stopped_epoch = epoch + 1\n",
    "                stop_training = True\n",
    "            wait += 1\n",
    "        \n",
    "        if stop_training:\n",
    "            print('Epoch {e:3d}: early stopping'.format(e=stopped_epoch))\n",
    "            return train_losses, val_losses\n",
    "        \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(datadir, model_save_fn, pred_save_fn, vars, kernel_size, lead_time, lr=1e-4, activation='elu', dr=0, \n",
    "         batch_size=128, patience=3, train_years=('1979', '2015'), valid_years=('2016', '2016'), \n",
    "         test_years=('2017', '2018'), gpu=1, iterative=False):\n",
    "\n",
    "    # 1. Open dataset and create data generators\n",
    "    z = xr.open_mfdataset(f'{datadir}geopotential_500/*.nc', combine='by_coords')\n",
    "    t = xr.open_mfdataset(f'{datadir}temperature_850/*.nc', combine='by_coords')\n",
    "    ds = xr.merge([z, t], compat='override')  # Override level. discarded later anyway.\n",
    "\n",
    "    ds_train = ds.sel(time=slice(*train_years))\n",
    "    ds_valid = ds.sel(time=slice(*valid_years))\n",
    "    ds_test = ds.sel(time=slice(*test_years))\n",
    "\n",
    "    dic = {var: None for var in vars}\n",
    "    \n",
    "    ################## FROM NOW ON THIS IS NOT THE SAME #############\n",
    "    \n",
    "    dataset_train = Dataset_2D(ds_train, dic, lead_time)\n",
    "    dataset_valid = Dataset_2D(ds_valid, dic, lead_time, mean=dataset_train.mean, std=dataset_train.std)\n",
    "    dataset_test = Dataset_2D(ds_test, dic, lead_time, mean=dataset_train.mean, std=dataset_train.std)\n",
    "    \n",
    "    dg_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "    dg_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "    dg_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "    \n",
    "\n",
    "    # 2. Build model and put on GPU\n",
    "    model = PeriodicCNN(in_channels=2, out_channels=2, kernel_size=kernel_size)\n",
    "    if torch.cuda.is_available():\n",
    "        if gpu is None:\n",
    "            device = torch.device(\"cuda\")\n",
    "            model = model.to(device)\n",
    "            model = nn.DataParallel(model)\n",
    "        else:\n",
    "            device = torch.device(\"cuda:{}\".format(gpu))\n",
    "            model = model.to(device)\n",
    "\n",
    "\n",
    "    # 3. Train model\n",
    "    train_loss, val_loss = train_model(model, device, dg_train, epochs=100, lr=lr, \n",
    "                                       validation_data=dg_valid, patience=patience)\n",
    "\n",
    "    print(f'Saving model weights: {model_save_fn}')\n",
    "    torch.save(model.state_dict(), model_save_fn)\n",
    "\n",
    "   # Create predictions\n",
    "    pred = create_predictions(model, dg_test, mean=dataset_train.mean, std=dataset_train.std)\n",
    "    print(f'Saving predictions: {pred_save_fn}')\n",
    "    pred.to_netcdf(pred_save_fn)\n",
    "\n",
    "    # Print score in real units\n",
    "    z500_valid = load_test_data(f'{datadir}geopotential_500', 'z')\n",
    "    t850_valid = load_test_data(f'{datadir}temperature_850', 't')\n",
    "    valid = xr.merge([z500_valid, t850_valid], compat='override')\n",
    "    print(evaluate_iterative_forecast(pred, valid).load() if iterative else \n",
    "          compute_weighted_rmse(pred, valid).load())\n",
    "    \n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/mnt/scratch/students/illorens/data/5.625deg/\"\n",
    "\n",
    "lr=1e-4\n",
    "activation='elu'\n",
    "dr=0\n",
    "batch_size=128\n",
    "patience=3\n",
    "train_years=('1979', '2015')\n",
    "valid_years=('2016', '2016')\n",
    "test_years=('2017', '2018')\n",
    "gpu=1\n",
    "iterative=False\n",
    "\n",
    "vars = ['z', 't']\n",
    "kernel_size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 day prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_fn = \"/mnt/scratch/students/illorens/data/predictions/models/torch_fccnn_3d.h5\"\n",
    "pred_save_fn = \"/mnt/scratch/students/illorens/data/predictions/torch_fccnn_3d.nc\"\n",
    "lead_time = 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Open dataset and create data generators\n",
    "z = xr.open_mfdataset(f'{datadir}geopotential_500/*.nc', combine='by_coords')\n",
    "t = xr.open_mfdataset(f'{datadir}temperature_850/*.nc', combine='by_coords')\n",
    "ds = xr.merge([z, t], compat='override')  # Override level. discarded later anyway.\n",
    "\n",
    "ds_train = ds.sel(time=slice(*train_years))\n",
    "ds_valid = ds.sel(time=slice(*valid_years))\n",
    "ds_test = ds.sel(time=slice(*test_years))\n",
    "\n",
    "dic = {var: None for var in vars}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FROM NOW ON THIS IS NOT THE SAME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into RAM\n",
      "Loading data into RAM\n",
      "Loading data into RAM\n"
     ]
    }
   ],
   "source": [
    "dataset_train = Dataset_2D(ds_train, dic, lead_time)\n",
    "dataset_valid = Dataset_2D(ds_valid, dic, lead_time, mean=dataset_train.mean, std=dataset_train.std)\n",
    "dataset_test = Dataset_2D(ds_test, dic, lead_time, mean=dataset_train.mean, std=dataset_train.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "dg_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "dg_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Build model and put on GPU\n",
    "model = PeriodicCNN(in_channels=2, out_channels=2, kernel_size=kernel_size)\n",
    "if torch.cuda.is_available():\n",
    "    if gpu is None:\n",
    "        device = torch.device(\"cuda\")\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "    else:\n",
    "        device = torch.device(\"cuda:{}\".format(gpu))\n",
    "        model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1/100  - loss: 0.558  - val_loss: 0.52641  - time: 418.183556\n",
      "Epoch:   2/100  - loss: 0.517  - val_loss: 0.51401  - time: 417.826171\n",
      "Epoch:   3/100  - loss: 0.505  - val_loss: 0.50795  - time: 418.860838\n",
      "Epoch:   4/100  - loss: 0.497  - val_loss: 0.50516  - time: 418.327489\n",
      "Epoch:   5/100  - loss: 0.491  - val_loss: 0.50021  - time: 416.372558\n",
      "Epoch:   6/100  - loss: 0.486  - val_loss: 0.49648  - time: 415.465974\n",
      "Epoch:   7/100  - loss: 0.482  - val_loss: 0.48962  - time: 419.261449\n",
      "Epoch:   8/100  - loss: 0.479  - val_loss: 0.48886  - time: 416.700528\n",
      "Epoch:   9/100  - loss: 0.475  - val_loss: 0.48450  - time: 417.674933\n",
      "Epoch:  10/100  - loss: 0.473  - val_loss: 0.48325  - time: 420.157391\n",
      "Epoch:  11/100  - loss: 0.470  - val_loss: 0.48248  - time: 418.669121\n",
      "Epoch:  12/100  - loss: 0.467  - val_loss: 0.47766  - time: 418.484510\n",
      "Epoch:  13/100  - loss: 0.465  - val_loss: 0.47734  - time: 419.596740\n",
      "Epoch:  14/100  - loss: 0.463  - val_loss: 0.47575  - time: 419.760337\n"
     ]
    }
   ],
   "source": [
    "# 3. Train model\n",
    "train_loss, val_loss = train_model(model, device, dg_train, epochs=100, lr=lr, \n",
    "                                   validation_data=dg_valid, patience=patience)\n",
    "\n",
    "print(f'Saving model weights: {model_save_fn}')\n",
    "torch.save(model.state_dict(), model_save_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss, label='Training loss')\n",
    "plt.plot(val_loss, label='Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predictions: /mnt/scratch/students/illorens/data/predictions/torch_fccnn_3d.nc\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Coordinates:\n",
      "    level    int32 500\n",
      "Data variables:\n",
      "    z_rmse   float64 700.0\n",
      "    t_rmse   float64 3.156\n"
     ]
    }
   ],
   "source": [
    "# Create predictions\n",
    "pred = create_predictions(model, dg_test, mean=dataset_train.mean, std=dataset_train.std)\n",
    "print(f'Saving predictions: {pred_save_fn}')\n",
    "pred.to_netcdf(pred_save_fn)\n",
    "\n",
    "# Print score in real units\n",
    "z500_valid = load_test_data(f'{datadir}geopotential_500', 'z')\n",
    "t850_valid = load_test_data(f'{datadir}temperature_850', 't')\n",
    "valid = xr.merge([z500_valid, t850_valid], compat='override')\n",
    "print(evaluate_iterative_forecast(pred, valid).load() if iterative else compute_weighted_rmse(pred, valid).load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 day prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_fn_5d = \"/mnt/scratch/students/illorens/data/predictions/models/torch_fccnn_5d.h5\"\n",
    "pred_save_fn_5d = \"/mnt/scratch/students/illorens/data/predictions/torch_fccnn_5d.nc\"\n",
    "lead_time_5d = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_5d, val_loss_5d = main(datadir, model_save_fn_5d, pred_save_fn_5d, vars, kernel_size, lead_time_5d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss, label='Training loss')\n",
    "plt.plot(val_loss, label='Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
