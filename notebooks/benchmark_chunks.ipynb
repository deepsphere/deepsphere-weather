{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check influence of different parameters in performance. \n",
    "\n",
    "Check influence of:\n",
    "- specifying different chunk sizes and chunking along different dimensions\n",
    "- use already standardized data --> does it save memory?\n",
    "- use ``` .persist()``` to load data in a distributed way and speed up reading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 483"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/'.join(sys.path[0].split('/')[:-1]))\n",
    "\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import healpy as hp\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from modules.utils import train_model_2steps, init_device\n",
    "from modules.data import WeatherBenchDatasetXarrayHealpix\n",
    "from modules.healpix_models import UNetSphericalHealpix\n",
    "from modules.test import create_iterative_predictions_healpix\n",
    "from modules.test import compute_rmse_healpix\n",
    "from modules.plotting import plot_rmses\n",
    "\n",
    "datadir = \"../data/healpix/\"\n",
    "input_dir = datadir + \"5.625deg_nearest/\"\n",
    "model_save_path = datadir + \"models/\"\n",
    "pred_save_path = datadir + \"predictions/\"\n",
    "\n",
    "train_years = ('1979', '2012')\n",
    "val_years = ('2013', '2016')\n",
    "test_years = ('2017', '2018')\n",
    "\n",
    "nodes = 12*16*16\n",
    "max_lead_time = 5*24\n",
    "lead_time = 6\n",
    "out_features = 2\n",
    "nb_timesteps = 2\n",
    "len_sqce = 2\n",
    "#Â define time resolution\n",
    "delta_t = 6\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,4\"\n",
    "gpu = [0,1]\n",
    "num_workers = 10\n",
    "pin_memory = True\n",
    "batch_size = 95\n",
    "\n",
    "nb_epochs = 10\n",
    "learning_rate = 8e-3\n",
    "\n",
    "obs = xr.open_mfdataset(pred_save_path + 'observations_nearest.nc', combine='by_coords', chunks={'time':483})\n",
    "#rmses_weyn = xr.open_dataset(datadir + 'metrics/rmses_weyn.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.data import WeatherBenchDatasetIterative\n",
    "class WeatherBenchDatasetXarrayHealpixTemp(Dataset):\n",
    "    \n",
    "    \"\"\" Dataset used for graph models (1D), where data is loaded from stored numpy arrays.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : xarray Dataset\n",
    "        Dataset containing the input data\n",
    "    out_features : int\n",
    "        Number of output features\n",
    "    delta_t : int\n",
    "        Temporal spacing between samples in temporal sequence (in hours)\n",
    "    len_sqce : int\n",
    "        Length of the input and output (predicted) sequences\n",
    "    years : tuple(str)\n",
    "        Years used to split the data\n",
    "    nodes : float\n",
    "        Number of nodes each sample has\n",
    "    max_lead_time : int\n",
    "        Maximum lead time (in case of iterative predictions) in hours\n",
    "    load : bool\n",
    "        If true, load dataset to RAM\n",
    "    mean : np.ndarray of shape 2\n",
    "        Mean to use for data normalization. If None, mean is computed from data\n",
    "    std : np.ndarray of shape 2\n",
    "        std to use for data normalization. If None, mean is computed from data\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, ds, out_features, delta_t, len_sqce, years, nodes, nb_timesteps, \n",
    "                 max_lead_time=None, load=True, mean=None, std=None):\n",
    "        \n",
    "        \n",
    "        self.delta_t = delta_t\n",
    "        self.len_sqce = len_sqce\n",
    "        self.years = years\n",
    "        \n",
    "        self.nodes = nodes\n",
    "        self.out_features = out_features\n",
    "        self.max_lead_time = max_lead_time\n",
    "        self.nb_timesteps = nb_timesteps\n",
    "        \n",
    "        self.data = ds.to_array(dim='level', name='Dataset').transpose('time', 'node', 'level')\n",
    "        self.in_features = self.data.shape[-1]\n",
    "        \n",
    "        self.mean = self.data.mean(('time', 'node')).compute() if mean is None else mean\n",
    "        self.std = self.data.std(('time', 'node')).compute() if std is None else std\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Count total number of samples\n",
    "        total_samples = self.data.shape[0]        \n",
    "        \n",
    "        if max_lead_time is None:\n",
    "            self.n_samples = total_samples - (len_sqce+1) * delta_t\n",
    "        else:\n",
    "            self.n_samples = total_samples - (len_sqce+1) * delta_t - max_lead_time\n",
    "        \n",
    "        # Normalize\n",
    "        self.data = (self.data - self.mean.to_array(dim='level')) / self.std.to_array(dim='level')\n",
    "        \n",
    "        # Create indexes\n",
    "        #self.idxs = [[[[sample_idx + delta_t*k for k in range(len_sqce)], sample_idx + delta_t * len_sqce], \n",
    "        #              [sample_idx + delta_t * len_sqce, sample_idx + delta_t * (len_sqce+1)]] \n",
    "        #             for sample_idx in range(self.n_samples)]\n",
    "        \n",
    "        self.idxs = np.array(range(self.n_samples))\n",
    "        \n",
    "        \n",
    "        #if load: \n",
    "        #    print('Loading data into RAM')\n",
    "        #    self.data.load()\n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Returns sample and label corresponding to an index as torch.Tensor objects\n",
    "            The return tensor shapes are (for the sample and the label): [n_vertex, len_sqce, n_features]\n",
    "            \n",
    "            X = (\n",
    "            torch.tensor([self.data.isel(time=[idx_d + self.delta_t*k for k in range(self.len_sqce)]).values for idx_d in idx_data], \\\n",
    "                         dtype=torch.float).permute(0, 2,1,3).reshape(len(idx), self.nodes, -1),\\\n",
    "            \n",
    "             torch.tensor([self.data.isel(time=[idx_d + self.delta_t * self.len_sqce]).values[:,:,self.out_features:] for idx_d in idx_data],\\\n",
    "                         dtype=torch.float).reshape(len(idx), self.nodes, -1)\n",
    "        )\n",
    "        \"\"\"\n",
    "        idx_data = self.idxs[idx]\n",
    "        #1,0,2\n",
    "        \n",
    "        #batch[0] --> (batch_size, num_nodes, n_features*len_sq)\n",
    "        X = (\n",
    "            torch.tensor([self.data.isel(time=[idx_d + self.delta_t]).values for idx_d in idx_data], \\\n",
    "                         dtype=torch.float).permute(0, 2,1,3).reshape(len(idx), self.nodes, -1),\\\n",
    "            \n",
    "             torch.tensor([self.data.isel(time=[idx_d + self.delta_t * self.len_sqce]).values[:,:,self.out_features:] for idx_d in idx_data],\\\n",
    "                         dtype=torch.float).reshape(len(idx), self.nodes, -1)\n",
    "        )\n",
    "        \n",
    "        y = ( torch.tensor([self.data.isel(time=[idx_d + self.delta_t * self.len_sqce]).values[:,:,:self.out_features] for idx_d in idx_data],\\\n",
    "                         dtype=torch.float).reshape(len(idx), self.nodes, -1), \n",
    "             torch.tensor([self.data.isel(time=[idx_d + self.delta_t * (self.len_sqce+1)]).values[:,:,:self.out_features] for idx_d in idx_data],\\\n",
    "                         dtype=torch.float).reshape(len(idx), self.nodes, -1)\n",
    "        \n",
    "        )\n",
    "        \n",
    "        #X = (torch.tensor(self.data.isel(time=self.idxs[idx][0][0]).values).float().permute(1, 0, 2), \n",
    "        #     torch.tensor(self.data.isel(time=self.idxs[idx][0][1]).values[:, self.out_features:]).float())\n",
    "        \n",
    "        #y = (torch.Tensor(self.data.isel(time=self.idxs[idx][1][0]).values[:, :self.out_features]).float(), \n",
    "        #     torch.Tensor(self.data.isel(time=self.idxs[idx][1][1]).values[:, :self.out_features]).float())\n",
    "        \n",
    "        return X, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "z500 = xr.open_mfdataset(f'{input_dir}geopotential_500/*.nc', combine='by_coords', chunks={'time':chunk_size}).rename({'z':'z500'})\n",
    "t850 = xr.open_mfdataset(f'{input_dir}temperature_850/*.nc', combine='by_coords', chunks={'time':chunk_size}).rename({'t':'t850'})\n",
    "rad = xr.open_mfdataset(f'{input_dir}toa_incident_solar_radiation/*.nc', combine='by_coords', chunks={'time':chunk_size})\n",
    "\n",
    "z500 = z500.isel(time=slice(7, None))\n",
    "t850 = t850.isel(time=slice(7, None))\n",
    "\n",
    "constants = xr.open_dataset(f'{input_dir}constants/constants_5.625deg.nc').rename({'orography' :'orog'})\n",
    "constants = constants.assign(cos_lon=lambda x: np.cos(np.deg2rad(x.lon)))\n",
    "constants = constants.assign(sin_lon=lambda x: np.sin(np.deg2rad(x.lon)))\n",
    "\n",
    "temp = xr.DataArray(np.zeros(z500.dims['time']), coords=[('time', z500.time.values)])\n",
    "constants, _ = xr.broadcast(constants, temp)\n",
    "\n",
    "orog = constants['orog']\n",
    "lsm = constants['lsm']\n",
    "lats = constants['lat2d']\n",
    "slt = constants['slt']\n",
    "cos_lon = constants['cos_lon']\n",
    "sin_lon = constants['sin_lon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#description = \"no_const\"\n",
    "description = \"all_const\"\n",
    "\n",
    "model_filename = model_save_path + \"spherical_unet_\" + description + \".h5\"\n",
    "pred_filename = pred_save_path + \"spherical_unet_\" + description + \".nc\"\n",
    "rmse_filename = datadir + 'metrics/rmse_' + description + '.nc'\n",
    "\n",
    "# z500, t850, orog, lats, lsm, slt, rad\n",
    "#feature_idx = [0, 1]\n",
    "in_features = 7 #len(feature_idx)\n",
    "#ds = xr.merge([z500, t850], compat='override')\n",
    "ds = xr.merge([z500, t850, orog, lats, lsm, slt, rad], compat='override')\n",
    "\n",
    "ds_train = ds.sel(time=slice(*train_years))\n",
    "ds_valid = ds.sel(time=slice(*val_years))\n",
    "ds_test = ds.sel(time=slice(*test_years))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean_ = xr.open_mfdataset(f'{input_dir}mean_train_full_all_const.nc')\n",
    "train_std_ = xr.open_mfdataset(f'{input_dir}mean_train_full_all_const.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validation data\n",
    "training_ds = WeatherBenchDatasetXarrayHealpixTemp(ds=ds_train, out_features=out_features, delta_t=delta_t,\n",
    "                                                   len_sqce=len_sqce, max_lead_time=max_lead_time,\n",
    "                                                   years=train_years, nodes=nodes, nb_timesteps=nb_timesteps, \n",
    "                                                   mean=train_mean_, std=train_std_, load=False)\n",
    "validation_ds = WeatherBenchDatasetXarrayHealpixTemp(ds=ds_valid, out_features=out_features, delta_t=delta_t,\n",
    "                                                     len_sqce=len_sqce, max_lead_time=max_lead_time,\n",
    "                                                     years=train_years, nodes=nodes, nb_timesteps=nb_timesteps, \n",
    "                                                     mean=train_mean_, std=train_std_, load=False)\n",
    "\n",
    "dl_train = DataLoader(training_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers,\\\n",
    "                      pin_memory=pin_memory)\n",
    "\n",
    "dl_val = DataLoader(validation_ds, batch_size=batch_size*2, shuffle=False, num_workers=num_workers,\\\n",
    "                    pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model #old: in_channels=in_features*len_sqce\n",
    "spherical_unet = UNetSphericalHealpix(N=nodes, in_channels=in_features, out_channels=out_features, \n",
    "                                      kernel_size=3)\n",
    "spherical_unet, device = init_device(spherical_unet, gpu=gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_2steps_custom(model, device, training_ds, batch_size, epochs, lr, validation_data):    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, eps=1e-7, weight_decay=0, amsgrad=False)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    n_samples = training_ds.n_samples\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        print('\\rEpoch : {}'.format(epoch), end=\"\")\n",
    "        \n",
    "        time1 = time.time()\n",
    "        \n",
    "        val_loss = 0\n",
    "        train_loss = 0\n",
    "        \n",
    "        model.train()  \n",
    "        \n",
    "        random.shuffle(training_ds.idxs)\n",
    "        idxs = training_ds.idxs\n",
    "        \n",
    "        batch_idx = 0\n",
    "        for i in range(0, n_samples - batch_size, batch_size):\n",
    "            i_next = min(i + batch_size, n_samples)\n",
    "        #Â for batch_idx, (batch, labels) in enumerate(train_generator):\n",
    "            batch, labels = training_ds[idxs[i:i_next]]\n",
    "            \n",
    "            \n",
    "            # Transfer to GPU\n",
    "            batch1 = batch[0].to(device) #shape: input:(batch_size, num_nodes, num_features);\n",
    "            constants1 = batch[1].to(device) #shape: input:(batch_size, num_nodes, num_features - 2); 2: z500, T850\n",
    "            label1 = labels[0].to(device)\n",
    "            label2 = labels[1].to(device)\n",
    "            \n",
    "            batch_size = batch1.shape[0]\n",
    "            \n",
    "            # Model\n",
    "            \n",
    "            output1 = model(batch1)  \n",
    "            batch2 = torch.cat((output1, constants1), dim=2)\n",
    "            output2 = model(batch2)\n",
    "            \n",
    "            loss = criterion(output1, label1) + criterion(output2, label2)\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss = train_loss + loss.item() * batch_size\n",
    "            \n",
    "            print('\\rBatch idx: {}; Loss: {:.3f}'.format(batch_idx, train_loss/(batch_size*(batch_idx+1))), end=\"\")\n",
    "            batch_idx += 1\n",
    "            \n",
    "        if epoch == 2:\n",
    "            return output1, output2, label1, label2\n",
    "        \n",
    "        train_loss = train_loss / (len(train_generator.dataset))\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            index = 0\n",
    "            \n",
    "            for batch, labels in validation_data:\n",
    "                # Transfer to GPU\n",
    "                batch1 = batch[0].to(device)\n",
    "                constants1 = batch[1].to(device)\n",
    "                label1 = labels[0].to(device)\n",
    "                label2 = labels[1].to(device)\n",
    "\n",
    "                batch_size = batch1.shape[0]\n",
    "                \n",
    "                output1 = model(batch1)\n",
    "                batch2 = torch.cat((output1, constants1), dim=2)\n",
    "                output2 = model(batch2)\n",
    "                \n",
    "                val_loss = val_loss + (criterion(output1, label1).item() \n",
    "                                       + criterion(output2, label2).item()) * batch_size\n",
    "                index = index + batch_size\n",
    "                \n",
    "        val_loss = val_loss / (len(validation_data.dataset))\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        time2 = time.time()\n",
    "        \n",
    "        # Print stuff\n",
    "        print('Epoch: {e:3d}/{n_e:3d}  - loss: {l:.3f}  - val_loss: {v_l:.5f}  - time: {t:2f}'\n",
    "              .format(e=epoch+1, n_e=epochs, l=train_loss, v_l=val_loss, t=time2-time1))\n",
    "        \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0Input 1\n",
      "torch.Size([95, 3072, 7])\n",
      "worked input 1\n",
      "torch.Size([95, 3072, 5]) torch.Size([95, 3072, 2])\n",
      "Input 2 with shape torch.Size([95, 3072, 7])\n",
      "Batch idx: 0; Loss: nanInput 1\n",
      "torch.Size([95, 3072, 7])\n",
      "worked input 1\n",
      "torch.Size([95, 3072, 5]) torch.Size([95, 3072, 2])\n",
      "Input 2 with shape torch.Size([95, 3072, 7])\n",
      "Batch idx: 1; Loss: nanInput 1\n",
      "torch.Size([95, 3072, 7])\n",
      "worked input 1\n",
      "torch.Size([95, 3072, 5]) torch.Size([95, 3072, 2])\n",
      "Input 2 with shape torch.Size([95, 3072, 7])\n",
      "Batch idx: 2; Loss: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-348ae6136508>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m o1, o2, l1, l2 = train_model_2steps_custom(spherical_unet, device, training_ds, batch_size, epochs=7, \\\n\u001b[0;32m----> 2\u001b[0;31m                                            lr=learning_rate, validation_data=dl_val)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-fe9a2de3ead4>\u001b[0m in \u001b[0;36mtrain_model_2steps_custom\u001b[0;34m(model, device, training_ds, batch_size, epochs, lr, validation_data)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mi_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#Â for batch_idx, (batch, labels) in enumerate(train_generator):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi_next\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-67e1feb28b0c>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    100\u001b[0m         )\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         y = ( torch.tensor([self.data.isel(time=[idx_d + self.delta_t * self.len_sqce]).values[:,:,:self.out_features] for idx_d in idx_data],\\\n\u001b[0m\u001b[1;32m    103\u001b[0m                          dtype=torch.float).reshape(len(idx), self.nodes, -1), \n\u001b[1;32m    104\u001b[0m              torch.tensor([self.data.isel(time=[idx_d + self.delta_t * (self.len_sqce+1)]).values[:,:,:self.out_features] for idx_d in idx_data],\\\n",
      "\u001b[0;32m<ipython-input-25-67e1feb28b0c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    100\u001b[0m         )\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         y = ( torch.tensor([self.data.isel(time=[idx_d + self.delta_t * self.len_sqce]).values[:,:,:self.out_features] for idx_d in idx_data],\\\n\u001b[0m\u001b[1;32m    103\u001b[0m                          dtype=torch.float).reshape(len(idx), self.nodes, -1), \n\u001b[1;32m    104\u001b[0m              torch.tensor([self.data.isel(time=[idx_d + self.delta_t * (self.len_sqce+1)]).values[:,:,:self.out_features] for idx_d in idx_data],\\\n",
      "\u001b[0;32m~/miniconda3/envs/weather/lib/python3.7/site-packages/xarray/core/dataarray.py\u001b[0m in \u001b[0;36mvalues\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;34m\"\"\"The array's data as a numpy.ndarray\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weather/lib/python3.7/site-packages/xarray/core/variable.py\u001b[0m in \u001b[0;36mvalues\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;34m\"\"\"The variable's data as a numpy.ndarray\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_as_array_or_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weather/lib/python3.7/site-packages/xarray/core/variable.py\u001b[0m in \u001b[0;36m_as_array_or_item\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0mTODO\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mremove\u001b[0m \u001b[0mthis\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreplace\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0monce\u001b[0m \u001b[0mthese\u001b[0m \u001b[0missues\u001b[0m \u001b[0mare\u001b[0m \u001b[0mfixed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \"\"\"\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"M\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weather/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weather/lib/python3.7/site-packages/dask/array/core.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weather/lib/python3.7/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weather/lib/python3.7/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    439\u001b[0m     )\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m     \u001b[0mdsk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections_to_dsk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weather/lib/python3.7/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcollections_to_dsk\u001b[0;34m(collections, optimize_graph, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_graph_and_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0mgroups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0m_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m             \u001b[0m_opt_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weather/lib/python3.7/site-packages/dask/array/optimization.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(dsk, keys, fuse_keys, fast_functions, inline_functions_fast_functions, rename_fused_keys, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Low level task optimizations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mdsk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfast_functions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0minline_functions_fast_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfast_functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weather/lib/python3.7/site-packages/dask/utils.py\u001b[0m in \u001b[0;36mensure_dict\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0mdd_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdd_id\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m                 \u001b[0mseen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdd_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weather/lib/python3.7/_collections_abc.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[0mKeysView\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weather/lib/python3.7/site-packages/dask/blockwise.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weather/lib/python3.7/site-packages/dask/blockwise.py\u001b[0m in \u001b[0;36m_dict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mnew_axes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0mnumblocks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumblocks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                 \u001b[0mconcatenate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m             )\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weather/lib/python3.7/site-packages/dask/blockwise.py\u001b[0m in \u001b[0;36mmake_blockwise_graph\u001b[0;34m(func, output, out_indices, *arrind_pairs, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 \u001b[0marg_coords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                     \u001b[0mtups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlol_product\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_coords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "o1, o2, l1, l2 = train_model_2steps_custom(spherical_unet, device, training_ds, batch_size, epochs=7, \\\n",
    "                                           lr=learning_rate, validation_data=dl_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
