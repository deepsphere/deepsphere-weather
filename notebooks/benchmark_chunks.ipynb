{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check influence of different parameters in performance. \n",
    "\n",
    "Check influence of:\n",
    "- specifying different chunk sizes and chunking along different dimensions\n",
    "- use already standardized data --> does it save memory?\n",
    "- use ``` .persist()``` to load data in a distributed way and speed up reading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 521#483*2 #483"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/'.join(sys.path[0].split('/')[:-1]))\n",
    "\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import healpy as hp\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from modules.utils import train_model_2steps, init_device\n",
    "from modules.data import WeatherBenchDatasetXarrayHealpix\n",
    "from modules.healpix_models import UNetSphericalHealpix\n",
    "from modules.test import create_iterative_predictions_healpix\n",
    "from modules.test import compute_rmse_healpix\n",
    "from modules.plotting import plot_rmses\n",
    "\n",
    "datadir = \"../data/healpix/\"\n",
    "input_dir = datadir + \"5.625deg_nearest/\"\n",
    "model_save_path = datadir + \"models/\"\n",
    "pred_save_path = datadir + \"predictions/\"\n",
    "\n",
    "train_years = ('1979', '2012')\n",
    "val_years = ('2013', '2016')\n",
    "test_years = ('2017', '2018')\n",
    "\n",
    "nodes = 12*16*16\n",
    "max_lead_time = 5*24\n",
    "lead_time = 6\n",
    "out_features = 2\n",
    "nb_timesteps = 2\n",
    "len_sqce = 2\n",
    "#Â define time resolution\n",
    "delta_t = 6\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,4\"\n",
    "gpu = [0,1]\n",
    "num_workers = 10\n",
    "pin_memory = True\n",
    "batch_size = 95\n",
    "\n",
    "nb_epochs = 10\n",
    "learning_rate = 8e-3\n",
    "\n",
    "#obs = xr.open_mfdataset(pred_save_path + 'observations_nearest.nc', combine='by_coords', chunks={'time':483})\n",
    "#rmses_weyn = xr.open_dataset(datadir + 'metrics/rmses_weyn.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.data import WeatherBenchDatasetIterative\n",
    "class WeatherBenchDatasetXarrayHealpixTemp(Dataset):\n",
    "    \n",
    "    \"\"\" Dataset used for graph models (1D), where data is loaded from stored numpy arrays.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : xarray Dataset\n",
    "        Dataset containing the input data\n",
    "    out_features : int\n",
    "        Number of output features\n",
    "    delta_t : int\n",
    "        Temporal spacing between samples in temporal sequence (in hours)\n",
    "    len_sqce : int\n",
    "        Length of the input and output (predicted) sequences\n",
    "    years : tuple(str)\n",
    "        Years used to split the data\n",
    "    nodes : float\n",
    "        Number of nodes each sample has\n",
    "    max_lead_time : int\n",
    "        Maximum lead time (in case of iterative predictions) in hours\n",
    "    load : bool\n",
    "        If true, load dataset to RAM\n",
    "    mean : np.ndarray of shape 2\n",
    "        Mean to use for data normalization. If None, mean is computed from data\n",
    "    std : np.ndarray of shape 2\n",
    "        std to use for data normalization. If None, mean is computed from data\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, ds, out_features, delta_t, len_sqce, years, nodes, nb_timesteps, \n",
    "                 max_lead_time=None, load=True, mean=None, std=None):\n",
    "        \n",
    "        \n",
    "        self.delta_t = delta_t\n",
    "        self.len_sqce = len_sqce\n",
    "        self.years = years\n",
    "        \n",
    "        self.nodes = nodes\n",
    "        self.out_features = out_features\n",
    "        self.max_lead_time = max_lead_time\n",
    "        self.nb_timesteps = nb_timesteps\n",
    "        \n",
    "        self.data = ds.to_array(dim='level', name='Dataset').transpose('time', 'node', 'level')\n",
    "        self.in_features = self.data.shape[-1]\n",
    "        \n",
    "        self.mean = self.data.mean(('time', 'node')).compute() if mean is None else mean\n",
    "        self.std = self.data.std(('time', 'node')).compute() if std is None else std\n",
    "        \n",
    "        eps = 0.001 #add to std to avoid division by 0\n",
    "        \n",
    "        # Count total number of samples\n",
    "        total_samples = self.data.shape[0]        \n",
    "        \n",
    "        if max_lead_time is None:\n",
    "            self.n_samples = total_samples - (len_sqce+1) * delta_t\n",
    "        else:\n",
    "            self.n_samples = total_samples - (len_sqce+1) * delta_t - max_lead_time\n",
    "        \n",
    "        # Normalize\n",
    "        self.data = (self.data - self.mean.to_array(dim='level')) / (self.std.to_array(dim='level') + eps)\n",
    "        self.data.persist()\n",
    "        \n",
    "        self.idxs = np.array(range(self.n_samples))\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Returns sample and label corresponding to an index as torch.Tensor objects\n",
    "            The return tensor shapes are (for the sample and the label): [n_vertex, len_sqce, n_features]\n",
    "            \n",
    "        \"\"\"\n",
    "        idx_data = idx#self.idxs[idx]\n",
    "        #1,0,2\n",
    "        \n",
    "        #batch[0] --> (batch_size, num_nodes, n_features*len_sq)\n",
    "        idx_full = np.concatenate([idx_data+delta_t,  idx_data + delta_t * len_sqce, idx_data + delta_t * (len_sqce+1)])\n",
    "        dat = self.data.isel(time=idx_full).values\n",
    "        \n",
    "        \n",
    "        X = (\n",
    "            torch.tensor(dat[:len(idx),:,:] , \\\n",
    "                         dtype=torch.float).reshape(len(idx), self.nodes, -1),\n",
    "        )\n",
    "        \n",
    "        y = (torch.tensor(dat[len(idx):len(idx)*2,:,:],\\\n",
    "                         dtype=torch.float).reshape(len(idx), self.nodes, -1),\\\n",
    "             torch.tensor(dat[len(idx)*2:,:,:out_features],\\\n",
    "                         dtype=torch.float).reshape(len(idx), self.nodes, -1)\n",
    "        \n",
    "        )\n",
    "        return X, y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```standardization_contants``` is a boolean that will enable the creation of a file that contains the constants already standardized. Set to True only if it is the first time executing the notebook or the file was lost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardization_contants = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if standardization_contants:\n",
    "    constants = xr.open_dataset(f'{input_dir}constants/constants_5.625deg.nc').rename({'orography' :'orog'})\n",
    "    constants = constants.assign(cos_lon=lambda x: np.cos(np.deg2rad(x.lon)))\n",
    "    constants = constants.assign(sin_lon=lambda x: np.sin(np.deg2rad(x.lon)))\n",
    "    \n",
    "    constants_mean = constants.mean().compute()\n",
    "    constants_std = constants.std().compute()\n",
    "    \n",
    "    constants_mean.to_netcdf(f'{input_dir}constants/mean.nc')\n",
    "    constants_std.to_netcdf(f'{input_dir}constants/std.nc')\n",
    "    \n",
    "    c_mean = xr.open_dataset(f'{input_dir}constants/mean.nc')\n",
    "    c_std = xr.open_dataset(f'{input_dir}constants/std.nc')\n",
    "    \n",
    "    constants_ss = (constants - c_mean)/c_std\n",
    "    \n",
    "    constants_ss.to_netcdf(f'{input_dir}constants/constants_5.625deg_standardized.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "z500 = xr.open_mfdataset(f'{input_dir}geopotential_500/*.nc', combine='by_coords', chunks={'time':chunk_size}).rename({'z':'z500'})\n",
    "t850 = xr.open_mfdataset(f'{input_dir}temperature_850/*.nc', combine='by_coords', chunks={'time':chunk_size}).rename({'t':'t850'})\n",
    "rad = xr.open_mfdataset(f'{input_dir}toa_incident_solar_radiation/*.nc', combine='by_coords', chunks={'time':chunk_size})\n",
    "\n",
    "z500 = z500.isel(time=slice(7, None))\n",
    "t850 = t850.isel(time=slice(7, None))\n",
    "\n",
    "constants = xr.open_dataset(f'{input_dir}constants/constants_5.625deg_standardized.nc')\n",
    "#constants = constants.assign(cos_lon=lambda x: np.cos(np.deg2rad(x.lon)))\n",
    "#constants = constants.assign(sin_lon=lambda x: np.sin(np.deg2rad(x.lon)))\n",
    "\n",
    "#temp = xr.DataArray(np.zeros(z500.dims['time']), coords=[('time', z500.time.values)])\n",
    "#constants, _ = xr.broadcast(constants, temp)\n",
    "\n",
    "orog = constants['orog']\n",
    "lsm = constants['lsm']\n",
    "lats = constants['lat2d']\n",
    "slt = constants['slt']\n",
    "cos_lon = constants['cos_lon']\n",
    "sin_lon = constants['sin_lon']\n",
    "\n",
    "num_constants = len([orog, lats, lsm, slt])\n",
    "constants_tensor = torch.tensor(xr.merge([orog, lats, lsm, slt], compat='override').to_array().values, \\\n",
    "                            dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#description = \"no_const\"\n",
    "description = \"all_const\"\n",
    "\n",
    "model_filename = model_save_path + \"spherical_unet_\" + description + \".h5\"\n",
    "pred_filename = pred_save_path + \"spherical_unet_\" + description + \".nc\"\n",
    "rmse_filename = datadir + 'metrics/rmse_' + description + '.nc'\n",
    "\n",
    "# z500, t850, orog, lats, lsm, slt, rad\n",
    "#feature_idx = [0, 1]\n",
    "in_features = 7 #len(feature_idx)\n",
    "ds = xr.merge([z500, t850, rad], compat='override')\n",
    "#ds = xr.merge([z500, t850, orog, lats, lsm, slt, rad], compat='override')\n",
    "\n",
    "ds_train = ds.sel(time=slice(*train_years))\n",
    "ds_valid = ds.sel(time=slice(*val_years))\n",
    "ds_test = ds.sel(time=slice(*test_years))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if standardization_contants:\n",
    "    mean_features = ds_train.mean(('time','node')).compute()\n",
    "    std_features = ds_train.std('time').mean('node').compute()\n",
    "\n",
    "    mean_features.to_netcdf(f'{input_dir}mean_train_features_dynamic.nc')\n",
    "    std_features.to_netcdf(f'{input_dir}std_train_features_dynamic.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean_ = xr.open_mfdataset(f'{input_dir}mean_train_features_dynamic.nc')\n",
    "train_std_ = xr.open_mfdataset(f'{input_dir}std_train_features_dynamic.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validation data\n",
    "training_ds = WeatherBenchDatasetXarrayHealpixTemp(ds=ds_train, out_features=out_features, delta_t=delta_t,\n",
    "                                                   len_sqce=len_sqce, max_lead_time=max_lead_time,\n",
    "                                                   years=train_years, nodes=nodes, nb_timesteps=nb_timesteps, \n",
    "                                                   mean=train_mean_, std=train_std_, load=False)\n",
    "validation_ds = WeatherBenchDatasetXarrayHealpixTemp(ds=ds_valid, out_features=out_features, delta_t=delta_t,\n",
    "                                                     len_sqce=len_sqce, max_lead_time=max_lead_time,\n",
    "                                                     years=val_years, nodes=nodes, nb_timesteps=nb_timesteps, \n",
    "                                                     mean=train_mean_, std=train_std_, load=False)\n",
    "\n",
    "dl_train = DataLoader(training_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers,\\\n",
    "                      pin_memory=pin_memory)\n",
    "\n",
    "dl_val = DataLoader(validation_ds, batch_size=batch_size*2, shuffle=False, num_workers=num_workers,\\\n",
    "                    pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model #old: in_channels=in_features*len_sqce\n",
    "spherical_unet = UNetSphericalHealpix(N=nodes, in_channels=in_features, out_channels=out_features, \n",
    "                                      kernel_size=3)\n",
    "spherical_unet, device = init_device(spherical_unet, gpu=gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_2steps_custom(model, device, training_ds, constants, batch_size, epochs, lr, validation_ds):    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, eps=1e-7, weight_decay=0, amsgrad=False)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    n_samples = training_ds.n_samples\n",
    "    n_samples_val = validation_ds.n_samples\n",
    "    num_nodes = training_ds.nodes\n",
    "    num_constants = constants.shape[1]\n",
    "    out_features = training_ds.out_features\n",
    "    \n",
    "    constants_expanded = constants.expand(batch_size, num_nodes, num_constants)\n",
    "    constants1 = constants_expanded.to(device)\n",
    "    idxs_val = validation_ds.idxs\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        print('\\rEpoch : {}'.format(epoch), end=\"\")\n",
    "        \n",
    "        time1 = time.time()\n",
    "        \n",
    "        val_loss = 0\n",
    "        train_loss = 0\n",
    "        \n",
    "        model.train()  \n",
    "        \n",
    "        random.shuffle(training_ds.idxs)\n",
    "        idxs = training_ds.idxs\n",
    "        \n",
    "        batch_idx = 0\n",
    "        \n",
    "        for i in range(0, n_samples - batch_size, batch_size):\n",
    "            i_next = min(i + batch_size, n_samples)\n",
    "            \n",
    "            if len(idxs[i:i_next]) < batch_size:\n",
    "                constants_expanded = contants.expand(len(idxs[i:i_next]), num_nodes, num_constants)\n",
    "                constants1 = constants_expanded.to(device)\n",
    "        \n",
    "            \n",
    "            #t1 = time.time()\n",
    "            batch, labels = training_ds[idxs[i:i_next]]\n",
    "            \n",
    "            #t2 = time.time()\n",
    "            \n",
    "            # Transfer to GPU\n",
    "            \n",
    "            \n",
    "            batch1 = torch.cat((batch[0], constants_expanded), dim=2).to(device)\n",
    "            label1 = labels[0].to(device)\n",
    "            label2 = labels[1].to(device)\n",
    "            \n",
    "            \n",
    "            #t3 = time.time()\n",
    "            batch_size = batch1.shape[0]\n",
    "            \n",
    "            # Model\n",
    "            \n",
    "            #t4 = time.time()\n",
    "            output1 = model(batch1)  \n",
    "            #t5 = time.time()\n",
    "            batch2 = torch.cat((output1, label1[:,:,-1].view(-1, num_nodes, 1), constants1), dim=2)\n",
    "            #t6 = time.time()\n",
    "            output2 = model(batch2)\n",
    "            #t7 = time.time()\n",
    "            loss = criterion(output1, label1[:,:,:out_features]) + criterion(output2, label2)\n",
    "            #t8 = time.time()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss = train_loss + loss.item() * batch_size\n",
    "            \n",
    "            \n",
    "            #print('\\nTime to read batch: {}s'.format(t2-t1))\n",
    "            #print('Time to transfer data to GPU: {}s'.format(t3-t2))\n",
    "            #print('Time to process input 1: {}s'.format(t5-t4))\n",
    "            #print('Time to process input 2: {}s'.format(t7-t6))\n",
    "            #print('Time to compute loss: {}s'.format(t8-t7))\n",
    "            #print('\\n')\n",
    "            print('\\rBatch idx: {}; Loss: {:.3f}'.format(batch_idx, train_loss/(batch_size*(batch_idx+1))), end=\"\")\n",
    "            batch_idx += 1\n",
    "        \n",
    "        train_loss = train_loss / n_samples\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        constants1 = constants_expanded.to(device)\n",
    "        with torch.set_grad_enabled(False):\n",
    "            index = 0\n",
    "            \n",
    "            for i in range(0, n_samples_val - batch_size, batch_size):\n",
    "                i_next = min(i + batch_size, n_samples_val)\n",
    "\n",
    "                if len(idxs_val[i:i_next]) < batch_size:\n",
    "                    constants_expanded = contants.expand(len(idxs_val[i:i_next]), num_nodes, num_constants)\n",
    "                    constants1 = constants_expanded.to(device)\n",
    "\n",
    "\n",
    "                #t1 = time.time()\n",
    "                batch, labels = validation_ds[idxs_val[i:i_next]]\n",
    "                # Transfer to GPU\n",
    "                batch1 = torch.cat((batch[0], constants_expanded), dim=2).to(device)\n",
    "                label1 = labels[0].to(device)\n",
    "                label2 = labels[1].to(device)\n",
    "\n",
    "                batch_size = batch1.shape[0]\n",
    "                \n",
    "                output1 = model(batch1)\n",
    "                batch2 = torch.cat((output1, label1[:,:,-1].view(-1, num_nodes, 1), constants1), dim=2)\n",
    "                output2 = model(batch2)\n",
    "                \n",
    "                val_loss = val_loss + (criterion(output1, label1[:,:,:out_features]).item() \n",
    "                                       + criterion(output2, label2).item()) * batch_size\n",
    "                index = index + batch_size\n",
    "                \n",
    "        val_loss = val_loss / n_samples_val\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        time2 = time.time()\n",
    "        \n",
    "        # Print stuff\n",
    "        print('Epoch: {e:3d}/{n_e:3d}  - loss: {l:.3f}  - val_loss: {v_l:.5f}  - time: {t:2f}'\n",
    "              .format(e=epoch+1, n_e=epochs, l=train_loss, v_l=val_loss, t=time2-time1))\n",
    "        \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch idx: 3134; Loss: 0.146Epoch:   1/  7  - loss: 0.146  - val_loss: 0.08996  - time: 9112.442816\n",
      "Batch idx: 3134; Loss: 0.067Epoch:   2/  7  - loss: 0.067  - val_loss: 0.07784  - time: 8952.762427\n",
      "Batch idx: 3134; Loss: 0.059Epoch:   3/  7  - loss: 0.059  - val_loss: 0.06506  - time: 9087.411525\n",
      "Batch idx: 3134; Loss: 0.055Epoch:   4/  7  - loss: 0.055  - val_loss: 0.06146  - time: 9108.357737\n",
      "Batch idx: 3134; Loss: 0.052Epoch:   5/  7  - loss: 0.052  - val_loss: 0.05875  - time: 9163.751619\n",
      "Batch idx: 3134; Loss: 0.051Epoch:   6/  7  - loss: 0.051  - val_loss: 0.06549  - time: 8639.539581\n",
      "Batch idx: 3134; Loss: 0.050Epoch:   7/  7  - loss: 0.050  - val_loss: 0.09280  - time: 7845.371900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.14609487630704313,\n",
       "  0.06691656585619492,\n",
       "  0.05884525018606003,\n",
       "  0.05463608803415421,\n",
       "  0.05232742307901584,\n",
       "  0.05091046646963797,\n",
       "  0.049796112792113253],\n",
       " [0.0899640527970213,\n",
       "  0.07783917889697838,\n",
       "  0.06505507764545217,\n",
       "  0.06146494293280259,\n",
       "  0.058750781663517763,\n",
       "  0.06549294110768775,\n",
       "  0.09280043438659136])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model_2steps_custom(spherical_unet, device, training_ds, constants_tensor.transpose(1,0), batch_size, epochs=7, \\\n",
    "                                           lr=learning_rate, validation_ds=validation_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = [0.14609487630704313,\n",
    "  0.06691656585619492,\n",
    "  0.05884525018606003,\n",
    "  0.05463608803415421,\n",
    "  0.05232742307901584,\n",
    "  0.05091046646963797,\n",
    "  0.049796112792113253]\n",
    "\n",
    "val_loss = [0.0899640527970213,\n",
    "  0.07783917889697838,\n",
    "  0.06505507764545217,\n",
    "  0.06146494293280259,\n",
    "  0.058750781663517763,\n",
    "  0.06549294110768775,\n",
    "  0.09280043438659136]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(spherical_unet.state_dict(), model_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
