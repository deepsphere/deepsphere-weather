{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check influence of different parameters in performance. \n",
    "\n",
    "Check influence of:\n",
    "- specifying different chunk sizes and chunking along different dimensions\n",
    "- use already standardized data --> does it save memory?\n",
    "- use ``` .persist()``` to load data in a distributed way and speed up reading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 521#483*2 #483"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/'.join(sys.path[0].split('/')[:-1]))\n",
    "\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import healpy as hp\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from modules.utils import train_model_2steps, init_device\n",
    "from modules.data import WeatherBenchDatasetXarrayHealpix\n",
    "from modules.healpix_models import UNetSphericalHealpix\n",
    "from modules.test import create_iterative_predictions_healpix\n",
    "from modules.test import compute_rmse_healpix\n",
    "from modules.plotting import plot_rmses\n",
    "\n",
    "datadir = \"../data/healpix/\"\n",
    "input_dir = datadir + \"5.625deg_nearest/\"\n",
    "model_save_path = datadir + \"models/\"\n",
    "pred_save_path = datadir + \"predictions/\"\n",
    "\n",
    "train_years = ('1979', '2012')\n",
    "val_years = ('2013', '2016')\n",
    "test_years = ('2017', '2018')\n",
    "\n",
    "nodes = 12*16*16\n",
    "max_lead_time = 5*24\n",
    "lead_time = 6\n",
    "out_features = 2\n",
    "nb_timesteps = 2\n",
    "len_sqce = 2\n",
    "#Â define time resolution\n",
    "delta_t = 6\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,4\"\n",
    "gpu = [0,1]\n",
    "num_workers = 10\n",
    "pin_memory = True\n",
    "batch_size = 95\n",
    "\n",
    "nb_epochs = 10\n",
    "learning_rate = 8e-3\n",
    "\n",
    "#obs = xr.open_mfdataset(pred_save_path + 'observations_nearest.nc', combine='by_coords', chunks={'time':483})\n",
    "#rmses_weyn = xr.open_dataset(datadir + 'metrics/rmses_weyn.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.data import WeatherBenchDatasetIterative\n",
    "class WeatherBenchDatasetXarrayHealpixTemp(Dataset):\n",
    "    \n",
    "    \"\"\" Dataset used for graph models (1D), where data is loaded from stored numpy arrays.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : xarray Dataset\n",
    "        Dataset containing the input data\n",
    "    out_features : int\n",
    "        Number of output features\n",
    "    delta_t : int\n",
    "        Temporal spacing between samples in temporal sequence (in hours)\n",
    "    len_sqce : int\n",
    "        Length of the input and output (predicted) sequences\n",
    "    years : tuple(str)\n",
    "        Years used to split the data\n",
    "    nodes : float\n",
    "        Number of nodes each sample has\n",
    "    max_lead_time : int\n",
    "        Maximum lead time (in case of iterative predictions) in hours\n",
    "    load : bool\n",
    "        If true, load dataset to RAM\n",
    "    mean : np.ndarray of shape 2\n",
    "        Mean to use for data normalization. If None, mean is computed from data\n",
    "    std : np.ndarray of shape 2\n",
    "        std to use for data normalization. If None, mean is computed from data\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, ds, out_features, delta_t, len_sqce, years, nodes, nb_timesteps, \n",
    "                 max_lead_time=None, load=True, mean=None, std=None):\n",
    "        \n",
    "        \n",
    "        self.delta_t = delta_t\n",
    "        self.len_sqce = len_sqce\n",
    "        self.years = years\n",
    "        \n",
    "        self.nodes = nodes\n",
    "        self.out_features = out_features\n",
    "        self.max_lead_time = max_lead_time\n",
    "        self.nb_timesteps = nb_timesteps\n",
    "        \n",
    "        self.data = ds.to_array(dim='level', name='Dataset').transpose('time', 'node', 'level')\n",
    "        self.in_features = self.data.shape[-1]\n",
    "        \n",
    "        self.mean = self.data.mean(('time', 'node')).compute() if mean is None else mean\n",
    "        self.std = self.data.std(('time', 'node')).compute() if std is None else std\n",
    "        \n",
    "        eps = 0.001 #add to std to avoid division by 0\n",
    "        \n",
    "        # Count total number of samples\n",
    "        total_samples = self.data.shape[0]        \n",
    "        \n",
    "        if max_lead_time is None:\n",
    "            self.n_samples = total_samples - (len_sqce+1) * delta_t\n",
    "        else:\n",
    "            self.n_samples = total_samples - (len_sqce+1) * delta_t - max_lead_time\n",
    "        \n",
    "        # Normalize\n",
    "        self.data = (self.data - self.mean.to_array(dim='level')) / (self.std.to_array(dim='level') + eps)\n",
    "        self.data.persist()\n",
    "        \n",
    "        self.idxs = np.array(range(self.n_samples))\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Returns sample and label corresponding to an index as torch.Tensor objects\n",
    "            The return tensor shapes are (for the sample and the label): [n_vertex, len_sqce, n_features]\n",
    "            \n",
    "        \"\"\"\n",
    "        idx_data = self.idxs[idx]\n",
    "        #1,0,2\n",
    "        \n",
    "        #batch[0] --> (batch_size, num_nodes, n_features*len_sq)\n",
    "        idx_full = np.concatenate([idx_data+delta_t,  idx_data + delta_t * len_sqce, idx_data + delta_t * (len_sqce+1)])\n",
    "        dat = self.data.isel(time=idx_full).values\n",
    "        \n",
    "        \n",
    "        X = (\n",
    "            torch.tensor(dat[:len(idx),:,:] , \\\n",
    "                         dtype=torch.float).reshape(len(idx), self.nodes, -1),\n",
    "        )\n",
    "        \n",
    "        y = (torch.tensor(dat[len(idx):len(idx)*2,:,:],\\\n",
    "                         dtype=torch.float).reshape(len(idx), self.nodes, -1),\\\n",
    "             torch.tensor(dat[len(idx)*2:,:,:out_features],\\\n",
    "                         dtype=torch.float).reshape(len(idx), self.nodes, -1)\n",
    "        \n",
    "        )\n",
    "        return X, y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```standardization_contants``` is a boolean that will enable the creation of a file that contains the constants already standardized. Set to True only if it is the first time executing the notebook or the file was lost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardization_contants = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if standardization_contants:\n",
    "    constants = xr.open_dataset(f'{input_dir}constants/constants_5.625deg.nc').rename({'orography' :'orog'})\n",
    "    constants = constants.assign(cos_lon=lambda x: np.cos(np.deg2rad(x.lon)))\n",
    "    constants = constants.assign(sin_lon=lambda x: np.sin(np.deg2rad(x.lon)))\n",
    "    \n",
    "    constants_mean = constants.mean().compute()\n",
    "    constants_std = constants.std().compute()\n",
    "    \n",
    "    constants_mean.to_netcdf(f'{input_dir}constants/mean.nc')\n",
    "    constants_std.to_netcdf(f'{input_dir}constants/std.nc')\n",
    "    \n",
    "    c_mean = xr.open_dataset(f'{input_dir}constants/mean.nc')\n",
    "    c_std = xr.open_dataset(f'{input_dir}constants/std.nc')\n",
    "    \n",
    "    constants_ss = (constants - c_mean)/c_std\n",
    "    \n",
    "    constants_ss.to_netcdf(f'{input_dir}constants/constants_5.625deg_standardized.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "z500 = xr.open_mfdataset(f'{input_dir}geopotential_500/*.nc', combine='by_coords', chunks={'time':chunk_size}).rename({'z':'z500'})\n",
    "t850 = xr.open_mfdataset(f'{input_dir}temperature_850/*.nc', combine='by_coords', chunks={'time':chunk_size}).rename({'t':'t850'})\n",
    "rad = xr.open_mfdataset(f'{input_dir}toa_incident_solar_radiation/*.nc', combine='by_coords', chunks={'time':chunk_size})\n",
    "\n",
    "z500 = z500.isel(time=slice(7, None))\n",
    "t850 = t850.isel(time=slice(7, None))\n",
    "\n",
    "constants = xr.open_dataset(f'{input_dir}constants/constants_5.625deg_standardized.nc')\n",
    "#constants = constants.assign(cos_lon=lambda x: np.cos(np.deg2rad(x.lon)))\n",
    "#constants = constants.assign(sin_lon=lambda x: np.sin(np.deg2rad(x.lon)))\n",
    "\n",
    "#temp = xr.DataArray(np.zeros(z500.dims['time']), coords=[('time', z500.time.values)])\n",
    "#constants, _ = xr.broadcast(constants, temp)\n",
    "\n",
    "orog = constants['orog']\n",
    "lsm = constants['lsm']\n",
    "lats = constants['lat2d']\n",
    "slt = constants['slt']\n",
    "cos_lon = constants['cos_lon']\n",
    "sin_lon = constants['sin_lon']\n",
    "\n",
    "num_constants = len([orog, lats, lsm, slt])\n",
    "constants_tensor = torch.tensor(xr.merge([orog, lats, lsm, slt], compat='override').to_array().values, \\\n",
    "                            dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#description = \"no_const\"\n",
    "description = \"all_const\"\n",
    "\n",
    "model_filename = model_save_path + \"spherical_unet_\" + description + \".h5\"\n",
    "pred_filename = pred_save_path + \"spherical_unet_\" + description + \".nc\"\n",
    "rmse_filename = datadir + 'metrics/rmse_' + description + '.nc'\n",
    "\n",
    "# z500, t850, orog, lats, lsm, slt, rad\n",
    "#feature_idx = [0, 1]\n",
    "in_features = 7 #len(feature_idx)\n",
    "ds = xr.merge([z500, t850, rad], compat='override')\n",
    "#ds = xr.merge([z500, t850, orog, lats, lsm, slt, rad], compat='override')\n",
    "\n",
    "ds_train = ds.sel(time=slice(*train_years))\n",
    "ds_valid = ds.sel(time=slice(*val_years))\n",
    "ds_test = ds.sel(time=slice(*test_years))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if standardization_contants:\n",
    "    mean_features = ds_train.mean(('time','node')).compute()\n",
    "    std_features = ds_train.std('time').mean('node').compute()\n",
    "\n",
    "    mean_features.to_netcdf(f'{input_dir}mean_train_features_dynamic.nc')\n",
    "    std_features.to_netcdf(f'{input_dir}std_train_features_dynamic.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean_ = xr.open_mfdataset(f'{input_dir}mean_train_features_dynamic.nc')\n",
    "train_std_ = xr.open_mfdataset(f'{input_dir}std_train_features_dynamic.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validation data\n",
    "training_ds = WeatherBenchDatasetXarrayHealpixTemp(ds=ds_train, out_features=out_features, delta_t=delta_t,\n",
    "                                                   len_sqce=len_sqce, max_lead_time=max_lead_time,\n",
    "                                                   years=train_years, nodes=nodes, nb_timesteps=nb_timesteps, \n",
    "                                                   mean=train_mean_, std=train_std_, load=False)\n",
    "validation_ds = WeatherBenchDatasetXarrayHealpixTemp(ds=ds_valid, out_features=out_features, delta_t=delta_t,\n",
    "                                                     len_sqce=len_sqce, max_lead_time=max_lead_time,\n",
    "                                                     years=train_years, nodes=nodes, nb_timesteps=nb_timesteps, \n",
    "                                                     mean=train_mean_, std=train_std_, load=False)\n",
    "\n",
    "dl_train = DataLoader(training_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers,\\\n",
    "                      pin_memory=pin_memory)\n",
    "\n",
    "dl_val = DataLoader(validation_ds, batch_size=batch_size*2, shuffle=False, num_workers=num_workers,\\\n",
    "                    pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model #old: in_channels=in_features*len_sqce\n",
    "spherical_unet = UNetSphericalHealpix(N=nodes, in_channels=in_features, out_channels=out_features, \n",
    "                                      kernel_size=3)\n",
    "spherical_unet, device = init_device(spherical_unet, gpu=gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_2steps_custom(model, device, training_ds, constants, batch_size, epochs, lr, validation_data):    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, eps=1e-7, weight_decay=0, amsgrad=False)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    n_samples = training_ds.n_samples\n",
    "    num_nodes = training_ds.nodes\n",
    "    num_constants = constants.shape[1]\n",
    "    out_features = training_ds.out_features\n",
    "    \n",
    "    constants_expanded = constants.expand(batch_size, num_nodes, num_constants)\n",
    "    constants1 = constants_expanded.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        print('\\rEpoch : {}'.format(epoch), end=\"\")\n",
    "        \n",
    "        time1 = time.time()\n",
    "        \n",
    "        val_loss = 0\n",
    "        train_loss = 0\n",
    "        \n",
    "        model.train()  \n",
    "        \n",
    "        random.shuffle(training_ds.idxs)\n",
    "        idxs = training_ds.idxs\n",
    "        \n",
    "        batch_idx = 0\n",
    "        for i in range(0, n_samples - batch_size, batch_size):\n",
    "            i_next = min(i + batch_size, n_samples)\n",
    "            \n",
    "            if len(idxs[i:i_next]) < batch_size:\n",
    "                constants_expanded = contants.expand(len(idxs[i:i_next]), num_nodes, num_constants)\n",
    "                constants1 = constants_expanded.to(device)\n",
    "        \n",
    "            \n",
    "            t1 = time.time()\n",
    "            batch, labels = training_ds[idxs[i:i_next]]\n",
    "            \n",
    "            t2 = time.time()\n",
    "            \n",
    "            # Transfer to GPU\n",
    "            \n",
    "            \n",
    "            batch1 = torch.cat((batch[0], constants_expanded), dim=2).to(device)\n",
    "            label1 = labels[0].to(device)\n",
    "            label2 = labels[1].to(device)\n",
    "            \n",
    "            \n",
    "            t3 = time.time()\n",
    "            batch_size = batch1.shape[0]\n",
    "            \n",
    "            # Model\n",
    "            \n",
    "            t4 = time.time()\n",
    "            output1 = model(batch1)  \n",
    "            t5 = time.time()\n",
    "            batch2 = torch.cat((output1, label1[:,:,-1].view(-1, num_nodes, 1), constants1), dim=2)\n",
    "            t6 = time.time()\n",
    "            output2 = model(batch2)\n",
    "            t7 = time.time()\n",
    "            loss = criterion(output1, label1[:,:,:out_features]) + criterion(output2, label2)\n",
    "            t8 = time.time()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss = train_loss + loss.item() * batch_size\n",
    "            \n",
    "            \n",
    "            print('\\nTime to read batch: {}s'.format(t2-t1))\n",
    "            print('Time to transfer data to GPU: {}s'.format(t3-t2))\n",
    "            print('Time to process input 1: {}s'.format(t5-t4))\n",
    "            print('Time to process input 2: {}s'.format(t7-t6))\n",
    "            print('Time to compute loss: {}s'.format(t8-t7))\n",
    "            print('\\n')\n",
    "            print('\\rBatch idx: {}; Loss: {:.3f}'.format(batch_idx, train_loss/(batch_size*(batch_idx+1))), end=\"\")\n",
    "            batch_idx += 1\n",
    "            \n",
    "        if epoch == 2:\n",
    "            return output1, output2, label1, label2\n",
    "        \n",
    "        train_loss = train_loss / (len(train_generator.dataset))\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            index = 0\n",
    "            \n",
    "            for batch, labels in validation_data:\n",
    "                # Transfer to GPU\n",
    "                batch1 = torch.cat((batch[0], constants1), dim=2).to(device)\n",
    "                label1 = labels[0].to(device)\n",
    "                label2 = labels[1].to(device)\n",
    "\n",
    "                batch_size = batch1.shape[0]\n",
    "                \n",
    "                output1 = model(batch1)\n",
    "                batch2 = torch.cat((output1, constants1), dim=2)\n",
    "                output2 = model(batch2)\n",
    "                \n",
    "                val_loss = val_loss + (criterion(output1, label1).item() \n",
    "                                       + criterion(output2, label2).item()) * batch_size\n",
    "                index = index + batch_size\n",
    "                \n",
    "        val_loss = val_loss / (len(validation_data.dataset))\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        time2 = time.time()\n",
    "        \n",
    "        # Print stuff\n",
    "        print('Epoch: {e:3d}/{n_e:3d}  - loss: {l:.3f}  - val_loss: {v_l:.5f}  - time: {t:2f}'\n",
    "              .format(e=epoch+1, n_e=epochs, l=train_loss, v_l=val_loss, t=time2-time1))\n",
    "        \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0\n",
      "Time to read batch: 2.8489584922790527s\n",
      "Time to transfer data to GPU: 0.005797386169433594s\n",
      "Time to process input 1: 2.5217576026916504s\n",
      "Time to process input 2: 0.10119962692260742s\n",
      "Time to compute loss: 0.048406362533569336s\n",
      "\n",
      "\n",
      "Batch idx: 0; Loss: 23.178\n",
      "Time to read batch: 3.2484841346740723s\n",
      "Time to transfer data to GPU: 0.005156040191650391s\n",
      "Time to process input 1: 0.05000710487365723s\n",
      "Time to process input 2: 0.04090094566345215s\n",
      "Time to compute loss: 0.15488314628601074s\n",
      "\n",
      "\n",
      "Batch idx: 1; Loss: 17.992\n",
      "Time to read batch: 2.227978467941284s\n",
      "Time to transfer data to GPU: 0.0050048828125s\n",
      "Time to process input 1: 0.04160046577453613s\n",
      "Time to process input 2: 0.04461359977722168s\n",
      "Time to compute loss: 0.14378929138183594s\n",
      "\n",
      "\n",
      "Batch idx: 2; Loss: 14.503\n",
      "Time to read batch: 2.5850720405578613s\n",
      "Time to transfer data to GPU: 0.004888057708740234s\n",
      "Time to process input 1: 0.039263248443603516s\n",
      "Time to process input 2: 0.03805685043334961s\n",
      "Time to compute loss: 0.15384244918823242s\n",
      "\n",
      "\n",
      "Batch idx: 3; Loss: 12.024\n",
      "Time to read batch: 2.190979480743408s\n",
      "Time to transfer data to GPU: 0.004604339599609375s\n",
      "Time to process input 1: 0.03262782096862793s\n",
      "Time to process input 2: 0.037258148193359375s\n",
      "Time to compute loss: 0.1562647819519043s\n",
      "\n",
      "\n",
      "Batch idx: 4; Loss: 10.204\n",
      "Time to read batch: 2.423586368560791s\n",
      "Time to transfer data to GPU: 0.004754066467285156s\n",
      "Time to process input 1: 0.03890800476074219s\n",
      "Time to process input 2: 0.05193161964416504s\n",
      "Time to compute loss: 0.13992094993591309s\n",
      "\n",
      "\n",
      "Batch idx: 5; Loss: 8.886\n",
      "Time to read batch: 2.313802480697632s\n",
      "Time to transfer data to GPU: 0.004996061325073242s\n",
      "Time to process input 1: 0.038947105407714844s\n",
      "Time to process input 2: 0.03863382339477539s\n",
      "Time to compute loss: 0.15145635604858398s\n",
      "\n",
      "\n",
      "Batch idx: 6; Loss: 7.904\n",
      "Time to read batch: 2.461594343185425s\n",
      "Time to transfer data to GPU: 0.0052947998046875s\n",
      "Time to process input 1: 0.040430545806884766s\n",
      "Time to process input 2: 0.03738522529602051s\n",
      "Time to compute loss: 0.15370535850524902s\n",
      "\n",
      "\n",
      "Batch idx: 7; Loss: 7.173\n",
      "Time to read batch: 2.3186402320861816s\n",
      "Time to transfer data to GPU: 0.005309581756591797s\n",
      "Time to process input 1: 0.04767608642578125s\n",
      "Time to process input 2: 0.040602922439575195s\n",
      "Time to compute loss: 0.14520812034606934s\n",
      "\n",
      "\n",
      "Batch idx: 8; Loss: 6.617\n",
      "Time to read batch: 2.4272799491882324s\n",
      "Time to transfer data to GPU: 0.005092144012451172s\n",
      "Time to process input 1: 0.05141305923461914s\n",
      "Time to process input 2: 0.03887605667114258s\n",
      "Time to compute loss: 0.14480829238891602s\n",
      "\n",
      "\n",
      "Batch idx: 9; Loss: 6.176\n",
      "Time to read batch: 2.590805768966675s\n",
      "Time to transfer data to GPU: 0.0059969425201416016s\n",
      "Time to process input 1: 0.039984941482543945s\n",
      "Time to process input 2: 0.04153275489807129s\n",
      "Time to compute loss: 0.14838361740112305s\n",
      "\n",
      "\n",
      "Batch idx: 10; Loss: 5.814\n",
      "Time to read batch: 2.3948726654052734s\n",
      "Time to transfer data to GPU: 0.005999088287353516s\n",
      "Time to process input 1: 0.04214286804199219s\n",
      "Time to process input 2: 0.044180870056152344s\n",
      "Time to compute loss: 0.14705944061279297s\n",
      "\n",
      "\n",
      "Batch idx: 11; Loss: 5.511\n",
      "Time to read batch: 2.1779673099517822s\n",
      "Time to transfer data to GPU: 0.004677772521972656s\n",
      "Time to process input 1: 0.04202866554260254s\n",
      "Time to process input 2: 0.03907370567321777s\n",
      "Time to compute loss: 0.14812636375427246s\n",
      "\n",
      "\n",
      "Batch idx: 12; Loss: 5.251\n",
      "Time to read batch: 2.462989568710327s\n",
      "Time to transfer data to GPU: 0.004978179931640625s\n",
      "Time to process input 1: 0.04391670227050781s\n",
      "Time to process input 2: 0.040929555892944336s\n",
      "Time to compute loss: 0.1467597484588623s\n",
      "\n",
      "\n",
      "Batch idx: 13; Loss: 5.016\n",
      "Time to read batch: 2.2110142707824707s\n",
      "Time to transfer data to GPU: 0.004899263381958008s\n",
      "Time to process input 1: 0.04351663589477539s\n",
      "Time to process input 2: 0.040343284606933594s\n",
      "Time to compute loss: 0.1448962688446045s\n",
      "\n",
      "\n",
      "Batch idx: 14; Loss: 4.810\n",
      "Time to read batch: 2.5291202068328857s\n",
      "Time to transfer data to GPU: 0.004869699478149414s\n",
      "Time to process input 1: 0.038481950759887695s\n",
      "Time to process input 2: 0.038947343826293945s\n",
      "Time to compute loss: 0.15175342559814453s\n",
      "\n",
      "\n",
      "Batch idx: 15; Loss: 4.623\n",
      "Time to read batch: 2.650336980819702s\n",
      "Time to transfer data to GPU: 0.005554914474487305s\n",
      "Time to process input 1: 0.04283308982849121s\n",
      "Time to process input 2: 0.038922786712646484s\n",
      "Time to compute loss: 0.16139745712280273s\n",
      "\n",
      "\n",
      "Batch idx: 16; Loss: 4.454\n",
      "Time to read batch: 2.205498218536377s\n",
      "Time to transfer data to GPU: 0.004796266555786133s\n",
      "Time to process input 1: 0.03801870346069336s\n",
      "Time to process input 2: 0.03766369819641113s\n",
      "Time to compute loss: 0.15598750114440918s\n",
      "\n",
      "\n",
      "Batch idx: 17; Loss: 4.304\n",
      "Time to read batch: 2.5296688079833984s\n",
      "Time to transfer data to GPU: 0.004914522171020508s\n",
      "Time to process input 1: 0.042658329010009766s\n",
      "Time to process input 2: 0.04812979698181152s\n",
      "Time to compute loss: 0.14275193214416504s\n",
      "\n",
      "\n",
      "Batch idx: 18; Loss: 4.169\n",
      "Time to read batch: 2.318469285964966s\n",
      "Time to transfer data to GPU: 0.005528688430786133s\n",
      "Time to process input 1: 0.03980660438537598s\n",
      "Time to process input 2: 0.03996849060058594s\n",
      "Time to compute loss: 0.15148472785949707s\n",
      "\n",
      "\n",
      "Batch idx: 19; Loss: 4.043\n",
      "Time to read batch: 2.408322811126709s\n",
      "Time to transfer data to GPU: 0.0050508975982666016s\n",
      "Time to process input 1: 0.036476850509643555s\n",
      "Time to process input 2: 0.043756723403930664s\n",
      "Time to compute loss: 0.1487727165222168s\n",
      "\n",
      "\n",
      "Batch idx: 20; Loss: 3.930\n",
      "Time to read batch: 2.415386199951172s\n",
      "Time to transfer data to GPU: 0.005358695983886719s\n",
      "Time to process input 1: 0.04161691665649414s\n",
      "Time to process input 2: 0.03878641128540039s\n",
      "Time to compute loss: 0.15047812461853027s\n",
      "\n",
      "\n",
      "Batch idx: 21; Loss: 3.823\n",
      "Time to read batch: 2.188685417175293s\n",
      "Time to transfer data to GPU: 0.004686594009399414s\n",
      "Time to process input 1: 0.04153132438659668s\n",
      "Time to process input 2: 0.03954958915710449s\n",
      "Time to compute loss: 0.14601397514343262s\n",
      "\n",
      "\n",
      "Batch idx: 22; Loss: 3.726\n",
      "Time to read batch: 2.385129928588867s\n",
      "Time to transfer data to GPU: 0.0054814815521240234s\n",
      "Time to process input 1: 0.040703773498535156s\n",
      "Time to process input 2: 0.05019497871398926s\n",
      "Time to compute loss: 0.144026517868042s\n",
      "\n",
      "\n",
      "Batch idx: 23; Loss: 3.633\n",
      "Time to read batch: 2.166855573654175s\n",
      "Time to transfer data to GPU: 0.004839897155761719s\n",
      "Time to process input 1: 0.03910565376281738s\n",
      "Time to process input 2: 0.04247283935546875s\n",
      "Time to compute loss: 0.14687585830688477s\n",
      "\n",
      "\n",
      "Batch idx: 24; Loss: 3.551\n",
      "Time to read batch: 2.8599627017974854s\n",
      "Time to transfer data to GPU: 0.0053195953369140625s\n",
      "Time to process input 1: 0.051041364669799805s\n",
      "Time to process input 2: 0.03839421272277832s\n",
      "Time to compute loss: 0.14546799659729004s\n",
      "\n",
      "\n",
      "Batch idx: 25; Loss: 3.473\n",
      "Time to read batch: 2.2564008235931396s\n",
      "Time to transfer data to GPU: 0.005039215087890625s\n",
      "Time to process input 1: 0.03808021545410156s\n",
      "Time to process input 2: 0.03896975517272949s\n",
      "Time to compute loss: 0.1510612964630127s\n",
      "\n",
      "\n",
      "Batch idx: 26; Loss: 3.401\n",
      "Time to read batch: 2.5294547080993652s\n",
      "Time to transfer data to GPU: 0.004925966262817383s\n",
      "Time to process input 1: 0.04029130935668945s\n",
      "Time to process input 2: 0.0401151180267334s\n",
      "Time to compute loss: 0.1504218578338623s\n",
      "\n",
      "\n",
      "Batch idx: 27; Loss: 3.334\n",
      "Time to read batch: 2.264204263687134s\n",
      "Time to transfer data to GPU: 0.009865999221801758s\n",
      "Time to process input 1: 0.041344404220581055s\n",
      "Time to process input 2: 0.04146766662597656s\n",
      "Time to compute loss: 0.1485910415649414s\n",
      "\n",
      "\n",
      "Batch idx: 28; Loss: 3.271\n",
      "Time to read batch: 2.5116496086120605s\n",
      "Time to transfer data to GPU: 0.005167484283447266s\n",
      "Time to process input 1: 0.03810310363769531s\n",
      "Time to process input 2: 0.037946462631225586s\n",
      "Time to compute loss: 0.15371417999267578s\n",
      "\n",
      "\n",
      "Batch idx: 29; Loss: 3.211\n",
      "Time to read batch: 2.748257875442505s\n",
      "Time to transfer data to GPU: 0.005052328109741211s\n",
      "Time to process input 1: 0.0419771671295166s\n",
      "Time to process input 2: 0.03992009162902832s\n",
      "Time to compute loss: 0.1479809284210205s\n",
      "\n",
      "\n",
      "Batch idx: 30; Loss: 3.156\n",
      "Time to read batch: 2.4128246307373047s\n",
      "Time to transfer data to GPU: 0.0053462982177734375s\n",
      "Time to process input 1: 0.04508399963378906s\n",
      "Time to process input 2: 0.03952670097351074s\n",
      "Time to compute loss: 0.14745545387268066s\n",
      "\n",
      "\n",
      "Batch idx: 31; Loss: 3.102\n",
      "Time to read batch: 2.355640172958374s\n",
      "Time to transfer data to GPU: 0.0048563480377197266s\n",
      "Time to process input 1: 0.03887534141540527s\n",
      "Time to process input 2: 0.03706693649291992s\n",
      "Time to compute loss: 0.15613317489624023s\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch idx: 32; Loss: 3.051\n",
      "Time to read batch: 2.4416162967681885s\n",
      "Time to transfer data to GPU: 0.00510859489440918s\n",
      "Time to process input 1: 0.038666725158691406s\n",
      "Time to process input 2: 0.039018869400024414s\n",
      "Time to compute loss: 0.15257787704467773s\n",
      "\n",
      "\n",
      "Batch idx: 33; Loss: 3.004\n",
      "Time to read batch: 2.553452730178833s\n",
      "Time to transfer data to GPU: 0.004793882369995117s\n",
      "Time to process input 1: 0.046502113342285156s\n",
      "Time to process input 2: 0.038489341735839844s\n",
      "Time to compute loss: 0.15085411071777344s\n",
      "\n",
      "\n",
      "Batch idx: 34; Loss: 2.957\n",
      "Time to read batch: 2.407186985015869s\n",
      "Time to transfer data to GPU: 0.005374431610107422s\n",
      "Time to process input 1: 0.04132390022277832s\n",
      "Time to process input 2: 0.038373470306396484s\n",
      "Time to compute loss: 0.15488052368164062s\n",
      "\n",
      "\n",
      "Batch idx: 35; Loss: 2.913\n",
      "Time to read batch: 2.247410774230957s\n",
      "Time to transfer data to GPU: 0.004984140396118164s\n",
      "Time to process input 1: 0.04841923713684082s\n",
      "Time to process input 2: 0.038690805435180664s\n",
      "Time to compute loss: 0.14669084548950195s\n",
      "\n",
      "\n",
      "Batch idx: 36; Loss: 2.870\n",
      "Time to read batch: 2.543917655944824s\n",
      "Time to transfer data to GPU: 0.008794546127319336s\n",
      "Time to process input 1: 0.04133152961730957s\n",
      "Time to process input 2: 0.039717674255371094s\n",
      "Time to compute loss: 0.14980149269104004s\n",
      "\n",
      "\n",
      "Batch idx: 37; Loss: 2.830\n",
      "Time to read batch: 2.1860945224761963s\n",
      "Time to transfer data to GPU: 0.004853487014770508s\n",
      "Time to process input 1: 0.037580013275146484s\n",
      "Time to process input 2: 0.03704953193664551s\n",
      "Time to compute loss: 0.15405607223510742s\n",
      "\n",
      "\n",
      "Batch idx: 38; Loss: 2.792\n",
      "Time to read batch: 2.6918270587921143s\n",
      "Time to transfer data to GPU: 0.004992246627807617s\n",
      "Time to process input 1: 0.04171943664550781s\n",
      "Time to process input 2: 0.042249202728271484s\n",
      "Time to compute loss: 0.15654516220092773s\n",
      "\n",
      "\n",
      "Batch idx: 39; Loss: 2.755\n",
      "Time to read batch: 2.269669771194458s\n",
      "Time to transfer data to GPU: 0.0049648284912109375s\n",
      "Time to process input 1: 0.039226531982421875s\n",
      "Time to process input 2: 0.03823113441467285s\n",
      "Time to compute loss: 0.15178847312927246s\n",
      "\n",
      "\n",
      "Batch idx: 40; Loss: 2.719\n",
      "Time to read batch: 2.3198163509368896s\n",
      "Time to transfer data to GPU: 0.004935264587402344s\n",
      "Time to process input 1: 0.036858320236206055s\n",
      "Time to process input 2: 0.03923201560974121s\n",
      "Time to compute loss: 0.15289998054504395s\n",
      "\n",
      "\n",
      "Batch idx: 41; Loss: 2.685\n",
      "Time to read batch: 2.5943470001220703s\n",
      "Time to transfer data to GPU: 0.004753828048706055s\n",
      "Time to process input 1: 0.04023027420043945s\n",
      "Time to process input 2: 0.04029393196105957s\n",
      "Time to compute loss: 0.15037202835083008s\n",
      "\n",
      "\n",
      "Batch idx: 42; Loss: 2.651\n",
      "Time to read batch: 2.366076707839966s\n",
      "Time to transfer data to GPU: 0.005549907684326172s\n",
      "Time to process input 1: 0.03867697715759277s\n",
      "Time to process input 2: 0.03871893882751465s\n",
      "Time to compute loss: 0.15512323379516602s\n",
      "\n",
      "\n",
      "Batch idx: 43; Loss: 2.619\n",
      "Time to read batch: 2.1819984912872314s\n",
      "Time to transfer data to GPU: 0.0046541690826416016s\n",
      "Time to process input 1: 0.03784584999084473s\n",
      "Time to process input 2: 0.03800177574157715s\n",
      "Time to compute loss: 0.15329980850219727s\n",
      "\n",
      "\n",
      "Batch idx: 44; Loss: 2.588\n",
      "Time to read batch: 2.5987632274627686s\n",
      "Time to transfer data to GPU: 0.004957914352416992s\n",
      "Time to process input 1: 0.04143023490905762s\n",
      "Time to process input 2: 0.04015302658081055s\n",
      "Time to compute loss: 0.15068483352661133s\n",
      "\n",
      "\n",
      "Batch idx: 45; Loss: 2.557"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0f3d504c75bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m train_model_2steps_custom(spherical_unet, device, training_ds, constants_tensor.transpose(1,0), batch_size, epochs=7, \\\n\u001b[0;32m----> 2\u001b[0;31m                                            lr=learning_rate, validation_data=dl_val)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-626718d75a82>\u001b[0m in \u001b[0;36mtrain_model_2steps_custom\u001b[0;34m(model, device, training_ds, constants, batch_size, epochs, lr, validation_data)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0moutput2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mt7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mt8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weather/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weather/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weather/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2202\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2203\u001b[0m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2204\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2205\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model_2steps_custom(spherical_unet, device, training_ds, constants_tensor.transpose(1,0), batch_size, epochs=7, \\\n",
    "                                           lr=learning_rate, validation_data=dl_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
