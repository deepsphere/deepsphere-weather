{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing\n",
    "\n",
    "Download, save, visualize and load dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/\"\n",
    "resolution = \"5.625deg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. geopotential_500, temperature_850"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data\n",
    "\n",
    "- Test: years 2017 and 2018\n",
    "- Validation: year 2016\n",
    "- Train: year 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(DATA_DIR, train_years, val_years, test_years):\n",
    "    \n",
    "    time_slices = {'train': train_years, 'val': val_years, 'test': test_years}\n",
    "    \n",
    "    \n",
    "    zpath = DATA_DIR + '5.625deg/geopotential_500/'\n",
    "    tpath = DATA_DIR + '5.625deg/temperature_850/'\n",
    "    \n",
    "    z = xr.open_mfdataset(zpath+'/*.nc', combine='by_coords')['z'].assign_coords(level=1)\n",
    "    t = xr.open_mfdataset(tpath+'/*.nc', combine='by_coords')['t'].assign_coords(level=1)\n",
    "\n",
    "    ratio = len(z.coords['lon'])/len(z.coords['lat'])\n",
    "\n",
    "    data = xr.concat([z, t], 'level').stack(v=('lat', 'lon')).transpose('time', 'v', 'level').drop('level')\n",
    "    \n",
    "    data_paths = []\n",
    "    for set_name in ['train', 'val', 'test']:\n",
    "    \n",
    "        # Create directory\n",
    "        out_path = DATA_DIR + set_name + \"/\"\n",
    "        Path(out_path).mkdir(parents=True, exist_ok=True)\n",
    "        data_paths.append(out_path)\n",
    "        \n",
    "        # Select relevant years\n",
    "        dataset = data.sel(time=time_slices[set_name])\n",
    "\n",
    "        # Compute mean and std\n",
    "        mean = data.mean(('time', 'v')).compute()\n",
    "        std = data.std('time').mean(('v')).compute()\n",
    "        np.save(out_path + 'mean.npy', mean.values)\n",
    "        np.save(out_path + 'std.npy', std.values)\n",
    "    \n",
    "        # Save individual arrays\n",
    "        for i, array in enumerate(dataset):\n",
    "            np.save(out_path + str(i) + '.npy', array.values)\n",
    "    \n",
    "    return tuple(data_paths)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherBenchDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path, delta_t, mean=None, std=None):\n",
    "        \n",
    "        self.delta_t = delta_t\n",
    "        \n",
    "        self.mean = np.load(data_path + 'mean.npy') if mean is None else mean\n",
    "        self.std = np.load(data_path + 'std.npy') if std is None else std\n",
    "        \n",
    "        '''self.transform = transforms.Compose([torch.Tensor(), \n",
    "                                              transforms.Normalize(mean=self.mean, std=self.std)])'''\n",
    "        \n",
    "        total_samples = len(os.listdir(data_path)) - 2\n",
    "        self.datafiles = [data_path+str(id)+'.npy' for id in list(range(total_samples))]\n",
    "        \n",
    "        self.n_samples = len(self.datafiles) - self.delta_t\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Returns sample and label corresponding to an index as torch.Tensor objects\n",
    "            The return tensor shapes are (for the sample and the label): [n_vertex, n_features]\n",
    "        \"\"\"\n",
    "        \n",
    "        '''X = self.transform(np.load(self.datafiles[idx]))\n",
    "        y = self.transform(np.load(self.datafiles[idx+delta_t]))'''\n",
    "        \n",
    "        X = torch.Tensor((np.load(self.datafiles[idx])-self.mean)/self.std)\n",
    "        y = torch.Tensor((np.load(self.datafiles[idx+delta_t])-self.mean)/self.std)\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The graphs\n",
    "\n",
    "Bandwidth = [dim1/2, dim2/2]  \n",
    "sampling = 'SOFT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepsphere.utils.laplacian_funcs import prepare_laplacian\n",
    "from deepsphere.utils.samplings import equiangular_dimension_unpack\n",
    "from pygsp.graphs.sphereequiangular import SphereEquiangular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_laplacian(nodes, ratio, laplacian_type):\n",
    "    dim1, dim2 = equiangular_dimension_unpack(nodes, ratio)\n",
    "    \n",
    "    bw = [int(dim1/2), int(dim2/2)]\n",
    "\n",
    "    G = SphereEquiangular(bandwidth=bw, sampling=\"SOFT\")\n",
    "    G.compute_laplacian(laplacian_type)\n",
    "    laplacian = prepare_laplacian(G.L)\n",
    "    \n",
    "    return laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_equiangular_laplacians2(nodes, ratio, depth, laplacian_type, pool_size):\n",
    "    \"\"\"Get the equiangular laplacian list for a certain depth.\n",
    "    Args:\n",
    "        nodes (tuple): input signal size (lat x lon)\n",
    "        depth (int): the depth of the UNet.\n",
    "        laplacian_type [\"combinatorial\", \"normalized\"]: the type of the laplacian.\n",
    "        pool_size: size of the pooling kernel\n",
    "    Returns:\n",
    "        laps (list): increasing list of laplacians\n",
    "    \"\"\"\n",
    "    laps = []\n",
    "    dim1, dim2 = equiangular_dimension_unpack(nodes, ratio)\n",
    "    \n",
    "    for i in range(depth):\n",
    "        # Adjust dimensions with depth!\n",
    "        bw1 = int(dim1/(2*pool_size**i))\n",
    "        bw2 =int(dim2/(2*pool_size**i))\n",
    "        bw = [bw1, bw2]\n",
    "        \n",
    "        G = SphereEquiangular(bandwidth=bw, sampling=\"SOFT\")\n",
    "        G.compute_laplacian(laplacian_type)\n",
    "        laplacian = prepare_laplacian(G.L)\n",
    "        laps.append(laplacian)\n",
    "    return laps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The models\n",
    "\n",
    "https://github.com/ArcaniteSolutions/deepsphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "from deepsphere.models.spherical_unet.encoder import SphericalChebBN2\n",
    "from deepsphere.models.spherical_unet.utils import SphericalChebBNPool\n",
    "from deepsphere.models.spherical_unet.decoder import SphericalChebBNPoolConcat, SphericalChebBNPoolCheb\n",
    "\n",
    "\n",
    "from deepsphere.layers.chebyshev import SphericalChebConv\n",
    "from deepsphere.utils.laplacian_funcs import get_equiangular_laplacians\n",
    "from deepsphere.layers.samplings.equiangular_pool_unpool import Equiangular, reformat\n",
    "from deepsphere.utils.samplings import equiangular_calculator\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EquiangularMaxPool2(nn.MaxPool1d):\n",
    "    \"\"\"EquiAngular Maxpooling module using MaxPool 1d from torch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ratio, kernel_size, return_indices=True):\n",
    "        \"\"\"Initialization\n",
    "        Args:\n",
    "            ratio (float): ratio between latitude and longitude dimensions of the data\n",
    "        \"\"\"\n",
    "        self.ratio = ratio\n",
    "        super().__init__(kernel_size=kernel_size, return_indices=return_indices)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"calls Maxpool1d and if desired, keeps indices of the pixels pooled to unpool them\n",
    "        Args:\n",
    "            input (:obj:`torch.tensor`): batch x pixels x features\n",
    "        Returns:\n",
    "            tuple(:obj:`torch.tensor`, list(int)): batch x pooled pixels x features and the indices of the pixels pooled\n",
    "        \"\"\"\n",
    "        x, _ = equiangular_calculator(x, self.ratio)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        if self.return_indices:\n",
    "            x, indices = F.max_pool2d(x, self.kernel_size, return_indices=self.return_indices)\n",
    "        else:\n",
    "            x = F.max_pool2d(x, self.kernel_size)\n",
    "        x = reformat(x)\n",
    "\n",
    "        if self.return_indices:\n",
    "            output = x, indices\n",
    "        else:\n",
    "            output = x\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class EquiangularMaxUnpool2(nn.MaxUnpool1d):\n",
    "    \"\"\"Equiangular Maxunpooling using the MaxUnpool1d of pytorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ratio, kernel_size):\n",
    "        \"\"\"Initialization\n",
    "        Args:\n",
    "            ratio (float): ratio between latitude and longitude dimensions of the data\n",
    "        \"\"\"\n",
    "        self.ratio = ratio\n",
    "        \n",
    "        super().__init__(kernel_size=(kernel_size, kernel_size))\n",
    "\n",
    "    def forward(self, x, indices):\n",
    "        \"\"\"calls MaxUnpool1d using the indices returned previously by EquiAngMaxPool\n",
    "        Args:\n",
    "            x (:obj:`torch.tensor`): batch x pixels x features\n",
    "            indices (int): indices of pixels equiangular maxpooled previously\n",
    "        Returns:\n",
    "            :obj:`torch.tensor`: batch x unpooled pixels x features\n",
    "        \"\"\"\n",
    "        x, _ = equiangular_calculator(x, self.ratio)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.max_unpool2d(x, indices, self.kernel_size)\n",
    "        x = reformat(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SphericalConvNet(nn.Module):\n",
    "    \"\"\"Spherical GCNN Autoencoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nodes, ratio, depth, channels_in, channels_out, laplacian_type, kernel_size):\n",
    "        \"\"\"Initialization.\n",
    "        Args:\n",
    "            N (int): Number of pixels in the input image\n",
    "            depth (int): The depth of the UNet, which is bounded by the N and the type of pooling\n",
    "            kernel_size (int): chebychev polynomial degree\n",
    "            ratio (float): Parameter for equiangular sampling -> width/height\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.laplacian = compute_laplacian(nodes, ratio, laplacian_type)\n",
    "        \n",
    "        self.conv1 = SphericalChebConv(channels_in, 64, self.laplacian, self.kernel_size)\n",
    "        self.conv2 = SphericalChebConv(64, 64, self.laplacian, self.kernel_size)\n",
    "        self.conv3 = SphericalChebConv(64, 64, self.laplacian, self.kernel_size)\n",
    "        self.conv4 = SphericalChebConv(64, 64, self.laplacian, self.kernel_size)\n",
    "        self.conv5 = SphericalChebConv(64, channels_out, self.laplacian, self.kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward Pass.\n",
    "        Args:\n",
    "            x (:obj:`torch.Tensor`): input to be forwarded.\n",
    "        Returns:\n",
    "            :obj:`torch.Tensor`: output\n",
    "        \"\"\"\n",
    "        \n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = F.elu(self.conv4(x))\n",
    "        x = self.conv5(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder2(nn.Module):\n",
    "    \"\"\"\n",
    "    SphericalChebConv has \"same\" padding\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, channels_in, kernel_size, ratio, laplacians, pool_size, pooling=\"max\"):\n",
    "    \n",
    "        super().__init__()\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.conv1 = SphericalChebConv(channels_in, 64, laplacians[0], self.kernel_size)\n",
    "        self.conv2 = SphericalChebConv(64, 128, laplacians[1], self.kernel_size)\n",
    "        self.conv3 = SphericalChebConv(128, 128, laplacians[2], self.kernel_size)\n",
    "        self.conv4 = SphericalChebConv(128, 128, laplacians[3], self.kernel_size)\n",
    "\n",
    "        self.pool = EquiangularMaxPool2(ratio, pool_size, return_indices=True)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward Pass.\n",
    "        Args:\n",
    "            x (:obj:`torch.Tensor`): input [batch x vertices x channels/features]\n",
    "        Returns:\n",
    "            x_enc* :obj: `torch.Tensor`: output [batch x vertices x channels/features]\n",
    "        \"\"\"\n",
    "        \n",
    "        x_enc1 = F.relu(self.conv1(x))\n",
    "        \n",
    "        x_enc2, idx2 = self.pool(x_enc1)\n",
    "        x_enc2 = F.relu(self.conv2(x_enc2))\n",
    "        \n",
    "        x_enc3, idx3 = self.pool(x_enc2)\n",
    "        x_enc3 = F.relu(self.conv3(x_enc3))\n",
    "        \n",
    "        x_enc4, idx4 = self.pool(x_enc3)\n",
    "        x_enc4 = self.conv4(x_enc4)\n",
    "\n",
    "        return x_enc2, idx2, x_enc3, idx3, x_enc4, idx4\n",
    "\n",
    "class Decoder2(nn.Module):\n",
    "    \"\"\"The decoder of the Spherical UNet.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, channels_out, kernel_size, ratio, laplacians, pool_size, pooling=\"max\"):\n",
    "        \"\"\"Initialization.\n",
    "        Args:\n",
    "            unpooling (:obj:`torch.nn.Module`): The unpooling object.\n",
    "            laps (list): List of laplacians.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.unpool = EquiangularMaxUnpool2(ratio, pool_size)\n",
    "\n",
    "        self.deconv3 = SphericalChebConv(128, 128, laplacians[2], self.kernel_size)\n",
    "        self.deconv2 = SphericalChebConv(128, 64, laplacians[1], self.kernel_size)\n",
    "        self.deconv1 = SphericalChebConv(64, channels_out, laplacians[0], self.kernel_size)\n",
    "        \n",
    "        self.conv3 = SphericalChebConv(128+128, 128, laplacians[2], self.kernel_size)\n",
    "        self.conv2 = SphericalChebConv(128+64, 64, laplacians[1], self.kernel_size)\n",
    "\n",
    "        # Switch from Logits to Probabilities if evaluating model\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x_enc2, idx2, x_enc3, idx3, x_enc4, idx4):\n",
    "        \"\"\"Forward Pass.\n",
    "        Args:\n",
    "            x_enc* (:obj:`torch.Tensor`): input tensors.\n",
    "        Returns:\n",
    "            :obj:`torch.Tensor`: output after forward pass.\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.unpool(x_enc4, idx4)\n",
    "        x = F.relu(self.deconv3(x))\n",
    "        x = torch.cat((x, x_enc3), dim=2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        x = self.unpool(x, idx3)\n",
    "        x = F.relu(self.deconv2(x))\n",
    "        x = torch.cat((x, x_enc2), dim=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        \n",
    "        x = self.unpool(x, idx2)\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        \n",
    "        '''if not self.training:\n",
    "            x = self.softmax(x)'''\n",
    "        return x\n",
    "    \n",
    "class SphericalUNet2(nn.Module):\n",
    "    \"\"\"Spherical GCNN Autoencoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nodes, ratio, depth, channels_in, channels_out, laplacian_type, kernel_size, pooling_size, \n",
    "                 pooling=\"max\"):\n",
    "        \"\"\"Initialization.\n",
    "        Args:\n",
    "            N (int): Number of pixels in the input image\n",
    "            depth (int): The depth of the UNet, which is bounded by the N and the type of pooling\n",
    "            kernel_size (int): chebychev polynomial degree\n",
    "            ratio (float): Parameter for equiangular sampling -> width/height\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.pooling_class = Equiangular(mode=pooling)\n",
    "        self.laplacians = get_equiangular_laplacians2(nodes, ratio, depth, laplacian_type, pooling_size)\n",
    "\n",
    "        self.encoder = Encoder2(channels_in, self.kernel_size, ratio, self.laplacians, pooling_size, pooling)\n",
    "        self.decoder = Decoder2(channels_out, self.kernel_size, ratio, self.laplacians, pooling_size, pooling)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward Pass.\n",
    "        Args:\n",
    "            x (:obj:`torch.Tensor`): input to be forwarded.\n",
    "        Returns:\n",
    "            :obj:`torch.Tensor`: output\n",
    "        \"\"\"\n",
    "        x_encoder = self.encoder(x)\n",
    "        output = self.decoder(*x_encoder)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, lr, device, train_generator, val_generator, patience):\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    min_val_loss = float(\"inf\")\n",
    "    epoch_no_improve = 0\n",
    "    \n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        time1 = time.time()\n",
    "        \n",
    "        val_loss = 0\n",
    "        train_loss = 0\n",
    "        \n",
    "        model.train()\n",
    "        for batch_idx, (batch, labels) in enumerate(train_generator):\n",
    "            # Transfer to GPU\n",
    "            batch, labels = batch.to(device), labels.to(device)\n",
    "            \n",
    "            \n",
    "            # Model\n",
    "            output = model(batch)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            train_loss = train_loss + loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        train_loss = train_loss / len(train_generator)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            for batch, labels in val_generator:\n",
    "                # Transfer to GPU\n",
    "                batch, labels = batch.to(device), labels.to(device)\n",
    "                \n",
    "                output = model(batch)\n",
    "\n",
    "                val_loss = val_loss + criterion(output, labels).item()\n",
    "                \n",
    "        val_loss = val_loss/len(train_generator)\n",
    "        \n",
    "        time2 = time.time()\n",
    "        # Print stuff\n",
    "        print('Epoch: {e:3d}/{n_e:3d}  - loss: {l:.3f}  - val_loss: {v_l:.3f}  - time: {t:2f}'\n",
    "              .format(e=epoch+1, n_e=n_epochs, l=train_loss, v_l=val_loss, t=time2-time1))\n",
    "                \n",
    "        # Check for early stopping\n",
    "        if val_loss < min_val_loss:\n",
    "            epoch_no_improve = 0\n",
    "            min_val_loss = val_loss\n",
    "        else:\n",
    "            epoch_no_improve += 1\n",
    "\n",
    "        if epoch_no_improve == patience:\n",
    "            print('Epoch {e:3d}: early stopping'.format(e=epoch+1))\n",
    "            torch.save(model.state_dict(), 'model.pt')\n",
    "            break\n",
    "            \n",
    "    torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_device(model, ids=None):\n",
    "    \"\"\"Initialize device based on cpu/gpu and number of gpu\n",
    "    Args:\n",
    "        device (str): cpu or gpu\n",
    "        ids (list of int or str): list of gpus that should be used\n",
    "        unet (torch.Module): the model to place on the device(s)\n",
    "    Raises:\n",
    "        Exception: There is an error in configuring the cpu or gpu\n",
    "    Returns:\n",
    "        torch.Module, torch.device: the model placed on device, the device\n",
    "    \"\"\"\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        if ids is None:\n",
    "            device = torch.device(\"cuda\")\n",
    "            model = model.to(device)\n",
    "            model = nn.DataParallel(model)\n",
    "        elif len(ids) == 1:\n",
    "            device = torch.device(\"cuda:{}\".format(ids[0]))\n",
    "            model = model.to(device)\n",
    "        else:\n",
    "            device = torch.device(\"cuda:{}\".format(ids[0]))\n",
    "            model = model.to(device)\n",
    "            model = nn.DataParallel(model, device_ids=[int(i) for i in ids])\n",
    "        #cudnn.benchmark = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        model = model.to(device)\n",
    "\n",
    "    return model, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = DATA_DIR + \"train/\"\n",
    "val_path = DATA_DIR + \"val/\"\n",
    "test_path = DATA_DIR + \"test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_t = 3*24  # 3 days\n",
    "ratio = 64/32   # lon/lat\n",
    "\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4  # we doubled the learning rate as we doubled the batch size\n",
    "n_epochs = 100\n",
    "\n",
    "# Data\n",
    "training_set = WeatherBenchDataset(train_path, delta_t)\n",
    "validation_set = WeatherBenchDataset(val_path, delta_t, training_set.mean, training_set.std)\n",
    "dataloader_train = DataLoader(training_set, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "dataloader_validation = DataLoader(validation_set, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "N = len(training_set[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model - CONVNET\n",
    "convnet = SphericalConvNet(nodes=N, ratio=ratio, depth=4, channels_in=2, channels_out=2, laplacian_type=\"combinatorial\", \n",
    "                           kernel_size=5)\n",
    "\n",
    "convnet, device = init_device(model=convnet, ids=[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1/100  - loss: 0.663  - val_loss: 0.743  - time: 34.713446\n",
      "Epoch:   2/100  - loss: 0.659  - val_loss: 0.744  - time: 34.597876\n",
      "Epoch:   3/100  - loss: 0.657  - val_loss: 0.741  - time: 34.538268\n",
      "Epoch:   4/100  - loss: 0.655  - val_loss: 0.738  - time: 34.575342\n",
      "Epoch:   5/100  - loss: 0.653  - val_loss: 0.734  - time: 34.546397\n",
      "Epoch:   6/100  - loss: 0.651  - val_loss: 0.731  - time: 34.661316\n"
     ]
    }
   ],
   "source": [
    "train_model(convnet, learning_rate, device, dataloader_train, dataloader_validation, patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''trained_convnet = SphericalConvNet(nodes=N, ratio=ratio, depth=4, channels_in=2, channels_out=2, laplacian_type=\"combinatorial\", kernel_size=5)\n",
    "trained_convnet, device = init_device(model=trained_convnet, ids=[3])\n",
    "\n",
    "trained_convnet.load_state_dict(torch.load('model.pt'))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataset, mean, std):\n",
    "    \n",
    "    predictions = np.empty((len(dataset), *tuple(dataset[0][0].size())))\n",
    "    labels = np.empty((len(dataset), *tuple(dataset[0][0].size())))\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=8)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for i, (sample, label) in enumerate(loader):\n",
    "        sample, label = sample.to(device), label.to(device)\n",
    "        \n",
    "        output = model(sample)\n",
    "        \n",
    "        predictions[i] = output.detach().cpu().clone().numpy()\n",
    "        labels[i] = label.detach().cpu().clone().numpy()\n",
    "    \n",
    "    predictions = predictions * std + mean\n",
    "    labels = labels * std + mean\n",
    "    \n",
    "    return predictions, labels  \n",
    "\n",
    "def to_xarray(array, N, ratio):\n",
    "    d1, d2 =  equiangular_dimension_unpack(N, ratio)\n",
    "    bw = [int(d1/2), int(d2/2)]\n",
    "    G = SphereEquiangular(bandwidth=bw, sampling=\"SOFT\")\n",
    "    latitudes = G.lat[:, 0] * (180/np.pi)\n",
    "    longitudes = G.lon[0] * (180/np.pi)\n",
    "\n",
    "    array = array.reshape((array.shape[0], d1, d2, array.shape[2]))\n",
    "    \n",
    "    coordinates = {'time': list(range(array.shape[0])), 'lat': latitudes, 'lon': longitudes}\n",
    "    \n",
    "    dataset = xr.Dataset(data_vars={'z':  (('time', 'lat', 'lon'), array[:, :, :, 0]), \n",
    "                                    't':  (('time', 'lat', 'lon'), array[:, :, :, 1])}, \n",
    "                         coords=coordinates)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def compute_weighted_rmse(da_fc, da_true, mean_dims=xr.ALL_DIMS):\n",
    "    \"\"\"\n",
    "    Compute the RMSE with latitude weighting from two xr.DataArrays.\n",
    "    Args:\n",
    "        da_fc (xr.DataArray): Forecast. Time coordinate must be validation time.\n",
    "        da_true (xr.DataArray): Truth.\n",
    "    Returns:\n",
    "        rmse: Latitude weighted root mean squared error\n",
    "    \"\"\"\n",
    "    error = da_fc - da_true\n",
    "    weights_lat = np.cos(np.deg2rad(error.lat))\n",
    "    weights_lat /= weights_lat.mean()\n",
    "    rmse = np.sqrt(((error)**2 * weights_lat).mean(mean_dims))\n",
    "    if type(rmse) is xr.Dataset:\n",
    "        rmse = rmse.rename({v: v + '_rmse' for v in rmse})\n",
    "    else: # DataArray\n",
    "        rmse.name = error.name + '_rmse' if not error.name is None else 'rmse'\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WeatherBench CNN:  \n",
    "**3 days** \n",
    "\n",
    "  Z500: > ~600  \n",
    "  T850: < 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z_rmse   float64 934.3\n",
      "    t_rmse   float64 4.03\n"
     ]
    }
   ],
   "source": [
    "testing_set = WeatherBenchDataset(test_path, delta_t)\n",
    "\n",
    "predictions, labels = predict(convnet, testing_set, training_set.mean, training_set.std)\n",
    "pred = to_xarray(predictions, N, ratio)\n",
    "valid = to_xarray(labels, N, ratio)\n",
    "\n",
    "print(compute_weighted_rmse(pred, valid).load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictions(model, dg):\n",
    "    \"\"\"Create predictions for non-iterative model\"\"\"\n",
    "    preds = model.predict_generator(dg)\n",
    "    # Unnormalize\n",
    "    preds = preds * dg.std.values + dg.mean.values\n",
    "    fcs = []\n",
    "    lev_idx = 0\n",
    "    for var, levels in dg.var_dict.items():\n",
    "        if levels is None:\n",
    "            fcs.append(xr.DataArray(\n",
    "                preds[:, :, :, lev_idx],\n",
    "                dims=['time', 'lat', 'lon'],\n",
    "                coords={'time': dg.valid_time, 'lat': dg.ds.lat, 'lon': dg.ds.lon},\n",
    "                name=var\n",
    "            ))\n",
    "            lev_idx += 1\n",
    "        else:\n",
    "            nlevs = len(levels)\n",
    "            fcs.append(xr.DataArray(\n",
    "                preds[:, :, :, lev_idx:lev_idx+nlevs],\n",
    "                dims=['time', 'lat', 'lon', 'level'],\n",
    "                coords={'time': dg.valid_time, 'lat': dg.ds.lat, 'lon': dg.ds.lon, 'level': levels},\n",
    "                name=var\n",
    "            ))\n",
    "            lev_idx += nlevs\n",
    "    return xr.merge(fcs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
