{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing\n",
    "\n",
    "Download, save, visualize and load dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/\"\n",
    "resolution = \"5.625deg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. geopotential_500, temperature_850"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data\n",
    "\n",
    "- Test: years 2017 and 2018\n",
    "- Validation: year 2016\n",
    "- Train: years 1979 to 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(DATA_DIR, train_years, val_years, test_years):\n",
    "    \n",
    "    time_slices = {'train': train_years, 'val': val_years, 'test': test_years}\n",
    "    \n",
    "    \n",
    "    zpath = DATA_DIR + '5.625deg/geopotential_500/'\n",
    "    tpath = DATA_DIR + '5.625deg/temperature_850/'\n",
    "    \n",
    "    z = xr.open_mfdataset(zpath+'/*.nc', combine='by_coords')['z'].assign_coords(level=1)\n",
    "    t = xr.open_mfdataset(tpath+'/*.nc', combine='by_coords')['t'].assign_coords(level=1)\n",
    "\n",
    "    ratio = len(z.coords['lon'])/len(z.coords['lat'])\n",
    "\n",
    "    data = xr.concat([z, t], 'level').stack(v=('lat', 'lon')).transpose('time', 'v', 'level').drop('level')\n",
    "    \n",
    "    data_paths = []\n",
    "    for set_name in ['train']:\n",
    "    \n",
    "        # Create directory\n",
    "        out_path = DATA_DIR + set_name + \"/\"\n",
    "        Path(out_path).mkdir(parents=True, exist_ok=True)\n",
    "        data_paths.append(out_path)\n",
    "        \n",
    "        # Select relevant years\n",
    "        dataset = data.sel(time=time_slices[set_name])\n",
    "\n",
    "        # Compute mean and std\n",
    "        mean = data.mean(('time', 'v')).compute()\n",
    "        std = data.std('time').mean(('v')).compute()\n",
    "        np.save(out_path + 'mean.npy', mean.values)\n",
    "        np.save(out_path + 'std.npy', std.values)\n",
    "    \n",
    "        # Save individual arrays\n",
    "        for i, array in enumerate(dataset):\n",
    "            np.save(out_path + str(i) + '.npy', array.values)\n",
    "\n",
    "def load_test_data(path, delta_t, years=slice('2017', '2018')):\n",
    "    zpath = path + '/geopotential_500'\n",
    "    tpath = path + '/temperature_850'\n",
    "    \n",
    "    z = xr.open_mfdataset(zpath+'/*.nc', combine='by_coords')['z']\n",
    "    t = xr.open_mfdataset(tpath+'/*.nc', combine='by_coords')['t']\n",
    "\n",
    "    try:\n",
    "        z = z.sel(level=500).drop('level')\n",
    "        z = z.drop('level')\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        t = t.sel(level=850).drop('level')\n",
    "        t = t.drop('level')\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    dataset = xr.merge([z, t], compat='override')\n",
    "\n",
    "    return dataset.sel(time=years).isel(time=slice(delta_t, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherBenchDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path, delta_t, mean=None, std=None):\n",
    "        \n",
    "        self.delta_t = delta_t\n",
    "        \n",
    "        self.mean = np.load(data_path + 'mean.npy') if mean is None else mean\n",
    "        self.std = np.load(data_path + 'std.npy') if std is None else std\n",
    "        \n",
    "        '''self.transform = transforms.Compose([torch.Tensor(), \n",
    "                                              transforms.Normalize(mean=self.mean, std=self.std)])'''\n",
    "        \n",
    "        total_samples = len(os.listdir(data_path)) - 2\n",
    "        self.n_samples = total_samples - self.delta_t\n",
    "        \n",
    "        self.datafiles = [(data_path+str(id)+'.npy', data_path+str(id+delta_t)+'.npy')\n",
    "                           for id in list(range(self.n_samples))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Returns sample and label corresponding to an index as torch.Tensor objects\n",
    "            The return tensor shapes are (for the sample and the label): [n_vertex, n_features]\n",
    "        \"\"\"\n",
    "        \n",
    "        '''X = self.transform(np.load(self.datafiles[idx]))\n",
    "        y = self.transform(np.load(self.datafiles[idx+delta_t]))'''\n",
    "        \n",
    "        X = torch.Tensor((np.load(self.datafiles[idx][0])-self.mean)/self.std)\n",
    "        y = torch.Tensor((np.load(self.datafiles[idx][1])-self.mean)/self.std)\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The graphs\n",
    "\n",
    "Bandwidth = [dim1/2, dim2/2]  \n",
    "sampling = 'SOFT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepsphere.utils.laplacian_funcs import prepare_laplacian\n",
    "from deepsphere.utils.samplings import equiangular_dimension_unpack\n",
    "from pygsp.graphs.sphereequiangular import SphereEquiangular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_laplacian(nodes, ratio, laplacian_type):\n",
    "    dim1, dim2 = equiangular_dimension_unpack(nodes, ratio)\n",
    "    \n",
    "    bw = [int(dim1/2), int(dim2/2)]\n",
    "\n",
    "    G = SphereEquiangular(bandwidth=bw, sampling=\"SOFT\")\n",
    "    G.compute_laplacian(laplacian_type)\n",
    "    laplacian = prepare_laplacian(G.L)\n",
    "    \n",
    "    return laplacian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The models\n",
    "\n",
    "https://github.com/ArcaniteSolutions/deepsphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "from deepsphere.models.spherical_unet.encoder import SphericalChebBN2\n",
    "from deepsphere.models.spherical_unet.utils import SphericalChebBNPool\n",
    "from deepsphere.models.spherical_unet.decoder import SphericalChebBNPoolConcat, SphericalChebBNPoolCheb\n",
    "\n",
    "\n",
    "from deepsphere.layers.chebyshev import SphericalChebConv\n",
    "from deepsphere.utils.laplacian_funcs import get_equiangular_laplacians\n",
    "from deepsphere.layers.samplings.equiangular_pool_unpool import Equiangular, reformat\n",
    "from deepsphere.utils.samplings import equiangular_calculator\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EquiangularMaxPool2(nn.MaxPool1d):\n",
    "    \"\"\"EquiAngular Maxpooling module using MaxPool 1d from torch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ratio, kernel_size, return_indices=True):\n",
    "        \"\"\"Initialization\n",
    "        Args:\n",
    "            ratio (float): ratio between latitude and longitude dimensions of the data\n",
    "        \"\"\"\n",
    "        self.ratio = ratio\n",
    "        super().__init__(kernel_size=kernel_size, return_indices=return_indices)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"calls Maxpool1d and if desired, keeps indices of the pixels pooled to unpool them\n",
    "        Args:\n",
    "            input (:obj:`torch.tensor`): batch x pixels x features\n",
    "        Returns:\n",
    "            tuple(:obj:`torch.tensor`, list(int)): batch x pooled pixels x features and the indices of the pixels pooled\n",
    "        \"\"\"\n",
    "        x, _ = equiangular_calculator(x, self.ratio)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        if self.return_indices:\n",
    "            x, indices = F.max_pool2d(x, self.kernel_size, return_indices=self.return_indices)\n",
    "        else:\n",
    "            x = F.max_pool2d(x, self.kernel_size)\n",
    "        x = reformat(x)\n",
    "\n",
    "        if self.return_indices:\n",
    "            output = x, indices\n",
    "        else:\n",
    "            output = x\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class EquiangularMaxUnpool2(nn.MaxUnpool1d):\n",
    "    \"\"\"Equiangular Maxunpooling using the MaxUnpool1d of pytorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ratio, kernel_size):\n",
    "        \"\"\"Initialization\n",
    "        Args:\n",
    "            ratio (float): ratio between latitude and longitude dimensions of the data\n",
    "        \"\"\"\n",
    "        self.ratio = ratio\n",
    "        \n",
    "        super().__init__(kernel_size=(kernel_size, kernel_size))\n",
    "\n",
    "    def forward(self, x, indices):\n",
    "        \"\"\"calls MaxUnpool1d using the indices returned previously by EquiAngMaxPool\n",
    "        Args:\n",
    "            x (:obj:`torch.tensor`): batch x pixels x features\n",
    "            indices (int): indices of pixels equiangular maxpooled previously\n",
    "        Returns:\n",
    "            :obj:`torch.tensor`: batch x unpooled pixels x features\n",
    "        \"\"\"\n",
    "        x, _ = equiangular_calculator(x, self.ratio)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.max_unpool2d(x, indices, self.kernel_size)\n",
    "        x = reformat(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SphericalConvNet(nn.Module):\n",
    "    \"\"\"Spherical GCNN Autoencoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nodes, ratio, depth, channels_in, channels_out, laplacian_type, kernel_size):\n",
    "        \"\"\"Initialization.\n",
    "        Args:\n",
    "            N (int): Number of pixels in the input image\n",
    "            depth (int): The depth of the UNet, which is bounded by the N and the type of pooling\n",
    "            kernel_size (int): chebychev polynomial degree\n",
    "            ratio (float): Parameter for equiangular sampling -> width/height\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.laplacian = compute_laplacian(nodes, ratio, laplacian_type)\n",
    "        \n",
    "        self.conv1 = SphericalChebConv(channels_in, 64, self.laplacian, self.kernel_size)\n",
    "        self.conv2 = SphericalChebConv(64, 64, self.laplacian, self.kernel_size)\n",
    "        self.conv3 = SphericalChebConv(64, 64, self.laplacian, self.kernel_size)\n",
    "        self.conv4 = SphericalChebConv(64, 64, self.laplacian, self.kernel_size)\n",
    "        self.conv5 = SphericalChebConv(64, channels_out, self.laplacian, self.kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward Pass.\n",
    "        Args:\n",
    "            x (:obj:`torch.Tensor`): input to be forwarded.\n",
    "        Returns:\n",
    "            :obj:`torch.Tensor`: output\n",
    "        \"\"\"\n",
    "        \n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = F.elu(self.conv4(x))\n",
    "        x = self.conv5(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, lr, device, train_generator, val_generator, patience):\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    min_val_loss = float(\"inf\")\n",
    "    epoch_no_improve = 0\n",
    "    \n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        val_loss = 0\n",
    "        train_loss = 0\n",
    "        \n",
    "        model.train()\n",
    "        for batch_idx, (batch, labels) in enumerate(train_generator):\n",
    "            # Transfer to GPU\n",
    "            batch, labels = batch.to(device), labels.to(device)\n",
    "            \n",
    "            # Model\n",
    "            output = model(batch)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            train_loss = train_loss + loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        train_loss = train_loss / len(train_generator)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            for batch, labels in val_generator:\n",
    "                # Transfer to GPU\n",
    "                batch, labels = batch.to(device), labels.to(device)\n",
    "                \n",
    "                output = model(batch)\n",
    "\n",
    "                val_loss = val_loss + criterion(output, labels).item()\n",
    "                \n",
    "        val_loss = val_loss/len(train_generator)\n",
    "        \n",
    "        # Print stuff\n",
    "        print('Epoch: {e:3d}/{n_e:3d}  - loss: {l:.3f}  - val_loss: {v_l:.5f}  - time: {t:2f}'\n",
    "              .format(e=epoch+1, n_e=n_epochs, l=train_loss, v_l=val_loss, t=time2-time1))\n",
    "                \n",
    "        # Check for early stopping\n",
    "        if val_loss < min_val_loss:\n",
    "            epoch_no_improve = 0\n",
    "            min_val_loss = val_loss\n",
    "        else:\n",
    "            epoch_no_improve += 1\n",
    "\n",
    "        if epoch_no_improve == patience:\n",
    "            print('Epoch {e:3d}: early stopping'.format(e=epoch+1))\n",
    "            torch.save(model.state_dict(), 'model.pt')\n",
    "            break\n",
    "            \n",
    "    torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_device(model, ids=None):\n",
    "    \"\"\"Initialize device based on cpu/gpu and number of gpu\n",
    "    Args:\n",
    "        device (str): cpu or gpu\n",
    "        ids (list of int or str): list of gpus that should be used\n",
    "        unet (torch.Module): the model to place on the device(s)\n",
    "    Raises:\n",
    "        Exception: There is an error in configuring the cpu or gpu\n",
    "    Returns:\n",
    "        torch.Module, torch.device: the model placed on device, the device\n",
    "    \"\"\"\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        if ids is None:\n",
    "            device = torch.device(\"cuda\")\n",
    "            model = model.to(device)\n",
    "            model = nn.DataParallel(model)\n",
    "        elif len(ids) == 1:\n",
    "            device = torch.device(\"cuda:{}\".format(ids[0]))\n",
    "            model = model.to(device)\n",
    "        else:\n",
    "            device = torch.device(\"cuda:{}\".format(ids[0]))\n",
    "            model = model.to(device)\n",
    "            model = nn.DataParallel(model, device_ids=[int(i) for i in ids])\n",
    "        #cudnn.benchmark = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        model = model.to(device)\n",
    "\n",
    "    return model, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = DATA_DIR + \"train/\"\n",
    "val_path = DATA_DIR + \"val/\"\n",
    "test_path = DATA_DIR + \"test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_time = 3*24  # 3 days\n",
    "ratio = 64/32   # lon/lat\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4  # we doubled the learning rate as we doubled the batch size\n",
    "n_epochs = 100\n",
    "\n",
    "# Data\n",
    "training_set = WeatherBenchDataset(train_path, lead_time)\n",
    "validation_set = WeatherBenchDataset(val_path, lead_time, training_set.mean, training_set.std)\n",
    "dataloader_train = DataLoader(training_set, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "dataloader_validation = DataLoader(validation_set, batch_size=batch_size, shuffle=False, num_workers=16)\n",
    "\n",
    "N = len(training_set[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model - CONVNET\n",
    "convnet = SphericalConvNet(nodes=N, ratio=ratio, depth=4, channels_in=2, channels_out=2, laplacian_type=\"combinatorial\", \n",
    "                           kernel_size=5)\n",
    "\n",
    "convnet, device = init_device(model=convnet, ids=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1/100  - loss: 0.692  - val_loss: 0.017  - time: 408.169278\n",
      "Epoch:   2/100  - loss: 0.613  - val_loss: 0.017  - time: 409.275861\n",
      "Epoch:   3/100  - loss: 0.607  - val_loss: 0.017  - time: 408.563647\n",
      "Epoch:   4/100  - loss: 0.602  - val_loss: 0.016  - time: 409.316553\n",
      "Epoch:   5/100  - loss: 0.596  - val_loss: 0.016  - time: 410.081473\n",
      "Epoch:   6/100  - loss: 0.592  - val_loss: 0.016  - time: 409.226164\n",
      "Epoch:   7/100  - loss: 0.588  - val_loss: 0.016  - time: 409.561004\n",
      "Epoch:   8/100  - loss: 0.586  - val_loss: 0.016  - time: 409.839350\n",
      "Epoch:   9/100  - loss: 0.583  - val_loss: 0.016  - time: 410.048461\n",
      "Epoch:  10/100  - loss: 0.582  - val_loss: 0.016  - time: 410.152391\n",
      "Epoch:  11/100  - loss: 0.580  - val_loss: 0.016  - time: 410.555949\n",
      "Epoch:  12/100  - loss: 0.579  - val_loss: 0.016  - time: 410.275508\n",
      "Epoch:  13/100  - loss: 0.578  - val_loss: 0.016  - time: 410.017483\n",
      "Epoch:  14/100  - loss: 0.577  - val_loss: 0.016  - time: 410.114794\n",
      "Epoch:  15/100  - loss: 0.576  - val_loss: 0.016  - time: 410.040204\n",
      "Epoch:  16/100  - loss: 0.575  - val_loss: 0.016  - time: 410.948182\n",
      "Epoch:  17/100  - loss: 0.574  - val_loss: 0.016  - time: 410.707019\n",
      "Epoch:  18/100  - loss: 0.573  - val_loss: 0.016  - time: 410.176328\n",
      "Epoch:  19/100  - loss: 0.572  - val_loss: 0.016  - time: 410.415306\n",
      "Epoch:  20/100  - loss: 0.572  - val_loss: 0.016  - time: 409.885992\n",
      "Epoch:  21/100  - loss: 0.571  - val_loss: 0.016  - time: 411.117200\n",
      "Epoch:  22/100  - loss: 0.571  - val_loss: 0.016  - time: 410.993721\n",
      "Epoch:  23/100  - loss: 0.570  - val_loss: 0.016  - time: 410.926826\n",
      "Epoch:  24/100  - loss: 0.569  - val_loss: 0.016  - time: 410.300638\n",
      "Epoch:  25/100  - loss: 0.569  - val_loss: 0.016  - time: 410.352921\n",
      "Epoch:  26/100  - loss: 0.569  - val_loss: 0.016  - time: 410.467290\n",
      "Epoch:  27/100  - loss: 0.568  - val_loss: 0.015  - time: 410.315911\n",
      "Epoch:  28/100  - loss: 0.568  - val_loss: 0.016  - time: 410.732843\n",
      "Epoch:  29/100  - loss: 0.567  - val_loss: 0.016  - time: 411.494783\n",
      "Epoch:  30/100  - loss: 0.567  - val_loss: 0.015  - time: 410.424790\n",
      "Epoch:  31/100  - loss: 0.567  - val_loss: 0.015  - time: 410.065377\n",
      "Epoch:  32/100  - loss: 0.566  - val_loss: 0.015  - time: 410.030061\n",
      "Epoch:  33/100  - loss: 0.566  - val_loss: 0.015  - time: 410.966367\n",
      "Epoch:  34/100  - loss: 0.565  - val_loss: 0.016  - time: 411.179562\n",
      "Epoch:  35/100  - loss: 0.565  - val_loss: 0.015  - time: 410.631029\n",
      "Epoch  35: early stopping\n"
     ]
    }
   ],
   "source": [
    "train_model(convnet, learning_rate, device, dataloader_train, dataloader_validation, patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''trained_convnet = SphericalConvNet(nodes=N, ratio=ratio, depth=4, channels_in=2, channels_out=2, laplacian_type=\"combinatorial\", kernel_size=5)\n",
    "trained_convnet, device = init_device(model=trained_convnet, ids=[3])\n",
    "\n",
    "trained_convnet.load_state_dict(torch.load('model.pt'))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataset, valid_data):\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    model.eval()\n",
    "    \n",
    "    fcs = []\n",
    "    outputs = []\n",
    "    for i, (sample, label) in enumerate(loader):\n",
    "        sample, label = sample.to(device), label.to(device)\n",
    "        outputs.append(model(sample).detach().cpu().clone().numpy().squeeze(0))\n",
    "    \n",
    "    # Unnormalize and reshape\n",
    "    outputs = np.array(outputs)\n",
    "    outputs = outputs * dataset.std + dataset.mean\n",
    "    outputs = outputs.reshape((outputs.shape[0], valid.dims['lat'], valid.dims['lon'], outputs.shape[2]))\n",
    "        \n",
    "    for var_idx, var in enumerate(valid_data.data_vars):\n",
    "        fcs.append(xr.DataArray(outputs[:, :, :, var_idx],\n",
    "                                dims=['time', 'lat', 'lon'],\n",
    "                                coords={'time': valid_data.time, 'lat': valid_data.lat, 'lon': valid_data.lon},\n",
    "                                name=var))\n",
    "    \n",
    "    return xr.merge(fcs)\n",
    "\n",
    "def compute_weighted_rmse(da_fc, da_true, mean_dims=xr.ALL_DIMS):\n",
    "    \"\"\"\n",
    "    Compute the RMSE with latitude weighting from two xr.DataArrays.\n",
    "    Args:\n",
    "        da_fc (xr.DataArray): Forecast. Time coordinate must be validation time.\n",
    "        da_true (xr.DataArray): Truth.\n",
    "    Returns:\n",
    "        rmse: Latitude weighted root mean squared error\n",
    "    \"\"\"\n",
    "    error = da_fc - da_true\n",
    "    weights_lat = np.cos(np.deg2rad(error.lat))\n",
    "    weights_lat /= weights_lat.mean()\n",
    "    rmse = np.sqrt(((error)**2 * weights_lat).mean(mean_dims))\n",
    "    if type(rmse) is xr.Dataset:\n",
    "        rmse = rmse.rename({v: v + '_rmse' for v in rmse})\n",
    "    else: # DataArray\n",
    "        rmse.name = error.name + '_rmse' if not error.name is None else 'rmse'\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WeatherBench CNN:  \n",
    "**3 days** \n",
    "\n",
    "  Z500: ~600  \n",
    "  T850: < 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Coordinates:\n",
      "    level    int32 500\n",
      "Data variables:\n",
      "    z_rmse   float64 782.9\n",
      "    t_rmse   float64 3.514\n"
     ]
    }
   ],
   "source": [
    "testing_set = WeatherBenchDataset(test_path, lead_time, training_set.mean, training_set.std)\n",
    "\n",
    "valid = load_test_data(DATA_DIR + resolution, lead_time)\n",
    "pred = predict(convnet, testing_set, valid)\n",
    "\n",
    "print(compute_weighted_rmse(pred, valid).load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.to_netcdf(DATA_DIR + 'predictions/cnn_3d.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
