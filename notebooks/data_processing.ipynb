{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing\n",
    "\n",
    "Download, save, visualize and load dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/\"\n",
    "resolution = \"5.625deg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. geopotential_500, temperature_850"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data\n",
    "\n",
    "- Test: years 2017 and 2018\n",
    "- Validation: year 2016\n",
    "- Train: year 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(DATA_DIR, train_years, val_years, test_years):\n",
    "    \n",
    "    time_slices = {'train': train_years, 'val': val_years, 'test': test_years}\n",
    "    \n",
    "    \n",
    "    zpath = DATA_DIR + '5.625deg/geopotential_500/'\n",
    "    tpath = DATA_DIR + '5.625deg/temperature_850/'\n",
    "    \n",
    "    z = xr.open_mfdataset(zpath+'/*.nc', combine='by_coords')['z'].assign_coords(level=1)\n",
    "    t = xr.open_mfdataset(tpath+'/*.nc', combine='by_coords')['t'].assign_coords(level=1)\n",
    "\n",
    "    ratio = len(z.coords['lon'])/len(z.coords['lat'])\n",
    "\n",
    "    data = xr.concat([z, t], 'level').stack(v=('lat', 'lon')).transpose('time', 'v', 'level').drop('level')\n",
    "    \n",
    "    data_paths = []\n",
    "    for set_name in ['train', 'val', 'test']:\n",
    "    \n",
    "        # Create directory\n",
    "        out_path = DATA_DIR + set_name + \"/\"\n",
    "        Path(out_path).mkdir(parents=True, exist_ok=True)\n",
    "        data_paths.append(out_path)\n",
    "        \n",
    "        # Select relevant years\n",
    "        dataset = data.sel(time=time_slices[set_name])\n",
    "\n",
    "        # Compute mean and std\n",
    "        mean = data.mean(('time', 'v')).compute()\n",
    "        std = data.std('time').mean(('v')).compute()\n",
    "        np.save(out_path + 'mean.npy', mean.values)\n",
    "        np.save(out_path + 'std.npy', std.values)\n",
    "    \n",
    "        # Save individual arrays\n",
    "        for i, array in enumerate(dataset):\n",
    "            np.save(out_path + str(i) + '.npy', array.values)\n",
    "    \n",
    "    return tuple(data_paths)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_path, val_path, test_path = preprocess_data(DATA_DIR, \n",
    "                                                  train_years=slice('2015', '2015'), \n",
    "                                                  val_years=slice('2016', '2016'), \n",
    "                                                  test_years=slice('2017', '2018'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherBenchDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path, delta_t):\n",
    "        \n",
    "        self.delta_t = delta_t\n",
    "        \n",
    "        self.mean = np.load(data_path + 'mean.npy')\n",
    "        self.std = np.load(data_path + 'std.npy')\n",
    "        \n",
    "        '''self.transform = transforms.Compose([torch.Tensor(), \n",
    "                                              transforms.Normalize(mean=self.mean, std=self.std)])'''\n",
    "        \n",
    "        total_samples = len(os.listdir(data_path)) - 2\n",
    "        datafiles = [data_path+str(id)+'.npy' for id in list(range(total_samples))]\n",
    "        self.datafiles = datafiles[:-self.delta_t]\n",
    "        \n",
    "        self.n_samples = len(self.datafiles)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Returns sample and label corresponding to an index as torch.Tensor objects\n",
    "            The return tensor shapes are (for the sample and the label): [n_vertex, n_features]\n",
    "        \"\"\"\n",
    "        \n",
    "        '''X = self.transform(np.load(self.datafiles[idx]))\n",
    "        y = self.transform(np.load(self.datafiles[idx+delta_t]))'''\n",
    "        \n",
    "        X = torch.Tensor((np.load(self.datafiles[idx])-self.mean)/self.std)\n",
    "        y = torch.Tensor((np.load(self.datafiles[idx+delta_t])-self.mean)/self.std)\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The graphs\n",
    "\n",
    "Bandwidth = [dim1/2, dim2/2]  \n",
    "sampling = 'SOFT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_laplacian(nodes, ratio, laplacian_type):\n",
    "    dim1, dim2 = equiangular_dimension_unpack(nodes, ratio)\n",
    "    \n",
    "    bw = [int(dim1/2), int(dim2/2)]\n",
    "\n",
    "    G = SphereEquiangular(bandwidth=bw, sampling=\"SOFT\")\n",
    "    G.compute_laplacian(laplacian_type)\n",
    "    laplacian = prepare_laplacian(G.L)\n",
    "    \n",
    "    return laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepsphere.utils.laplacian_funcs import prepare_laplacian\n",
    "from deepsphere.utils.samplings import equiangular_dimension_unpack\n",
    "\n",
    "def get_equiangular_laplacians2(nodes, ratio, depth, laplacian_type, pool_size):\n",
    "    \"\"\"Get the equiangular laplacian list for a certain depth.\n",
    "    Args:\n",
    "        nodes (tuple): input signal size (lat x lon)\n",
    "        depth (int): the depth of the UNet.\n",
    "        laplacian_type [\"combinatorial\", \"normalized\"]: the type of the laplacian.\n",
    "        pool_size: size of the pooling kernel\n",
    "    Returns:\n",
    "        laps (list): increasing list of laplacians\n",
    "    \"\"\"\n",
    "    laps = []\n",
    "    dim1, dim2 = equiangular_dimension_unpack(nodes, ratio)\n",
    "    \n",
    "    for i in range(depth):\n",
    "        # Adjust dimensions with depth!\n",
    "        bw1 = int(dim1/(2*pool_size**i))\n",
    "        bw2 =int(dim2/(2*pool_size**i))\n",
    "        bw = [bw1, bw2]\n",
    "        \n",
    "        G = SphereEquiangular(bandwidth=bw, sampling=\"SOFT\")\n",
    "        G.compute_laplacian(laplacian_type)\n",
    "        laplacian = prepare_laplacian(G.L)\n",
    "        laps.append(laplacian)\n",
    "    return laps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The models\n",
    "\n",
    "https://github.com/ArcaniteSolutions/deepsphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "from deepsphere.models.spherical_unet.encoder import SphericalChebBN2\n",
    "from deepsphere.models.spherical_unet.utils import SphericalChebBNPool\n",
    "from deepsphere.models.spherical_unet.decoder import SphericalChebBNPoolConcat, SphericalChebBNPoolCheb\n",
    "\n",
    "\n",
    "from deepsphere.layers.chebyshev import SphericalChebConv\n",
    "from deepsphere.utils.laplacian_funcs import get_equiangular_laplacians\n",
    "from deepsphere.layers.samplings.equiangular_pool_unpool import Equiangular, reformat\n",
    "from deepsphere.utils.samplings import equiangular_calculator\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EquiangularMaxPool2(nn.MaxPool1d):\n",
    "    \"\"\"EquiAngular Maxpooling module using MaxPool 1d from torch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ratio, kernel_size, return_indices=False):\n",
    "        \"\"\"Initialization\n",
    "        Args:\n",
    "            ratio (float): ratio between latitude and longitude dimensions of the data\n",
    "        \"\"\"\n",
    "        self.ratio = ratio\n",
    "        super().__init__(kernel_size=kernel_size, return_indices=return_indices)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"calls Maxpool1d and if desired, keeps indices of the pixels pooled to unpool them\n",
    "        Args:\n",
    "            input (:obj:`torch.tensor`): batch x pixels x features\n",
    "        Returns:\n",
    "            tuple(:obj:`torch.tensor`, list(int)): batch x pooled pixels x features and the indices of the pixels pooled\n",
    "        \"\"\"\n",
    "        x, _ = equiangular_calculator(x, self.ratio)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        if self.return_indices:\n",
    "            x, indices = F.max_pool2d(x, self.kernel_size, return_indices=self.return_indices)\n",
    "        else:\n",
    "            x = F.max_pool2d(x, self.kernel_size)\n",
    "        x = reformat(x)\n",
    "\n",
    "        if self.return_indices:\n",
    "            output = x, indices\n",
    "        else:\n",
    "            output = x\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class EquiangularMaxUnpool2(nn.MaxUnpool1d):\n",
    "    \"\"\"Equiangular Maxunpooling using the MaxUnpool1d of pytorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ratio, kernel_size):\n",
    "        \"\"\"Initialization\n",
    "        Args:\n",
    "            ratio (float): ratio between latitude and longitude dimensions of the data\n",
    "        \"\"\"\n",
    "        self.ratio = ratio\n",
    "        \n",
    "        super().__init__(kernel_size=(kernel_size, kernel_size))\n",
    "\n",
    "    def forward(self, x, indices):\n",
    "        \"\"\"calls MaxUnpool1d using the indices returned previously by EquiAngMaxPool\n",
    "        Args:\n",
    "            x (:obj:`torch.tensor`): batch x pixels x features\n",
    "            indices (int): indices of pixels equiangular maxpooled previously\n",
    "        Returns:\n",
    "            :obj:`torch.tensor`: batch x unpooled pixels x features\n",
    "        \"\"\"\n",
    "        x, _ = equiangular_calculator(x, self.ratio)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.max_unpool2d(x, indices, self.kernel_size)\n",
    "        x = reformat(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SphericalConvNet(nn.Module):\n",
    "    \"\"\"Spherical GCNN Autoencoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nodes, ratio, depth, channels_in, channels_out, laplacian_type, kernel_size):\n",
    "        \"\"\"Initialization.\n",
    "        Args:\n",
    "            N (int): Number of pixels in the input image\n",
    "            depth (int): The depth of the UNet, which is bounded by the N and the type of pooling\n",
    "            kernel_size (int): chebychev polynomial degree\n",
    "            ratio (float): Parameter for equiangular sampling -> width/height\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.laplacian = compute_laplacian(nodes, ratio, laplacian_type)\n",
    "        \n",
    "        self.conv1 = SphericalChebConv(channels_in, 64, self.laplacian, self.kernel_size)\n",
    "        self.conv2 = SphericalChebConv(64, 64, self.laplacian, self.kernel_size)\n",
    "        self.conv3 = SphericalChebConv(64, 64, self.laplacian, self.kernel_size)\n",
    "        self.conv4 = SphericalChebConv(64, 64, self.laplacian, self.kernel_size)\n",
    "        self.conv5 = SphericalChebConv(64, channels_out, self.laplacian, self.kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward Pass.\n",
    "        Args:\n",
    "            x (:obj:`torch.Tensor`): input to be forwarded.\n",
    "        Returns:\n",
    "            :obj:`torch.Tensor`: output\n",
    "        \"\"\"\n",
    "        \n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = F.elu(self.conv4(x))\n",
    "        x = F.elu(self.conv5(x))\n",
    "        \n",
    "        if not self.training:\n",
    "            x = self.softmax(x)\n",
    "        return x\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder2(nn.Module):\n",
    "    \"\"\"\n",
    "    SphericalChebConv has \"same\" padding\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, channels_in, kernel_size, ratio, laplacians, pool_size, pooling=\"max\"):\n",
    "    \n",
    "        super().__init__()\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.conv1 = SphericalChebConv(channels_in, 64, laplacians[0], self.kernel_size)\n",
    "        self.conv2 = SphericalChebConv(64, 128, laplacians[1], self.kernel_size)\n",
    "        self.conv3 = SphericalChebConv(128, 128, laplacians[2], self.kernel_size)\n",
    "        self.conv4 = SphericalChebConv(128, 128, laplacians[3], self.kernel_size)\n",
    "\n",
    "        self.pool = EquiangularMaxPool2(ratio, pool_size, return_indices=True)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward Pass.\n",
    "        Args:\n",
    "            x (:obj:`torch.Tensor`): input [batch x vertices x channels/features]\n",
    "        Returns:\n",
    "            x_enc* :obj: `torch.Tensor`: output [batch x vertices x channels/features]\n",
    "        \"\"\"\n",
    "        \n",
    "        x_enc1 = F.relu(self.conv1(x))\n",
    "        \n",
    "        x_enc2, idx2 = self.pool(x_enc1)\n",
    "        x_enc2 = F.relu(self.conv2(x_enc2))\n",
    "        \n",
    "        x_enc3, idx3 = self.pool(x_enc2)\n",
    "        x_enc3 = F.relu(self.conv3(x_enc3))\n",
    "        \n",
    "        x_enc4, idx4 = self.pool(x_enc3)\n",
    "        x_enc4 = self.conv4(x_enc4)\n",
    "\n",
    "        return x_enc2, idx2, x_enc3, idx3, x_enc4, idx4\n",
    "\n",
    "class Decoder2(nn.Module):\n",
    "    \"\"\"The decoder of the Spherical UNet.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, channels_out, kernel_size, ratio, laplacians, pool_size, pooling=\"max\"):\n",
    "        \"\"\"Initialization.\n",
    "        Args:\n",
    "            unpooling (:obj:`torch.nn.Module`): The unpooling object.\n",
    "            laps (list): List of laplacians.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.unpool = EquiangularMaxUnpool2(ratio, pool_size)\n",
    "\n",
    "        self.deconv3 = SphericalChebConv(128, 128, laplacians[2], self.kernel_size)\n",
    "        self.deconv2 = SphericalChebConv(128, 64, laplacians[1], self.kernel_size)\n",
    "        self.deconv1 = SphericalChebConv(64, channels_out, laplacians[0], self.kernel_size)\n",
    "        \n",
    "        self.conv3 = SphericalChebConv(128+128, 128, laplacians[2], self.kernel_size)\n",
    "        self.conv2 = SphericalChebConv(128+64, 64, laplacians[1], self.kernel_size)\n",
    "\n",
    "        # Switch from Logits to Probabilities if evaluating model\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x_enc2, idx2, x_enc3, idx3, x_enc4, idx4):\n",
    "        \"\"\"Forward Pass.\n",
    "        Args:\n",
    "            x_enc* (:obj:`torch.Tensor`): input tensors.\n",
    "        Returns:\n",
    "            :obj:`torch.Tensor`: output after forward pass.\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.unpool(x_enc4, idx4)\n",
    "        x = F.relu(self.deconv3(x))\n",
    "        x = torch.cat((x, x_enc3), dim=2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        x = self.unpool(x, idx3)\n",
    "        x = F.relu(self.deconv2(x))\n",
    "        x = torch.cat((x, x_enc2), dim=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        \n",
    "        x = self.unpool(x, idx2)\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        \n",
    "        if not self.training:\n",
    "            x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "class SphericalUNet2(nn.Module):\n",
    "    \"\"\"Spherical GCNN Autoencoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nodes, ratio, depth, channels_in, channels_out, laplacian_type, kernel_size, pooling_size, \n",
    "                 pooling=\"max\"):\n",
    "        \"\"\"Initialization.\n",
    "        Args:\n",
    "            N (int): Number of pixels in the input image\n",
    "            depth (int): The depth of the UNet, which is bounded by the N and the type of pooling\n",
    "            kernel_size (int): chebychev polynomial degree\n",
    "            ratio (float): Parameter for equiangular sampling -> width/height\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.pooling_class = Equiangular(mode=pooling)\n",
    "        self.laplacians = get_equiangular_laplacians2(nodes, ratio, depth, laplacian_type, pooling_size)\n",
    "\n",
    "        self.encoder = Encoder2(channels_in, self.kernel_size, ratio, self.laplacians, pooling_size, pooling)\n",
    "        self.decoder = Decoder2(channels_out, self.kernel_size, ratio, self.laplacians, pooling_size, pooling)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward Pass.\n",
    "        Args:\n",
    "            x (:obj:`torch.Tensor`): input to be forwarded.\n",
    "        Returns:\n",
    "            :obj:`torch.Tensor`: output\n",
    "        \"\"\"\n",
    "        x_encoder = self.encoder(x)\n",
    "        output = self.decoder(*x_encoder)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, lr, device, train_generator, val_generator, patience):\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    min_val_loss = float(\"inf\")\n",
    "    epoch_no_improve = 0\n",
    "    \n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        val_loss = 0\n",
    "        loss = 0\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        for batch, labels in train_generator:\n",
    "            # Transfer to GPU\n",
    "            batch, labels = batch.to(device), labels.to(device)\n",
    "            \n",
    "            \n",
    "            # Model\n",
    "            output = model(batch)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "        model.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            for batch, labels in val_generator:\n",
    "                # Transfer to GPU\n",
    "                batch, labels = batch.to(device), labels.to(device)\n",
    "                \n",
    "                output = model(batch)\n",
    "\n",
    "                val_loss = val_loss + criterion(output, labels).item()/len(train_generator)\n",
    "                \n",
    "        # Print stuff\n",
    "        print('Epoch: {e:3d}/{n_e:3d}  - loss: {l:.3f}  - val_loss: {v_l:.3f}'.format(e=epoch, n_e=n_epochs, \n",
    "                                                                                      l=loss, v_l=val_loss))\n",
    "                \n",
    "        # Check for early stopping\n",
    "        if val_loss < min_val_loss:\n",
    "            epoch_no_improve = 0\n",
    "            min_val_loss = val_loss\n",
    "        else:\n",
    "            epoch_no_improve += 1\n",
    "\n",
    "        if epoch_no_improve == patience:\n",
    "            print('Epoch {e:3d}: early stopping'.format(e=epoch))\n",
    "            torch.save(model.state_dict(), 'model.pt')\n",
    "            break\n",
    "            \n",
    "    torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_device(model, ids=None):\n",
    "    \"\"\"Initialize device based on cpu/gpu and number of gpu\n",
    "    Args:\n",
    "        device (str): cpu or gpu\n",
    "        ids (list of int or str): list of gpus that should be used\n",
    "        unet (torch.Module): the model to place on the device(s)\n",
    "    Raises:\n",
    "        Exception: There is an error in configuring the cpu or gpu\n",
    "    Returns:\n",
    "        torch.Module, torch.device: the model placed on device, the device\n",
    "    \"\"\"\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        if ids is None:\n",
    "            device = torch.device(\"cuda\")\n",
    "            model = model.to(device)\n",
    "            unet = nn.DataParallel(unet)\n",
    "        elif len(ids) == 1:\n",
    "            device = torch.device(\"cuda:{}\".format(ids[0]))\n",
    "            model = model.to(device)\n",
    "        else:\n",
    "            device = torch.device(\"cuda:{}\".format(ids[0]))\n",
    "            model = model.to(device)\n",
    "            model = nn.DataParallel(model, device_ids=[int(i) for i in ids])\n",
    "        #cudnn.benchmark = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        model = model.to(device)\n",
    "\n",
    "    return model, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_t = 3*24\n",
    "ratio = 64/32\n",
    "\n",
    "batch_size = 68\n",
    "learning_rate = 1e-4\n",
    "n_epochs = 20\n",
    "\n",
    "# Data\n",
    "training_set = WeatherBenchDataset(train_path, delta_t)\n",
    "validation_set = WeatherBenchDataset(val_path, delta_t)\n",
    "dataloader_train = DataLoader(training_set, batch_size=batch_size, shuffle=False, num_workers=10)\n",
    "dataloader_validation = DataLoader(validation_set, batch_size=batch_size, shuffle=False, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model - CONVNET\n",
    "convnet = SphericalConvNet(nodes=N, ratio=ratio, depth=4, channels_in=2, channels_out=2, \"combinatorial\", \n",
    "                           kernel_size=5)\n",
    "\n",
    "convnet, device = init_device(model=unet, ids=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model - UNET\n",
    "unet = SphericalUNet2(nodes=N, ratio=ratio, depth=4, channels_in=2, channels_out=2, laplacian_type='combinatorial', \n",
    "                     kernel_size=5, pooling_size=2, pooling='max')\n",
    "\n",
    "unet, device = init_device(model=unet, ids=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff55d02a3b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff55d02a3b0>\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff55d02a3b0>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff55d02a3b0>\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    w.join()\n",
      "Traceback (most recent call last):\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "AssertionError: can only join a child process\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "    self._shutdown_workers()\n",
      "AssertionError: can only join a child process\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff55d02a3b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff55d02a3b0>\n",
      "    w.join()\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff55d02a3b0>\n",
      "    w.join()\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "Traceback (most recent call last):\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "AssertionError: can only join a child process\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff55d02a3b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff55d02a3b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff55d02a3b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Caught IndexError in DataLoader worker process 3.\nOriginal Traceback (most recent call last):\n  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-466-4804c5ca6a74>\", line 31, in __getitem__\n    y = torch.Tensor((np.load(self.datafiles[idx+delta_t])-self.mean)/self.std)\nIndexError: list index out of range\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-497-c3c77aed333b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-486-8798703677d3>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, lr, device, train_generator, val_generator, patience)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0;31m# Transfer to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 3.\nOriginal Traceback (most recent call last):\n  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/illorens/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-466-4804c5ca6a74>\", line 31, in __getitem__\n    y = torch.Tensor((np.load(self.datafiles[idx+delta_t])-self.mean)/self.std)\nIndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "train_model(convnet, learning_rate, device, dataloader_train, dataloader_validation, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(yhat, y):\n",
    "    return torch.mean((yhat-y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/168\n",
      "1/168\n",
      "2/168\n",
      "3/168\n",
      "4/168\n",
      "5/168\n",
      "6/168\n",
      "7/168\n",
      "8/168\n",
      "9/168\n",
      "10/168\n",
      "11/168\n",
      "12/168\n",
      "13/168\n",
      "14/168\n",
      "15/168\n",
      "16/168\n",
      "17/168\n",
      "18/168\n",
      "19/168\n",
      "20/168\n",
      "21/168\n",
      "22/168\n",
      "23/168\n",
      "24/168\n",
      "25/168\n",
      "26/168\n",
      "27/168\n",
      "28/168\n",
      "29/168\n",
      "30/168\n",
      "31/168\n",
      "32/168\n",
      "33/168\n",
      "34/168\n",
      "35/168\n",
      "36/168\n",
      "37/168\n",
      "38/168\n",
      "39/168\n",
      "40/168\n",
      "41/168\n",
      "42/168\n",
      "43/168\n",
      "44/168\n",
      "45/168\n",
      "46/168\n",
      "47/168\n",
      "48/168\n",
      "49/168\n",
      "50/168\n",
      "51/168\n",
      "52/168\n",
      "53/168\n",
      "54/168\n",
      "55/168\n",
      "56/168\n",
      "57/168\n",
      "58/168\n",
      "59/168\n",
      "60/168\n",
      "61/168\n",
      "62/168\n",
      "63/168\n",
      "64/168\n",
      "65/168\n",
      "66/168\n",
      "67/168\n",
      "68/168\n",
      "69/168\n",
      "70/168\n",
      "71/168\n",
      "72/168\n",
      "73/168\n",
      "74/168\n",
      "75/168\n",
      "76/168\n",
      "77/168\n",
      "78/168\n",
      "79/168\n",
      "80/168\n",
      "81/168\n",
      "82/168\n",
      "83/168\n",
      "84/168\n",
      "85/168\n",
      "86/168\n",
      "87/168\n",
      "88/168\n",
      "89/168\n",
      "90/168\n",
      "91/168\n",
      "92/168\n",
      "93/168\n",
      "94/168\n",
      "95/168\n",
      "96/168\n",
      "97/168\n",
      "98/168\n",
      "99/168\n",
      "100/168\n",
      "101/168\n",
      "102/168\n",
      "103/168\n",
      "104/168\n",
      "105/168\n",
      "106/168\n",
      "107/168\n",
      "108/168\n",
      "109/168\n",
      "110/168\n",
      "111/168\n",
      "112/168\n",
      "113/168\n",
      "114/168\n",
      "115/168\n",
      "116/168\n",
      "117/168\n",
      "118/168\n",
      "119/168\n",
      "120/168\n",
      "121/168\n",
      "122/168\n",
      "123/168\n",
      "124/168\n",
      "125/168\n",
      "126/168\n",
      "127/168\n",
      "128/168\n",
      "129/168\n",
      "130/168\n",
      "131/168\n",
      "132/168\n",
      "133/168\n",
      "134/168\n",
      "135/168\n",
      "136/168\n",
      "137/168\n",
      "138/168\n",
      "139/168\n",
      "140/168\n",
      "141/168\n",
      "142/168\n",
      "143/168\n",
      "144/168\n",
      "145/168\n",
      "146/168\n",
      "147/168\n",
      "148/168\n",
      "149/168\n",
      "150/168\n",
      "151/168\n",
      "152/168\n",
      "153/168\n",
      "154/168\n",
      "155/168\n",
      "156/168\n",
      "157/168\n",
      "158/168\n",
      "159/168\n",
      "160/168\n",
      "161/168\n",
      "162/168\n",
      "163/168\n",
      "164/168\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-499-5fdfde0df0bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0munet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/weather_modelling/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-466-4804c5ca6a74>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatafiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatafiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdelta_t\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "testing_set = WeatherBenchDataset(test_path, delta_t)\n",
    "dataloader_test = DataLoader(testing_set, batch_size=32, shuffle=False)\n",
    "\n",
    "'''trained_unet = SphericalUnet(N=N, depth=3, channels_in=2, channels_out=2, laplacian_type='combinatorial', \n",
    "                     kernel_size=5, ratio=ratio, pooling='max')\n",
    "trained_unet.load_state_dict(torch.load('model.pt'))\n",
    "trained_unet.eval()'''\n",
    "\n",
    "mse = torch.tensor(0, requires_grad=False).to(device)\n",
    "evaluation = nn.MSELoss()\n",
    "\n",
    "unet.eval()\n",
    "with torch.set_grad_enabled(False):\n",
    "    for batch_idx, (batch, labels) in enumerate(dataloader_test):\n",
    "        batch, labels = batch, labels\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "\n",
    "        output = unet(batch)\n",
    "\n",
    "        mse = mse + MSE(output, labels)\n",
    "\n",
    "RMSE = torch.sqrt(mse/len(dataloader_test)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
